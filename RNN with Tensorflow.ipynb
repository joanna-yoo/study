{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(name, shape):\n",
    "    return tf.get_variable(name=name, shape=shape, dtype=tf.float32,\n",
    "                          initializer=tf.initializers.glorot_uniform)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    return tf.get_variable(name=name, dtype=tf.float32,\n",
    "                          initializer=tf.constant(0., shape=shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Credit to https://www.udemy.com/course/complete-guide-to-tensorflow-for-deep-learning-with-python\n",
    "\"\"\"\n",
    "\n",
    "sample_data = pd.read_csv('data/monthly-milk-production.csv', index_col='Month')\n",
    "sample_data.plot()\n",
    "sample_data.head()\n",
    "\n",
    "seq = sample_data['Milk Production'].values\n",
    "test_seq = seq[-12:]\n",
    "train_seq = seq[:-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_seq = scaler.fit_transform(train_seq.reshape(-1, 1))\n",
    "test_seq = scaler.transform(test_seq.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 1\n",
    "n_out = 1\n",
    "batch_size = 1\n",
    "time_step_size = 12\n",
    "n_hid = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(seq, time_step_size, batch_size):\n",
    "    y_batch = []\n",
    "    for i in range(batch_size):\n",
    "        start_index = np.random.randint(0, high=len(seq) - time_step_size)\n",
    "        y_batch.append(seq[start_index:start_index + time_step_size + 1].reshape(\n",
    "            1, time_step_size + 1, -1))\n",
    "        \n",
    "    y_batch = np.concatenate(y_batch, axis=0)\n",
    "    \n",
    "    return y_batch[:,:-1, :], y_batch[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_x, _y = next_batch(train_seq, time_step_size, batch_size)\n",
    "print(_x.shape) # (batch_size, time_step_size,embedding_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Definition\n",
    "\"\"\"\n",
    "x = tf.placeholder(tf.float32, [None, time_step_size, n_in])\n",
    "y = tf.placeholder(tf.float32, [None, time_step_size, n_out])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hid, activation=tf.nn.relu)\n",
    "output, memory = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "W = weight_variable('W', [n_hid, n_out])\n",
    "b = bias_variable('b', [n_out])\n",
    "\n",
    "y_pred = tf.nn.relu(tf.matmul(output, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('logs/', tf.get_default_graph())\n",
    "tf.summary.scalar('training_loss', loss)\n",
    "summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_seq = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        x_batch, y_batch = next_batch(train_seq, time_step_size, batch_size)\n",
    "        \n",
    "        _, loss_step, _summary = sess.run([train, loss, summaries], \n",
    "                                          feed_dict={x: x_batch, y: y_batch})\n",
    "        writer.add_summary(_summary, global_step=i)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {i}:\\tTraining Loss - {loss_step:.04f}')\n",
    "        \n",
    "    \n",
    "    seq = list(train_seq[-12:, :].squeeze())\n",
    "    \n",
    "    for j in range(12):\n",
    "        x_batch = np.array(seq[-12:]).reshape(1, time_step_size, -1)\n",
    "        pred = sess.run(y_pred, feed_dict={x:x_batch})\n",
    "        seq.append(pred[0,-1, 0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = seq[-12:]\n",
    "pred = scaler.inverse_transform(np.array(pred).reshape(-1, 1))\n",
    "pred = pred.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = sample_data.tail(12)\n",
    "test_df['prediction'] = pred\n",
    "\n",
    "test_df.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Implementation of single cells \n",
    "#### Simple RNN\n",
    "#### GRU\n",
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of single cell for simple RNN, LSTM, and GRU\n",
    "\"\"\"\n",
    "\n",
    "class SingleRNNCellExamples:\n",
    "    def __init__(self, input_vocab_size, embedding_dim, n_hid):\n",
    "        #self.embedding = tf.Variable(tf.random_uniform([input_vocab_size, embedding_dim]))\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        # SimpleRNN\n",
    "        self.rnn_W = weight_variable('rnn_W', [embedding_dim + n_hid, n_hid])\n",
    "        self.rnn_b = bias_variable('rnn_b', [n_hid])\n",
    "        \n",
    "        # GRU\n",
    "        self.gru_W_h = weight_variable('gru_W_h', [embedding_dim + n_hid, n_hid])\n",
    "        self.gru_b_h = bias_variable('gru_b_h', [n_hid])\n",
    "        \n",
    "        self.gru_W_r = weight_variable('gru_W_r', [embedding_dim + n_hid, n_hid])\n",
    "        self.gru_b_r = bias_variable('gru_b_r', [n_hid])\n",
    "        \n",
    "        self.gru_W_u = weight_variable('gru_W_u', [embedding_dim + n_hid, n_hid])\n",
    "        self.gru_b_u = bias_variable('gru_b_u', [n_hid])\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm_W_o = weight_variable('lstm_W_o', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_o = bias_variable('lstm_b_o', [n_hid])\n",
    "        \n",
    "        self.lstm_W_u = weight_variable('lstm_W_u', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_u = bias_variable('lstm_b_u', [n_hid])    \n",
    " \n",
    "        self.lstm_W_f = weight_variable('lstm_W_f', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_f = bias_variable('lstm_b_f', [n_hid])    \n",
    "        \n",
    "        self.lstm_W_h = weight_variable('lstm_W_h', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_h = bias_variable('lstm_b_h', [n_hid])\n",
    "        \n",
    "    def _rnn(self, x, hid, activation=tf.nn.tanh):\n",
    "        _input = tf.concat([x, hid], -1)\n",
    "        return activation(tf.matmul(_input, self.rnn_W) + self.rnn_b)\n",
    "    \n",
    "    def _gru(self, x, hid):\n",
    "        _input = tf.concat([x, hid], -1)\n",
    "        relevance_gate = tf.nn.sigmoid(tf.matmul(_input, self.gru_W_r) + self.gru_b_r)\n",
    "        update_gate = tf.nn.sigmoid(tf.matmul(_input, self.gru_W_u) + self.gru_b_u)\n",
    "        \n",
    "        scaled_input = tf.concat([x, tf.multiply(relevance_gate, hid)], -1)\n",
    "        candidate_hid = tf.nn.sigmoid(tf.matmul(scaled_input, self.gru_W_h) + self.gru_b_h) \n",
    "        \n",
    "        return update_gate * candidate_hidden + (tf.ones_like(update_gate) - update_gate) * hid\n",
    "    \n",
    "    def _lstm(self, x, hid):\n",
    "        _input = tf.concat([x, hid], -1)\n",
    "        candidate_hid = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_h) + self.lstm_b_h)\n",
    "        \n",
    "        update_gate = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_u) + self.lstm_b_u)\n",
    "        forget_gate = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_f) + self.lstm_b_f)\n",
    "        output_gate = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_o) + self.lstm_b_o)\n",
    "        \n",
    "        hid = update_gate * candidate_hid + forget_gate * hid\n",
    "        return output_gate * tf.nn.tanh(hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
