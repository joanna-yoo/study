{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(name, shape):\n",
    "    return tf.get_variable(name=name, shape=shape, dtype=tf.float32,\n",
    "                          initializer=tf.initializers.glorot_uniform)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    return tf.get_variable(name=name, dtype=tf.float32,\n",
    "                          initializer=tf.constant(0., shape=shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Forcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Credit to https://www.udemy.com/course/complete-guide-to-tensorflow-for-deep-learning-with-python\n",
    "\"\"\"\n",
    "\n",
    "sample_data = pd.read_csv('data/monthly-milk-production.csv', index_col='Month')\n",
    "sample_data.plot()\n",
    "sample_data.head()\n",
    "\n",
    "seq = sample_data['Milk Production'].values\n",
    "test_seq = seq[-12:]\n",
    "train_seq = seq[:-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_seq = scaler.fit_transform(train_seq.reshape(-1, 1))\n",
    "test_seq = scaler.transform(test_seq.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 1\n",
    "n_out = 1\n",
    "batch_size = 1\n",
    "time_step_size = 12\n",
    "n_hid = 100\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(seq, time_step_size, batch_size):\n",
    "    y_batch = []\n",
    "    for i in range(batch_size):\n",
    "        start_index = np.random.randint(0, high=len(seq) - time_step_size)\n",
    "        y_batch.append(seq[start_index:start_index + time_step_size + 1].reshape(\n",
    "            1, time_step_size + 1, -1))\n",
    "        \n",
    "    y_batch = np.concatenate(y_batch, axis=0)\n",
    "    \n",
    "    return y_batch[:,:-1, :], y_batch[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "_x, _y = next_batch(train_seq, time_step_size, batch_size)\n",
    "print(_x.shape) # (batch_size, time_step_size,embedding_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-011eabb710f4>:7: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-8-011eabb710f4>:8: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/joanna/anaconda3/envs/default/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/joanna/anaconda3/envs/default/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4f4c058a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4f4c058a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4f4c058a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f4f4c058a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Definition\n",
    "\"\"\"\n",
    "x = tf.placeholder(tf.float32, [None, time_step_size, n_in])\n",
    "y = tf.placeholder(tf.float32, [None, time_step_size, n_out])\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hid, activation=tf.nn.relu)\n",
    "output, memory = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "W = weight_variable('W', [n_hid, n_out])\n",
    "b = bias_variable('b', [n_out])\n",
    "\n",
    "y_pred = tf.nn.relu(tf.matmul(output, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('logs/', tf.get_default_graph())\n",
    "tf.summary.scalar('training_loss', loss)\n",
    "summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6610576923076923,\n",
       " 0.5408653846153846,\n",
       " 0.8076923076923077,\n",
       " 0.8389423076923077,\n",
       " 1.0000000000000002,\n",
       " 0.9471153846153848,\n",
       " 0.8533653846153848,\n",
       " 0.7548076923076923,\n",
       " 0.6298076923076923,\n",
       " 0.622596153846154,\n",
       " 0.528846153846154,\n",
       " 0.625]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_seq[-12:, :].squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\tTraining Loss - 0.2347\n",
      "Epoch 1:\tTraining Loss - 0.0521\n",
      "Epoch 2:\tTraining Loss - 0.2731\n",
      "Epoch 3:\tTraining Loss - 0.0846\n",
      "Epoch 4:\tTraining Loss - 0.1967\n",
      "Epoch 5:\tTraining Loss - 0.4338\n",
      "Epoch 6:\tTraining Loss - 0.3871\n",
      "Epoch 7:\tTraining Loss - 0.1533\n",
      "Epoch 8:\tTraining Loss - 0.1548\n",
      "Epoch 9:\tTraining Loss - 0.0790\n",
      "Epoch 10:\tTraining Loss - 0.0266\n",
      "Epoch 11:\tTraining Loss - 0.2948\n",
      "Epoch 12:\tTraining Loss - 0.2812\n",
      "Epoch 13:\tTraining Loss - 0.0726\n",
      "Epoch 14:\tTraining Loss - 0.2507\n",
      "Epoch 15:\tTraining Loss - 0.0753\n",
      "Epoch 16:\tTraining Loss - 0.2268\n",
      "Epoch 17:\tTraining Loss - 0.0271\n",
      "Epoch 18:\tTraining Loss - 0.1913\n",
      "Epoch 19:\tTraining Loss - 0.0454\n",
      "Epoch 20:\tTraining Loss - 0.0405\n",
      "Epoch 21:\tTraining Loss - 0.1484\n",
      "Epoch 22:\tTraining Loss - 0.0439\n",
      "Epoch 23:\tTraining Loss - 0.0343\n",
      "Epoch 24:\tTraining Loss - 0.0449\n",
      "Epoch 25:\tTraining Loss - 0.1247\n",
      "Epoch 26:\tTraining Loss - 0.1545\n",
      "Epoch 27:\tTraining Loss - 0.0718\n",
      "Epoch 28:\tTraining Loss - 0.0805\n",
      "Epoch 29:\tTraining Loss - 0.1062\n",
      "Epoch 30:\tTraining Loss - 0.0104\n",
      "Epoch 31:\tTraining Loss - 0.0588\n",
      "Epoch 32:\tTraining Loss - 0.0221\n",
      "Epoch 33:\tTraining Loss - 0.0958\n",
      "Epoch 34:\tTraining Loss - 0.0752\n",
      "Epoch 35:\tTraining Loss - 0.0214\n",
      "Epoch 36:\tTraining Loss - 0.0419\n",
      "Epoch 37:\tTraining Loss - 0.0856\n",
      "Epoch 38:\tTraining Loss - 0.0552\n",
      "Epoch 39:\tTraining Loss - 0.1228\n",
      "Epoch 40:\tTraining Loss - 0.0112\n",
      "Epoch 41:\tTraining Loss - 0.0674\n",
      "Epoch 42:\tTraining Loss - 0.0377\n",
      "Epoch 43:\tTraining Loss - 0.0687\n",
      "Epoch 44:\tTraining Loss - 0.0970\n",
      "Epoch 45:\tTraining Loss - 0.0402\n",
      "Epoch 46:\tTraining Loss - 0.0422\n",
      "Epoch 47:\tTraining Loss - 0.0590\n",
      "Epoch 48:\tTraining Loss - 0.0933\n",
      "Epoch 49:\tTraining Loss - 0.1216\n",
      "Epoch 50:\tTraining Loss - 0.1316\n",
      "Epoch 51:\tTraining Loss - 0.0889\n",
      "Epoch 52:\tTraining Loss - 0.0380\n",
      "Epoch 53:\tTraining Loss - 0.1105\n",
      "Epoch 54:\tTraining Loss - 0.0174\n",
      "Epoch 55:\tTraining Loss - 0.0372\n",
      "Epoch 56:\tTraining Loss - 0.0150\n",
      "Epoch 57:\tTraining Loss - 0.0157\n",
      "Epoch 58:\tTraining Loss - 0.0261\n",
      "Epoch 59:\tTraining Loss - 0.0438\n",
      "Epoch 60:\tTraining Loss - 0.0168\n",
      "Epoch 61:\tTraining Loss - 0.0337\n",
      "Epoch 62:\tTraining Loss - 0.0585\n",
      "Epoch 63:\tTraining Loss - 0.0686\n",
      "Epoch 64:\tTraining Loss - 0.0310\n",
      "Epoch 65:\tTraining Loss - 0.0526\n",
      "Epoch 66:\tTraining Loss - 0.1096\n",
      "Epoch 67:\tTraining Loss - 0.0393\n",
      "Epoch 68:\tTraining Loss - 0.1204\n",
      "Epoch 69:\tTraining Loss - 0.0620\n",
      "Epoch 70:\tTraining Loss - 0.0862\n",
      "Epoch 71:\tTraining Loss - 0.0746\n",
      "Epoch 72:\tTraining Loss - 0.0577\n",
      "Epoch 73:\tTraining Loss - 0.0994\n",
      "Epoch 74:\tTraining Loss - 0.0988\n",
      "Epoch 75:\tTraining Loss - 0.0559\n",
      "Epoch 76:\tTraining Loss - 0.0467\n",
      "Epoch 77:\tTraining Loss - 0.0542\n",
      "Epoch 78:\tTraining Loss - 0.0203\n",
      "Epoch 79:\tTraining Loss - 0.0187\n",
      "Epoch 80:\tTraining Loss - 0.0312\n",
      "Epoch 81:\tTraining Loss - 0.0174\n",
      "Epoch 82:\tTraining Loss - 0.0994\n",
      "Epoch 83:\tTraining Loss - 0.0284\n",
      "Epoch 84:\tTraining Loss - 0.0155\n",
      "Epoch 85:\tTraining Loss - 0.0852\n",
      "Epoch 86:\tTraining Loss - 0.0618\n",
      "Epoch 87:\tTraining Loss - 0.0382\n",
      "Epoch 88:\tTraining Loss - 0.0384\n",
      "Epoch 89:\tTraining Loss - 0.0601\n",
      "Epoch 90:\tTraining Loss - 0.0144\n",
      "Epoch 91:\tTraining Loss - 0.0192\n",
      "Epoch 92:\tTraining Loss - 0.0530\n",
      "Epoch 93:\tTraining Loss - 0.0235\n",
      "Epoch 94:\tTraining Loss - 0.0507\n",
      "Epoch 95:\tTraining Loss - 0.0294\n",
      "Epoch 96:\tTraining Loss - 0.0465\n",
      "Epoch 97:\tTraining Loss - 0.0181\n",
      "Epoch 98:\tTraining Loss - 0.0249\n",
      "Epoch 99:\tTraining Loss - 0.0511\n",
      "Epoch 100:\tTraining Loss - 0.0375\n",
      "Epoch 101:\tTraining Loss - 0.0553\n",
      "Epoch 102:\tTraining Loss - 0.0382\n",
      "Epoch 103:\tTraining Loss - 0.0309\n",
      "Epoch 104:\tTraining Loss - 0.0819\n",
      "Epoch 105:\tTraining Loss - 0.0114\n",
      "Epoch 106:\tTraining Loss - 0.0160\n",
      "Epoch 107:\tTraining Loss - 0.0336\n",
      "Epoch 108:\tTraining Loss - 0.0374\n",
      "Epoch 109:\tTraining Loss - 0.0126\n",
      "Epoch 110:\tTraining Loss - 0.0322\n",
      "Epoch 111:\tTraining Loss - 0.0652\n",
      "Epoch 112:\tTraining Loss - 0.0219\n",
      "Epoch 113:\tTraining Loss - 0.0349\n",
      "Epoch 114:\tTraining Loss - 0.0587\n",
      "Epoch 115:\tTraining Loss - 0.0286\n",
      "Epoch 116:\tTraining Loss - 0.0358\n",
      "Epoch 117:\tTraining Loss - 0.0123\n",
      "Epoch 118:\tTraining Loss - 0.0164\n",
      "Epoch 119:\tTraining Loss - 0.0209\n",
      "Epoch 120:\tTraining Loss - 0.0353\n",
      "Epoch 121:\tTraining Loss - 0.0943\n",
      "Epoch 122:\tTraining Loss - 0.0334\n",
      "Epoch 123:\tTraining Loss - 0.0222\n",
      "Epoch 124:\tTraining Loss - 0.0165\n",
      "Epoch 125:\tTraining Loss - 0.0414\n",
      "Epoch 126:\tTraining Loss - 0.0197\n",
      "Epoch 127:\tTraining Loss - 0.0503\n",
      "Epoch 128:\tTraining Loss - 0.0223\n",
      "Epoch 129:\tTraining Loss - 0.0561\n",
      "Epoch 130:\tTraining Loss - 0.0483\n",
      "Epoch 131:\tTraining Loss - 0.0405\n",
      "Epoch 132:\tTraining Loss - 0.0354\n",
      "Epoch 133:\tTraining Loss - 0.0361\n",
      "Epoch 134:\tTraining Loss - 0.0180\n",
      "Epoch 135:\tTraining Loss - 0.0863\n",
      "Epoch 136:\tTraining Loss - 0.0479\n",
      "Epoch 137:\tTraining Loss - 0.0712\n",
      "Epoch 138:\tTraining Loss - 0.0516\n",
      "Epoch 139:\tTraining Loss - 0.0126\n",
      "Epoch 140:\tTraining Loss - 0.0392\n",
      "Epoch 141:\tTraining Loss - 0.0219\n",
      "Epoch 142:\tTraining Loss - 0.0200\n",
      "Epoch 143:\tTraining Loss - 0.0168\n",
      "Epoch 144:\tTraining Loss - 0.0303\n",
      "Epoch 145:\tTraining Loss - 0.0382\n",
      "Epoch 146:\tTraining Loss - 0.0457\n",
      "Epoch 147:\tTraining Loss - 0.0172\n",
      "Epoch 148:\tTraining Loss - 0.0681\n",
      "Epoch 149:\tTraining Loss - 0.0825\n",
      "Epoch 150:\tTraining Loss - 0.0550\n",
      "Epoch 151:\tTraining Loss - 0.0225\n",
      "Epoch 152:\tTraining Loss - 0.0211\n",
      "Epoch 153:\tTraining Loss - 0.0605\n",
      "Epoch 154:\tTraining Loss - 0.0301\n",
      "Epoch 155:\tTraining Loss - 0.0511\n",
      "Epoch 156:\tTraining Loss - 0.0313\n",
      "Epoch 157:\tTraining Loss - 0.0119\n",
      "Epoch 158:\tTraining Loss - 0.0608\n",
      "Epoch 159:\tTraining Loss - 0.0130\n",
      "Epoch 160:\tTraining Loss - 0.0221\n",
      "Epoch 161:\tTraining Loss - 0.0213\n",
      "Epoch 162:\tTraining Loss - 0.0336\n",
      "Epoch 163:\tTraining Loss - 0.0282\n",
      "Epoch 164:\tTraining Loss - 0.0330\n",
      "Epoch 165:\tTraining Loss - 0.0103\n",
      "Epoch 166:\tTraining Loss - 0.0196\n",
      "Epoch 167:\tTraining Loss - 0.0131\n",
      "Epoch 168:\tTraining Loss - 0.0316\n",
      "Epoch 169:\tTraining Loss - 0.0257\n",
      "Epoch 170:\tTraining Loss - 0.0202\n",
      "Epoch 171:\tTraining Loss - 0.0278\n",
      "Epoch 172:\tTraining Loss - 0.0352\n",
      "Epoch 173:\tTraining Loss - 0.0200\n",
      "Epoch 174:\tTraining Loss - 0.0380\n",
      "Epoch 175:\tTraining Loss - 0.0361\n",
      "Epoch 176:\tTraining Loss - 0.0623\n",
      "Epoch 177:\tTraining Loss - 0.0274\n",
      "Epoch 178:\tTraining Loss - 0.0328\n",
      "Epoch 179:\tTraining Loss - 0.0487\n",
      "Epoch 180:\tTraining Loss - 0.0483\n",
      "Epoch 181:\tTraining Loss - 0.0144\n",
      "Epoch 182:\tTraining Loss - 0.0732\n",
      "Epoch 183:\tTraining Loss - 0.0601\n",
      "Epoch 184:\tTraining Loss - 0.0388\n",
      "Epoch 185:\tTraining Loss - 0.0303\n",
      "Epoch 186:\tTraining Loss - 0.0272\n",
      "Epoch 187:\tTraining Loss - 0.0367\n",
      "Epoch 188:\tTraining Loss - 0.0449\n",
      "Epoch 189:\tTraining Loss - 0.0702\n",
      "Epoch 190:\tTraining Loss - 0.0712\n",
      "Epoch 191:\tTraining Loss - 0.0401\n",
      "Epoch 192:\tTraining Loss - 0.0395\n",
      "Epoch 193:\tTraining Loss - 0.0214\n",
      "Epoch 194:\tTraining Loss - 0.0212\n",
      "Epoch 195:\tTraining Loss - 0.0584\n",
      "Epoch 196:\tTraining Loss - 0.0132\n",
      "Epoch 197:\tTraining Loss - 0.0125\n",
      "Epoch 198:\tTraining Loss - 0.0250\n",
      "Epoch 199:\tTraining Loss - 0.0673\n",
      "Epoch 200:\tTraining Loss - 0.0580\n",
      "Epoch 201:\tTraining Loss - 0.0242\n",
      "Epoch 202:\tTraining Loss - 0.0416\n",
      "Epoch 203:\tTraining Loss - 0.0304\n",
      "Epoch 204:\tTraining Loss - 0.0207\n",
      "Epoch 205:\tTraining Loss - 0.0301\n",
      "Epoch 206:\tTraining Loss - 0.0240\n",
      "Epoch 207:\tTraining Loss - 0.0276\n",
      "Epoch 208:\tTraining Loss - 0.0136\n",
      "Epoch 209:\tTraining Loss - 0.0146\n",
      "Epoch 210:\tTraining Loss - 0.0205\n",
      "Epoch 211:\tTraining Loss - 0.0377\n",
      "Epoch 212:\tTraining Loss - 0.0304\n",
      "Epoch 213:\tTraining Loss - 0.0462\n",
      "Epoch 214:\tTraining Loss - 0.0280\n",
      "Epoch 215:\tTraining Loss - 0.0155\n",
      "Epoch 216:\tTraining Loss - 0.0178\n",
      "Epoch 217:\tTraining Loss - 0.0199\n",
      "Epoch 218:\tTraining Loss - 0.0389\n",
      "Epoch 219:\tTraining Loss - 0.0410\n",
      "Epoch 220:\tTraining Loss - 0.0373\n",
      "Epoch 221:\tTraining Loss - 0.0170\n",
      "Epoch 222:\tTraining Loss - 0.0134\n",
      "Epoch 223:\tTraining Loss - 0.0243\n",
      "Epoch 224:\tTraining Loss - 0.0350\n",
      "Epoch 225:\tTraining Loss - 0.0300\n",
      "Epoch 226:\tTraining Loss - 0.0210\n",
      "Epoch 227:\tTraining Loss - 0.0272\n",
      "Epoch 228:\tTraining Loss - 0.0279\n",
      "Epoch 229:\tTraining Loss - 0.0104\n",
      "Epoch 230:\tTraining Loss - 0.0259\n",
      "Epoch 231:\tTraining Loss - 0.0386\n",
      "Epoch 232:\tTraining Loss - 0.0261\n",
      "Epoch 233:\tTraining Loss - 0.0150\n",
      "Epoch 234:\tTraining Loss - 0.0395\n",
      "Epoch 235:\tTraining Loss - 0.0132\n",
      "Epoch 236:\tTraining Loss - 0.0237\n",
      "Epoch 237:\tTraining Loss - 0.0321\n",
      "Epoch 238:\tTraining Loss - 0.0156\n",
      "Epoch 239:\tTraining Loss - 0.0393\n",
      "Epoch 240:\tTraining Loss - 0.0210\n",
      "Epoch 241:\tTraining Loss - 0.0448\n",
      "Epoch 242:\tTraining Loss - 0.0167\n",
      "Epoch 243:\tTraining Loss - 0.0128\n",
      "Epoch 244:\tTraining Loss - 0.0179\n",
      "Epoch 245:\tTraining Loss - 0.0355\n",
      "Epoch 246:\tTraining Loss - 0.0201\n",
      "Epoch 247:\tTraining Loss - 0.0193\n",
      "Epoch 248:\tTraining Loss - 0.0525\n",
      "Epoch 249:\tTraining Loss - 0.0550\n",
      "Epoch 250:\tTraining Loss - 0.0634\n",
      "Epoch 251:\tTraining Loss - 0.0156\n",
      "Epoch 252:\tTraining Loss - 0.0353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253:\tTraining Loss - 0.0532\n",
      "Epoch 254:\tTraining Loss - 0.0175\n",
      "Epoch 255:\tTraining Loss - 0.0350\n",
      "Epoch 256:\tTraining Loss - 0.0305\n",
      "Epoch 257:\tTraining Loss - 0.0339\n",
      "Epoch 258:\tTraining Loss - 0.0275\n",
      "Epoch 259:\tTraining Loss - 0.0201\n",
      "Epoch 260:\tTraining Loss - 0.0202\n",
      "Epoch 261:\tTraining Loss - 0.0157\n",
      "Epoch 262:\tTraining Loss - 0.0501\n",
      "Epoch 263:\tTraining Loss - 0.0193\n",
      "Epoch 264:\tTraining Loss - 0.0304\n",
      "Epoch 265:\tTraining Loss - 0.0168\n",
      "Epoch 266:\tTraining Loss - 0.0304\n",
      "Epoch 267:\tTraining Loss - 0.0230\n",
      "Epoch 268:\tTraining Loss - 0.0193\n",
      "Epoch 269:\tTraining Loss - 0.0261\n",
      "Epoch 270:\tTraining Loss - 0.0265\n",
      "Epoch 271:\tTraining Loss - 0.0237\n",
      "Epoch 272:\tTraining Loss - 0.0118\n",
      "Epoch 273:\tTraining Loss - 0.0206\n",
      "Epoch 274:\tTraining Loss - 0.0251\n",
      "Epoch 275:\tTraining Loss - 0.0308\n",
      "Epoch 276:\tTraining Loss - 0.0486\n",
      "Epoch 277:\tTraining Loss - 0.0143\n",
      "Epoch 278:\tTraining Loss - 0.0431\n",
      "Epoch 279:\tTraining Loss - 0.0325\n",
      "Epoch 280:\tTraining Loss - 0.0224\n",
      "Epoch 281:\tTraining Loss - 0.0164\n",
      "Epoch 282:\tTraining Loss - 0.0294\n",
      "Epoch 283:\tTraining Loss - 0.0235\n",
      "Epoch 284:\tTraining Loss - 0.0150\n",
      "Epoch 285:\tTraining Loss - 0.0179\n",
      "Epoch 286:\tTraining Loss - 0.0152\n",
      "Epoch 287:\tTraining Loss - 0.0228\n",
      "Epoch 288:\tTraining Loss - 0.0151\n",
      "Epoch 289:\tTraining Loss - 0.0253\n",
      "Epoch 290:\tTraining Loss - 0.0343\n",
      "Epoch 291:\tTraining Loss - 0.0202\n",
      "Epoch 292:\tTraining Loss - 0.0230\n",
      "Epoch 293:\tTraining Loss - 0.0390\n",
      "Epoch 294:\tTraining Loss - 0.0195\n",
      "Epoch 295:\tTraining Loss - 0.0359\n",
      "Epoch 296:\tTraining Loss - 0.0224\n",
      "Epoch 297:\tTraining Loss - 0.0324\n",
      "Epoch 298:\tTraining Loss - 0.0277\n",
      "Epoch 299:\tTraining Loss - 0.0145\n",
      "Epoch 300:\tTraining Loss - 0.0239\n",
      "Epoch 301:\tTraining Loss - 0.0250\n",
      "Epoch 302:\tTraining Loss - 0.0270\n",
      "Epoch 303:\tTraining Loss - 0.0240\n",
      "Epoch 304:\tTraining Loss - 0.0424\n",
      "Epoch 305:\tTraining Loss - 0.0181\n",
      "Epoch 306:\tTraining Loss - 0.0152\n",
      "Epoch 307:\tTraining Loss - 0.0320\n",
      "Epoch 308:\tTraining Loss - 0.0352\n",
      "Epoch 309:\tTraining Loss - 0.0171\n",
      "Epoch 310:\tTraining Loss - 0.0154\n",
      "Epoch 311:\tTraining Loss - 0.0215\n",
      "Epoch 312:\tTraining Loss - 0.0222\n",
      "Epoch 313:\tTraining Loss - 0.0230\n",
      "Epoch 314:\tTraining Loss - 0.0249\n",
      "Epoch 315:\tTraining Loss - 0.0147\n",
      "Epoch 316:\tTraining Loss - 0.0211\n",
      "Epoch 317:\tTraining Loss - 0.0452\n",
      "Epoch 318:\tTraining Loss - 0.0172\n",
      "Epoch 319:\tTraining Loss - 0.0329\n",
      "Epoch 320:\tTraining Loss - 0.0209\n",
      "Epoch 321:\tTraining Loss - 0.0166\n",
      "Epoch 322:\tTraining Loss - 0.0148\n",
      "Epoch 323:\tTraining Loss - 0.0366\n",
      "Epoch 324:\tTraining Loss - 0.0131\n",
      "Epoch 325:\tTraining Loss - 0.0147\n",
      "Epoch 326:\tTraining Loss - 0.0170\n",
      "Epoch 327:\tTraining Loss - 0.0223\n",
      "Epoch 328:\tTraining Loss - 0.0142\n",
      "Epoch 329:\tTraining Loss - 0.0254\n",
      "Epoch 330:\tTraining Loss - 0.0158\n",
      "Epoch 331:\tTraining Loss - 0.0146\n",
      "Epoch 332:\tTraining Loss - 0.0187\n",
      "Epoch 333:\tTraining Loss - 0.0219\n",
      "Epoch 334:\tTraining Loss - 0.0150\n",
      "Epoch 335:\tTraining Loss - 0.0356\n",
      "Epoch 336:\tTraining Loss - 0.0154\n",
      "Epoch 337:\tTraining Loss - 0.0231\n",
      "Epoch 338:\tTraining Loss - 0.0359\n",
      "Epoch 339:\tTraining Loss - 0.0143\n",
      "Epoch 340:\tTraining Loss - 0.0163\n",
      "Epoch 341:\tTraining Loss - 0.0203\n",
      "Epoch 342:\tTraining Loss - 0.0221\n",
      "Epoch 343:\tTraining Loss - 0.0215\n",
      "Epoch 344:\tTraining Loss - 0.0261\n",
      "Epoch 345:\tTraining Loss - 0.0164\n",
      "Epoch 346:\tTraining Loss - 0.0209\n",
      "Epoch 347:\tTraining Loss - 0.0185\n",
      "Epoch 348:\tTraining Loss - 0.0151\n",
      "Epoch 349:\tTraining Loss - 0.0326\n",
      "Epoch 350:\tTraining Loss - 0.0209\n",
      "Epoch 351:\tTraining Loss - 0.0155\n",
      "Epoch 352:\tTraining Loss - 0.0217\n",
      "Epoch 353:\tTraining Loss - 0.0416\n",
      "Epoch 354:\tTraining Loss - 0.0145\n",
      "Epoch 355:\tTraining Loss - 0.0187\n",
      "Epoch 356:\tTraining Loss - 0.0246\n",
      "Epoch 357:\tTraining Loss - 0.0108\n",
      "Epoch 358:\tTraining Loss - 0.0388\n",
      "Epoch 359:\tTraining Loss - 0.0153\n",
      "Epoch 360:\tTraining Loss - 0.0249\n",
      "Epoch 361:\tTraining Loss - 0.0183\n",
      "Epoch 362:\tTraining Loss - 0.0139\n",
      "Epoch 363:\tTraining Loss - 0.0159\n",
      "Epoch 364:\tTraining Loss - 0.0129\n",
      "Epoch 365:\tTraining Loss - 0.0275\n",
      "Epoch 366:\tTraining Loss - 0.0181\n",
      "Epoch 367:\tTraining Loss - 0.0180\n",
      "Epoch 368:\tTraining Loss - 0.0206\n",
      "Epoch 369:\tTraining Loss - 0.0204\n",
      "Epoch 370:\tTraining Loss - 0.0265\n",
      "Epoch 371:\tTraining Loss - 0.0293\n",
      "Epoch 372:\tTraining Loss - 0.0203\n",
      "Epoch 373:\tTraining Loss - 0.0247\n",
      "Epoch 374:\tTraining Loss - 0.0198\n",
      "Epoch 375:\tTraining Loss - 0.0206\n",
      "Epoch 376:\tTraining Loss - 0.0142\n",
      "Epoch 377:\tTraining Loss - 0.0126\n",
      "Epoch 378:\tTraining Loss - 0.0231\n",
      "Epoch 379:\tTraining Loss - 0.0146\n",
      "Epoch 380:\tTraining Loss - 0.0144\n",
      "Epoch 381:\tTraining Loss - 0.0320\n",
      "Epoch 382:\tTraining Loss - 0.0112\n",
      "Epoch 383:\tTraining Loss - 0.0131\n",
      "Epoch 384:\tTraining Loss - 0.0131\n",
      "Epoch 385:\tTraining Loss - 0.0129\n",
      "Epoch 386:\tTraining Loss - 0.0327\n",
      "Epoch 387:\tTraining Loss - 0.0332\n",
      "Epoch 388:\tTraining Loss - 0.0298\n",
      "Epoch 389:\tTraining Loss - 0.0157\n",
      "Epoch 390:\tTraining Loss - 0.0189\n",
      "Epoch 391:\tTraining Loss - 0.0150\n",
      "Epoch 392:\tTraining Loss - 0.0153\n",
      "Epoch 393:\tTraining Loss - 0.0228\n",
      "Epoch 394:\tTraining Loss - 0.0302\n",
      "Epoch 395:\tTraining Loss - 0.0184\n",
      "Epoch 396:\tTraining Loss - 0.0296\n",
      "Epoch 397:\tTraining Loss - 0.0237\n",
      "Epoch 398:\tTraining Loss - 0.0222\n",
      "Epoch 399:\tTraining Loss - 0.0153\n",
      "Epoch 400:\tTraining Loss - 0.0166\n",
      "Epoch 401:\tTraining Loss - 0.0254\n",
      "Epoch 402:\tTraining Loss - 0.0225\n",
      "Epoch 403:\tTraining Loss - 0.0169\n",
      "Epoch 404:\tTraining Loss - 0.0091\n",
      "Epoch 405:\tTraining Loss - 0.0326\n",
      "Epoch 406:\tTraining Loss - 0.0080\n",
      "Epoch 407:\tTraining Loss - 0.0129\n",
      "Epoch 408:\tTraining Loss - 0.0300\n",
      "Epoch 409:\tTraining Loss - 0.0224\n",
      "Epoch 410:\tTraining Loss - 0.0102\n",
      "Epoch 411:\tTraining Loss - 0.0139\n",
      "Epoch 412:\tTraining Loss - 0.0174\n",
      "Epoch 413:\tTraining Loss - 0.0144\n",
      "Epoch 414:\tTraining Loss - 0.0204\n",
      "Epoch 415:\tTraining Loss - 0.0141\n",
      "Epoch 416:\tTraining Loss - 0.0189\n",
      "Epoch 417:\tTraining Loss - 0.0180\n",
      "Epoch 418:\tTraining Loss - 0.0196\n",
      "Epoch 419:\tTraining Loss - 0.0132\n",
      "Epoch 420:\tTraining Loss - 0.0180\n",
      "Epoch 421:\tTraining Loss - 0.0135\n",
      "Epoch 422:\tTraining Loss - 0.0142\n",
      "Epoch 423:\tTraining Loss - 0.0239\n",
      "Epoch 424:\tTraining Loss - 0.0152\n",
      "Epoch 425:\tTraining Loss - 0.0131\n",
      "Epoch 426:\tTraining Loss - 0.0194\n",
      "Epoch 427:\tTraining Loss - 0.0126\n",
      "Epoch 428:\tTraining Loss - 0.0178\n",
      "Epoch 429:\tTraining Loss - 0.0266\n",
      "Epoch 430:\tTraining Loss - 0.0189\n",
      "Epoch 431:\tTraining Loss - 0.0204\n",
      "Epoch 432:\tTraining Loss - 0.0136\n",
      "Epoch 433:\tTraining Loss - 0.0174\n",
      "Epoch 434:\tTraining Loss - 0.0124\n",
      "Epoch 435:\tTraining Loss - 0.0202\n",
      "Epoch 436:\tTraining Loss - 0.0077\n",
      "Epoch 437:\tTraining Loss - 0.0200\n",
      "Epoch 438:\tTraining Loss - 0.0217\n",
      "Epoch 439:\tTraining Loss - 0.0229\n",
      "Epoch 440:\tTraining Loss - 0.0146\n",
      "Epoch 441:\tTraining Loss - 0.0171\n",
      "Epoch 442:\tTraining Loss - 0.0126\n",
      "Epoch 443:\tTraining Loss - 0.0169\n",
      "Epoch 444:\tTraining Loss - 0.0157\n",
      "Epoch 445:\tTraining Loss - 0.0206\n",
      "Epoch 446:\tTraining Loss - 0.0271\n",
      "Epoch 447:\tTraining Loss - 0.0160\n",
      "Epoch 448:\tTraining Loss - 0.0139\n",
      "Epoch 449:\tTraining Loss - 0.0101\n",
      "Epoch 450:\tTraining Loss - 0.0119\n",
      "Epoch 451:\tTraining Loss - 0.0293\n",
      "Epoch 452:\tTraining Loss - 0.0361\n",
      "Epoch 453:\tTraining Loss - 0.0200\n",
      "Epoch 454:\tTraining Loss - 0.0184\n",
      "Epoch 455:\tTraining Loss - 0.0203\n",
      "Epoch 456:\tTraining Loss - 0.0116\n",
      "Epoch 457:\tTraining Loss - 0.0116\n",
      "Epoch 458:\tTraining Loss - 0.0195\n",
      "Epoch 459:\tTraining Loss - 0.0197\n",
      "Epoch 460:\tTraining Loss - 0.0191\n",
      "Epoch 461:\tTraining Loss - 0.0164\n",
      "Epoch 462:\tTraining Loss - 0.0165\n",
      "Epoch 463:\tTraining Loss - 0.0115\n",
      "Epoch 464:\tTraining Loss - 0.0138\n",
      "Epoch 465:\tTraining Loss - 0.0134\n",
      "Epoch 466:\tTraining Loss - 0.0136\n",
      "Epoch 467:\tTraining Loss - 0.0161\n",
      "Epoch 468:\tTraining Loss - 0.0208\n",
      "Epoch 469:\tTraining Loss - 0.0144\n",
      "Epoch 470:\tTraining Loss - 0.0142\n",
      "Epoch 471:\tTraining Loss - 0.0168\n",
      "Epoch 472:\tTraining Loss - 0.0119\n",
      "Epoch 473:\tTraining Loss - 0.0105\n",
      "Epoch 474:\tTraining Loss - 0.0154\n",
      "Epoch 475:\tTraining Loss - 0.0091\n",
      "Epoch 476:\tTraining Loss - 0.0153\n",
      "Epoch 477:\tTraining Loss - 0.0333\n",
      "Epoch 478:\tTraining Loss - 0.0112\n",
      "Epoch 479:\tTraining Loss - 0.0183\n",
      "Epoch 480:\tTraining Loss - 0.0127\n",
      "Epoch 481:\tTraining Loss - 0.0280\n",
      "Epoch 482:\tTraining Loss - 0.0102\n",
      "Epoch 483:\tTraining Loss - 0.0140\n",
      "Epoch 484:\tTraining Loss - 0.0150\n",
      "Epoch 485:\tTraining Loss - 0.0167\n",
      "Epoch 486:\tTraining Loss - 0.0218\n",
      "Epoch 487:\tTraining Loss - 0.0133\n",
      "Epoch 488:\tTraining Loss - 0.0160\n",
      "Epoch 489:\tTraining Loss - 0.0126\n",
      "Epoch 490:\tTraining Loss - 0.0167\n",
      "Epoch 491:\tTraining Loss - 0.0127\n",
      "Epoch 492:\tTraining Loss - 0.0200\n",
      "Epoch 493:\tTraining Loss - 0.0168\n",
      "Epoch 494:\tTraining Loss - 0.0158\n",
      "Epoch 495:\tTraining Loss - 0.0117\n",
      "Epoch 496:\tTraining Loss - 0.0139\n",
      "Epoch 497:\tTraining Loss - 0.0119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498:\tTraining Loss - 0.0094\n",
      "Epoch 499:\tTraining Loss - 0.0102\n",
      "Epoch 500:\tTraining Loss - 0.0111\n",
      "Epoch 501:\tTraining Loss - 0.0227\n",
      "Epoch 502:\tTraining Loss - 0.0166\n",
      "Epoch 503:\tTraining Loss - 0.0073\n",
      "Epoch 504:\tTraining Loss - 0.0068\n",
      "Epoch 505:\tTraining Loss - 0.0140\n",
      "Epoch 506:\tTraining Loss - 0.0106\n",
      "Epoch 507:\tTraining Loss - 0.0069\n",
      "Epoch 508:\tTraining Loss - 0.0196\n",
      "Epoch 509:\tTraining Loss - 0.0151\n",
      "Epoch 510:\tTraining Loss - 0.0132\n",
      "Epoch 511:\tTraining Loss - 0.0191\n",
      "Epoch 512:\tTraining Loss - 0.0111\n",
      "Epoch 513:\tTraining Loss - 0.0170\n",
      "Epoch 514:\tTraining Loss - 0.0168\n",
      "Epoch 515:\tTraining Loss - 0.0137\n",
      "Epoch 516:\tTraining Loss - 0.0129\n",
      "Epoch 517:\tTraining Loss - 0.0120\n",
      "Epoch 518:\tTraining Loss - 0.0123\n",
      "Epoch 519:\tTraining Loss - 0.0159\n",
      "Epoch 520:\tTraining Loss - 0.0224\n",
      "Epoch 521:\tTraining Loss - 0.0132\n",
      "Epoch 522:\tTraining Loss - 0.0135\n",
      "Epoch 523:\tTraining Loss - 0.0094\n",
      "Epoch 524:\tTraining Loss - 0.0220\n",
      "Epoch 525:\tTraining Loss - 0.0120\n",
      "Epoch 526:\tTraining Loss - 0.0180\n",
      "Epoch 527:\tTraining Loss - 0.0145\n",
      "Epoch 528:\tTraining Loss - 0.0079\n",
      "Epoch 529:\tTraining Loss - 0.0096\n",
      "Epoch 530:\tTraining Loss - 0.0137\n",
      "Epoch 531:\tTraining Loss - 0.0207\n",
      "Epoch 532:\tTraining Loss - 0.0199\n",
      "Epoch 533:\tTraining Loss - 0.0119\n",
      "Epoch 534:\tTraining Loss - 0.0121\n",
      "Epoch 535:\tTraining Loss - 0.0140\n",
      "Epoch 536:\tTraining Loss - 0.0138\n",
      "Epoch 537:\tTraining Loss - 0.0227\n",
      "Epoch 538:\tTraining Loss - 0.0139\n",
      "Epoch 539:\tTraining Loss - 0.0128\n",
      "Epoch 540:\tTraining Loss - 0.0132\n",
      "Epoch 541:\tTraining Loss - 0.0053\n",
      "Epoch 542:\tTraining Loss - 0.0183\n",
      "Epoch 543:\tTraining Loss - 0.0181\n",
      "Epoch 544:\tTraining Loss - 0.0149\n",
      "Epoch 545:\tTraining Loss - 0.0064\n",
      "Epoch 546:\tTraining Loss - 0.0063\n",
      "Epoch 547:\tTraining Loss - 0.0177\n",
      "Epoch 548:\tTraining Loss - 0.0188\n",
      "Epoch 549:\tTraining Loss - 0.0090\n",
      "Epoch 550:\tTraining Loss - 0.0170\n",
      "Epoch 551:\tTraining Loss - 0.0162\n",
      "Epoch 552:\tTraining Loss - 0.0187\n",
      "Epoch 553:\tTraining Loss - 0.0209\n",
      "Epoch 554:\tTraining Loss - 0.0096\n",
      "Epoch 555:\tTraining Loss - 0.0104\n",
      "Epoch 556:\tTraining Loss - 0.0166\n",
      "Epoch 557:\tTraining Loss - 0.0128\n",
      "Epoch 558:\tTraining Loss - 0.0147\n",
      "Epoch 559:\tTraining Loss - 0.0116\n",
      "Epoch 560:\tTraining Loss - 0.0126\n",
      "Epoch 561:\tTraining Loss - 0.0156\n",
      "Epoch 562:\tTraining Loss - 0.0147\n",
      "Epoch 563:\tTraining Loss - 0.0173\n",
      "Epoch 564:\tTraining Loss - 0.0139\n",
      "Epoch 565:\tTraining Loss - 0.0124\n",
      "Epoch 566:\tTraining Loss - 0.0131\n",
      "Epoch 567:\tTraining Loss - 0.0102\n",
      "Epoch 568:\tTraining Loss - 0.0173\n",
      "Epoch 569:\tTraining Loss - 0.0182\n",
      "Epoch 570:\tTraining Loss - 0.0183\n",
      "Epoch 571:\tTraining Loss - 0.0098\n",
      "Epoch 572:\tTraining Loss - 0.0096\n",
      "Epoch 573:\tTraining Loss - 0.0117\n",
      "Epoch 574:\tTraining Loss - 0.0094\n",
      "Epoch 575:\tTraining Loss - 0.0209\n",
      "Epoch 576:\tTraining Loss - 0.0058\n",
      "Epoch 577:\tTraining Loss - 0.0107\n",
      "Epoch 578:\tTraining Loss - 0.0117\n",
      "Epoch 579:\tTraining Loss - 0.0139\n",
      "Epoch 580:\tTraining Loss - 0.0185\n",
      "Epoch 581:\tTraining Loss - 0.0106\n",
      "Epoch 582:\tTraining Loss - 0.0113\n",
      "Epoch 583:\tTraining Loss - 0.0111\n",
      "Epoch 584:\tTraining Loss - 0.0117\n",
      "Epoch 585:\tTraining Loss - 0.0104\n",
      "Epoch 586:\tTraining Loss - 0.0139\n",
      "Epoch 587:\tTraining Loss - 0.0098\n",
      "Epoch 588:\tTraining Loss - 0.0116\n",
      "Epoch 589:\tTraining Loss - 0.0117\n",
      "Epoch 590:\tTraining Loss - 0.0089\n",
      "Epoch 591:\tTraining Loss - 0.0070\n",
      "Epoch 592:\tTraining Loss - 0.0141\n",
      "Epoch 593:\tTraining Loss - 0.0092\n",
      "Epoch 594:\tTraining Loss - 0.0141\n",
      "Epoch 595:\tTraining Loss - 0.0196\n",
      "Epoch 596:\tTraining Loss - 0.0105\n",
      "Epoch 597:\tTraining Loss - 0.0081\n",
      "Epoch 598:\tTraining Loss - 0.0091\n",
      "Epoch 599:\tTraining Loss - 0.0152\n",
      "Epoch 600:\tTraining Loss - 0.0150\n",
      "Epoch 601:\tTraining Loss - 0.0160\n",
      "Epoch 602:\tTraining Loss - 0.0049\n",
      "Epoch 603:\tTraining Loss - 0.0103\n",
      "Epoch 604:\tTraining Loss - 0.0121\n",
      "Epoch 605:\tTraining Loss - 0.0115\n",
      "Epoch 606:\tTraining Loss - 0.0115\n",
      "Epoch 607:\tTraining Loss - 0.0116\n",
      "Epoch 608:\tTraining Loss - 0.0070\n",
      "Epoch 609:\tTraining Loss - 0.0279\n",
      "Epoch 610:\tTraining Loss - 0.0152\n",
      "Epoch 611:\tTraining Loss - 0.0198\n",
      "Epoch 612:\tTraining Loss - 0.0148\n",
      "Epoch 613:\tTraining Loss - 0.0081\n",
      "Epoch 614:\tTraining Loss - 0.0079\n",
      "Epoch 615:\tTraining Loss - 0.0121\n",
      "Epoch 616:\tTraining Loss - 0.0179\n",
      "Epoch 617:\tTraining Loss - 0.0118\n",
      "Epoch 618:\tTraining Loss - 0.0088\n",
      "Epoch 619:\tTraining Loss - 0.0104\n",
      "Epoch 620:\tTraining Loss - 0.0093\n",
      "Epoch 621:\tTraining Loss - 0.0126\n",
      "Epoch 622:\tTraining Loss - 0.0117\n",
      "Epoch 623:\tTraining Loss - 0.0109\n",
      "Epoch 624:\tTraining Loss - 0.0201\n",
      "Epoch 625:\tTraining Loss - 0.0093\n",
      "Epoch 626:\tTraining Loss - 0.0151\n",
      "Epoch 627:\tTraining Loss - 0.0090\n",
      "Epoch 628:\tTraining Loss - 0.0155\n",
      "Epoch 629:\tTraining Loss - 0.0130\n",
      "Epoch 630:\tTraining Loss - 0.0165\n",
      "Epoch 631:\tTraining Loss - 0.0146\n",
      "Epoch 632:\tTraining Loss - 0.0101\n",
      "Epoch 633:\tTraining Loss - 0.0188\n",
      "Epoch 634:\tTraining Loss - 0.0168\n",
      "Epoch 635:\tTraining Loss - 0.0088\n",
      "Epoch 636:\tTraining Loss - 0.0148\n",
      "Epoch 637:\tTraining Loss - 0.0112\n",
      "Epoch 638:\tTraining Loss - 0.0236\n",
      "Epoch 639:\tTraining Loss - 0.0187\n",
      "Epoch 640:\tTraining Loss - 0.0189\n",
      "Epoch 641:\tTraining Loss - 0.0166\n",
      "Epoch 642:\tTraining Loss - 0.0108\n",
      "Epoch 643:\tTraining Loss - 0.0145\n",
      "Epoch 644:\tTraining Loss - 0.0174\n",
      "Epoch 645:\tTraining Loss - 0.0179\n",
      "Epoch 646:\tTraining Loss - 0.0110\n",
      "Epoch 647:\tTraining Loss - 0.0164\n",
      "Epoch 648:\tTraining Loss - 0.0139\n",
      "Epoch 649:\tTraining Loss - 0.0123\n",
      "Epoch 650:\tTraining Loss - 0.0146\n",
      "Epoch 651:\tTraining Loss - 0.0146\n",
      "Epoch 652:\tTraining Loss - 0.0141\n",
      "Epoch 653:\tTraining Loss - 0.0165\n",
      "Epoch 654:\tTraining Loss - 0.0132\n",
      "Epoch 655:\tTraining Loss - 0.0143\n",
      "Epoch 656:\tTraining Loss - 0.0107\n",
      "Epoch 657:\tTraining Loss - 0.0097\n",
      "Epoch 658:\tTraining Loss - 0.0125\n",
      "Epoch 659:\tTraining Loss - 0.0075\n",
      "Epoch 660:\tTraining Loss - 0.0116\n",
      "Epoch 661:\tTraining Loss - 0.0135\n",
      "Epoch 662:\tTraining Loss - 0.0148\n",
      "Epoch 663:\tTraining Loss - 0.0079\n",
      "Epoch 664:\tTraining Loss - 0.0195\n",
      "Epoch 665:\tTraining Loss - 0.0071\n",
      "Epoch 666:\tTraining Loss - 0.0107\n",
      "Epoch 667:\tTraining Loss - 0.0105\n",
      "Epoch 668:\tTraining Loss - 0.0170\n",
      "Epoch 669:\tTraining Loss - 0.0165\n",
      "Epoch 670:\tTraining Loss - 0.0152\n",
      "Epoch 671:\tTraining Loss - 0.0084\n",
      "Epoch 672:\tTraining Loss - 0.0075\n",
      "Epoch 673:\tTraining Loss - 0.0180\n",
      "Epoch 674:\tTraining Loss - 0.0154\n",
      "Epoch 675:\tTraining Loss - 0.0074\n",
      "Epoch 676:\tTraining Loss - 0.0147\n",
      "Epoch 677:\tTraining Loss - 0.0071\n",
      "Epoch 678:\tTraining Loss - 0.0138\n",
      "Epoch 679:\tTraining Loss - 0.0145\n",
      "Epoch 680:\tTraining Loss - 0.0108\n",
      "Epoch 681:\tTraining Loss - 0.0089\n",
      "Epoch 682:\tTraining Loss - 0.0104\n",
      "Epoch 683:\tTraining Loss - 0.0117\n",
      "Epoch 684:\tTraining Loss - 0.0138\n",
      "Epoch 685:\tTraining Loss - 0.0165\n",
      "Epoch 686:\tTraining Loss - 0.0112\n",
      "Epoch 687:\tTraining Loss - 0.0221\n",
      "Epoch 688:\tTraining Loss - 0.0132\n",
      "Epoch 689:\tTraining Loss - 0.0105\n",
      "Epoch 690:\tTraining Loss - 0.0102\n",
      "Epoch 691:\tTraining Loss - 0.0148\n",
      "Epoch 692:\tTraining Loss - 0.0167\n",
      "Epoch 693:\tTraining Loss - 0.0159\n",
      "Epoch 694:\tTraining Loss - 0.0138\n",
      "Epoch 695:\tTraining Loss - 0.0117\n",
      "Epoch 696:\tTraining Loss - 0.0094\n",
      "Epoch 697:\tTraining Loss - 0.0119\n",
      "Epoch 698:\tTraining Loss - 0.0106\n",
      "Epoch 699:\tTraining Loss - 0.0094\n",
      "Epoch 700:\tTraining Loss - 0.0144\n",
      "Epoch 701:\tTraining Loss - 0.0076\n",
      "Epoch 702:\tTraining Loss - 0.0091\n",
      "Epoch 703:\tTraining Loss - 0.0091\n",
      "Epoch 704:\tTraining Loss - 0.0122\n",
      "Epoch 705:\tTraining Loss - 0.0143\n",
      "Epoch 706:\tTraining Loss - 0.0112\n",
      "Epoch 707:\tTraining Loss - 0.0090\n",
      "Epoch 708:\tTraining Loss - 0.0076\n",
      "Epoch 709:\tTraining Loss - 0.0123\n",
      "Epoch 710:\tTraining Loss - 0.0130\n",
      "Epoch 711:\tTraining Loss - 0.0091\n",
      "Epoch 712:\tTraining Loss - 0.0174\n",
      "Epoch 713:\tTraining Loss - 0.0144\n",
      "Epoch 714:\tTraining Loss - 0.0157\n",
      "Epoch 715:\tTraining Loss - 0.0127\n",
      "Epoch 716:\tTraining Loss - 0.0097\n",
      "Epoch 717:\tTraining Loss - 0.0116\n",
      "Epoch 718:\tTraining Loss - 0.0091\n",
      "Epoch 719:\tTraining Loss - 0.0102\n",
      "Epoch 720:\tTraining Loss - 0.0163\n",
      "Epoch 721:\tTraining Loss - 0.0131\n",
      "Epoch 722:\tTraining Loss - 0.0124\n",
      "Epoch 723:\tTraining Loss - 0.0143\n",
      "Epoch 724:\tTraining Loss - 0.0090\n",
      "Epoch 725:\tTraining Loss - 0.0061\n",
      "Epoch 726:\tTraining Loss - 0.0095\n",
      "Epoch 727:\tTraining Loss - 0.0091\n",
      "Epoch 728:\tTraining Loss - 0.0107\n",
      "Epoch 729:\tTraining Loss - 0.0053\n",
      "Epoch 730:\tTraining Loss - 0.0093\n",
      "Epoch 731:\tTraining Loss - 0.0076\n",
      "Epoch 732:\tTraining Loss - 0.0151\n",
      "Epoch 733:\tTraining Loss - 0.0063\n",
      "Epoch 734:\tTraining Loss - 0.0203\n",
      "Epoch 735:\tTraining Loss - 0.0133\n",
      "Epoch 736:\tTraining Loss - 0.0094\n",
      "Epoch 737:\tTraining Loss - 0.0129\n",
      "Epoch 738:\tTraining Loss - 0.0093\n",
      "Epoch 739:\tTraining Loss - 0.0062\n",
      "Epoch 740:\tTraining Loss - 0.0121\n",
      "Epoch 741:\tTraining Loss - 0.0139\n",
      "Epoch 742:\tTraining Loss - 0.0167\n",
      "Epoch 743:\tTraining Loss - 0.0116\n",
      "Epoch 744:\tTraining Loss - 0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 745:\tTraining Loss - 0.0120\n",
      "Epoch 746:\tTraining Loss - 0.0221\n",
      "Epoch 747:\tTraining Loss - 0.0170\n",
      "Epoch 748:\tTraining Loss - 0.0200\n",
      "Epoch 749:\tTraining Loss - 0.0129\n",
      "Epoch 750:\tTraining Loss - 0.0133\n",
      "Epoch 751:\tTraining Loss - 0.0099\n",
      "Epoch 752:\tTraining Loss - 0.0115\n",
      "Epoch 753:\tTraining Loss - 0.0123\n",
      "Epoch 754:\tTraining Loss - 0.0077\n",
      "Epoch 755:\tTraining Loss - 0.0109\n",
      "Epoch 756:\tTraining Loss - 0.0084\n",
      "Epoch 757:\tTraining Loss - 0.0124\n",
      "Epoch 758:\tTraining Loss - 0.0089\n",
      "Epoch 759:\tTraining Loss - 0.0170\n",
      "Epoch 760:\tTraining Loss - 0.0097\n",
      "Epoch 761:\tTraining Loss - 0.0093\n",
      "Epoch 762:\tTraining Loss - 0.0057\n",
      "Epoch 763:\tTraining Loss - 0.0218\n",
      "Epoch 764:\tTraining Loss - 0.0101\n",
      "Epoch 765:\tTraining Loss - 0.0141\n",
      "Epoch 766:\tTraining Loss - 0.0136\n",
      "Epoch 767:\tTraining Loss - 0.0054\n",
      "Epoch 768:\tTraining Loss - 0.0153\n",
      "Epoch 769:\tTraining Loss - 0.0056\n",
      "Epoch 770:\tTraining Loss - 0.0098\n",
      "Epoch 771:\tTraining Loss - 0.0131\n",
      "Epoch 772:\tTraining Loss - 0.0108\n",
      "Epoch 773:\tTraining Loss - 0.0060\n",
      "Epoch 774:\tTraining Loss - 0.0124\n",
      "Epoch 775:\tTraining Loss - 0.0137\n",
      "Epoch 776:\tTraining Loss - 0.0109\n",
      "Epoch 777:\tTraining Loss - 0.0063\n",
      "Epoch 778:\tTraining Loss - 0.0091\n",
      "Epoch 779:\tTraining Loss - 0.0095\n",
      "Epoch 780:\tTraining Loss - 0.0079\n",
      "Epoch 781:\tTraining Loss - 0.0138\n",
      "Epoch 782:\tTraining Loss - 0.0081\n",
      "Epoch 783:\tTraining Loss - 0.0096\n",
      "Epoch 784:\tTraining Loss - 0.0088\n",
      "Epoch 785:\tTraining Loss - 0.0078\n",
      "Epoch 786:\tTraining Loss - 0.0134\n",
      "Epoch 787:\tTraining Loss - 0.0104\n",
      "Epoch 788:\tTraining Loss - 0.0144\n",
      "Epoch 789:\tTraining Loss - 0.0096\n",
      "Epoch 790:\tTraining Loss - 0.0099\n",
      "Epoch 791:\tTraining Loss - 0.0207\n",
      "Epoch 792:\tTraining Loss - 0.0122\n",
      "Epoch 793:\tTraining Loss - 0.0067\n",
      "Epoch 794:\tTraining Loss - 0.0127\n",
      "Epoch 795:\tTraining Loss - 0.0122\n",
      "Epoch 796:\tTraining Loss - 0.0126\n",
      "Epoch 797:\tTraining Loss - 0.0054\n",
      "Epoch 798:\tTraining Loss - 0.0129\n",
      "Epoch 799:\tTraining Loss - 0.0124\n",
      "Epoch 800:\tTraining Loss - 0.0117\n",
      "Epoch 801:\tTraining Loss - 0.0056\n",
      "Epoch 802:\tTraining Loss - 0.0219\n",
      "Epoch 803:\tTraining Loss - 0.0095\n",
      "Epoch 804:\tTraining Loss - 0.0138\n",
      "Epoch 805:\tTraining Loss - 0.0122\n",
      "Epoch 806:\tTraining Loss - 0.0136\n",
      "Epoch 807:\tTraining Loss - 0.0115\n",
      "Epoch 808:\tTraining Loss - 0.0066\n",
      "Epoch 809:\tTraining Loss - 0.0101\n",
      "Epoch 810:\tTraining Loss - 0.0138\n",
      "Epoch 811:\tTraining Loss - 0.0146\n",
      "Epoch 812:\tTraining Loss - 0.0068\n",
      "Epoch 813:\tTraining Loss - 0.0065\n",
      "Epoch 814:\tTraining Loss - 0.0077\n",
      "Epoch 815:\tTraining Loss - 0.0129\n",
      "Epoch 816:\tTraining Loss - 0.0126\n",
      "Epoch 817:\tTraining Loss - 0.0057\n",
      "Epoch 818:\tTraining Loss - 0.0187\n",
      "Epoch 819:\tTraining Loss - 0.0075\n",
      "Epoch 820:\tTraining Loss - 0.0081\n",
      "Epoch 821:\tTraining Loss - 0.0184\n",
      "Epoch 822:\tTraining Loss - 0.0100\n",
      "Epoch 823:\tTraining Loss - 0.0106\n",
      "Epoch 824:\tTraining Loss - 0.0135\n",
      "Epoch 825:\tTraining Loss - 0.0127\n",
      "Epoch 826:\tTraining Loss - 0.0096\n",
      "Epoch 827:\tTraining Loss - 0.0102\n",
      "Epoch 828:\tTraining Loss - 0.0079\n",
      "Epoch 829:\tTraining Loss - 0.0088\n",
      "Epoch 830:\tTraining Loss - 0.0068\n",
      "Epoch 831:\tTraining Loss - 0.0106\n",
      "Epoch 832:\tTraining Loss - 0.0105\n",
      "Epoch 833:\tTraining Loss - 0.0195\n",
      "Epoch 834:\tTraining Loss - 0.0179\n",
      "Epoch 835:\tTraining Loss - 0.0115\n",
      "Epoch 836:\tTraining Loss - 0.0109\n",
      "Epoch 837:\tTraining Loss - 0.0086\n",
      "Epoch 838:\tTraining Loss - 0.0064\n",
      "Epoch 839:\tTraining Loss - 0.0102\n",
      "Epoch 840:\tTraining Loss - 0.0101\n",
      "Epoch 841:\tTraining Loss - 0.0084\n",
      "Epoch 842:\tTraining Loss - 0.0121\n",
      "Epoch 843:\tTraining Loss - 0.0085\n",
      "Epoch 844:\tTraining Loss - 0.0088\n",
      "Epoch 845:\tTraining Loss - 0.0115\n",
      "Epoch 846:\tTraining Loss - 0.0097\n",
      "Epoch 847:\tTraining Loss - 0.0093\n",
      "Epoch 848:\tTraining Loss - 0.0107\n",
      "Epoch 849:\tTraining Loss - 0.0095\n",
      "Epoch 850:\tTraining Loss - 0.0109\n",
      "Epoch 851:\tTraining Loss - 0.0094\n",
      "Epoch 852:\tTraining Loss - 0.0129\n",
      "Epoch 853:\tTraining Loss - 0.0119\n",
      "Epoch 854:\tTraining Loss - 0.0090\n",
      "Epoch 855:\tTraining Loss - 0.0111\n",
      "Epoch 856:\tTraining Loss - 0.0082\n",
      "Epoch 857:\tTraining Loss - 0.0086\n",
      "Epoch 858:\tTraining Loss - 0.0119\n",
      "Epoch 859:\tTraining Loss - 0.0088\n",
      "Epoch 860:\tTraining Loss - 0.0140\n",
      "Epoch 861:\tTraining Loss - 0.0132\n",
      "Epoch 862:\tTraining Loss - 0.0138\n",
      "Epoch 863:\tTraining Loss - 0.0079\n",
      "Epoch 864:\tTraining Loss - 0.0107\n",
      "Epoch 865:\tTraining Loss - 0.0099\n",
      "Epoch 866:\tTraining Loss - 0.0109\n",
      "Epoch 867:\tTraining Loss - 0.0083\n",
      "Epoch 868:\tTraining Loss - 0.0093\n",
      "Epoch 869:\tTraining Loss - 0.0121\n",
      "Epoch 870:\tTraining Loss - 0.0077\n",
      "Epoch 871:\tTraining Loss - 0.0104\n",
      "Epoch 872:\tTraining Loss - 0.0097\n",
      "Epoch 873:\tTraining Loss - 0.0096\n",
      "Epoch 874:\tTraining Loss - 0.0101\n",
      "Epoch 875:\tTraining Loss - 0.0116\n",
      "Epoch 876:\tTraining Loss - 0.0072\n",
      "Epoch 877:\tTraining Loss - 0.0089\n",
      "Epoch 878:\tTraining Loss - 0.0088\n",
      "Epoch 879:\tTraining Loss - 0.0104\n",
      "Epoch 880:\tTraining Loss - 0.0111\n",
      "Epoch 881:\tTraining Loss - 0.0079\n",
      "Epoch 882:\tTraining Loss - 0.0106\n",
      "Epoch 883:\tTraining Loss - 0.0074\n",
      "Epoch 884:\tTraining Loss - 0.0111\n",
      "Epoch 885:\tTraining Loss - 0.0068\n",
      "Epoch 886:\tTraining Loss - 0.0097\n",
      "Epoch 887:\tTraining Loss - 0.0105\n",
      "Epoch 888:\tTraining Loss - 0.0132\n",
      "Epoch 889:\tTraining Loss - 0.0120\n",
      "Epoch 890:\tTraining Loss - 0.0096\n",
      "Epoch 891:\tTraining Loss - 0.0086\n",
      "Epoch 892:\tTraining Loss - 0.0156\n",
      "Epoch 893:\tTraining Loss - 0.0093\n",
      "Epoch 894:\tTraining Loss - 0.0142\n",
      "Epoch 895:\tTraining Loss - 0.0084\n",
      "Epoch 896:\tTraining Loss - 0.0166\n",
      "Epoch 897:\tTraining Loss - 0.0113\n",
      "Epoch 898:\tTraining Loss - 0.0090\n",
      "Epoch 899:\tTraining Loss - 0.0105\n",
      "Epoch 900:\tTraining Loss - 0.0124\n",
      "Epoch 901:\tTraining Loss - 0.0136\n",
      "Epoch 902:\tTraining Loss - 0.0078\n",
      "Epoch 903:\tTraining Loss - 0.0178\n",
      "Epoch 904:\tTraining Loss - 0.0071\n",
      "Epoch 905:\tTraining Loss - 0.0139\n",
      "Epoch 906:\tTraining Loss - 0.0102\n",
      "Epoch 907:\tTraining Loss - 0.0111\n",
      "Epoch 908:\tTraining Loss - 0.0068\n",
      "Epoch 909:\tTraining Loss - 0.0116\n",
      "Epoch 910:\tTraining Loss - 0.0144\n",
      "Epoch 911:\tTraining Loss - 0.0089\n",
      "Epoch 912:\tTraining Loss - 0.0086\n",
      "Epoch 913:\tTraining Loss - 0.0128\n",
      "Epoch 914:\tTraining Loss - 0.0086\n",
      "Epoch 915:\tTraining Loss - 0.0087\n",
      "Epoch 916:\tTraining Loss - 0.0109\n",
      "Epoch 917:\tTraining Loss - 0.0096\n",
      "Epoch 918:\tTraining Loss - 0.0089\n",
      "Epoch 919:\tTraining Loss - 0.0151\n",
      "Epoch 920:\tTraining Loss - 0.0097\n",
      "Epoch 921:\tTraining Loss - 0.0118\n",
      "Epoch 922:\tTraining Loss - 0.0168\n",
      "Epoch 923:\tTraining Loss - 0.0097\n",
      "Epoch 924:\tTraining Loss - 0.0107\n",
      "Epoch 925:\tTraining Loss - 0.0114\n",
      "Epoch 926:\tTraining Loss - 0.0081\n",
      "Epoch 927:\tTraining Loss - 0.0108\n",
      "Epoch 928:\tTraining Loss - 0.0131\n",
      "Epoch 929:\tTraining Loss - 0.0099\n",
      "Epoch 930:\tTraining Loss - 0.0127\n",
      "Epoch 931:\tTraining Loss - 0.0083\n",
      "Epoch 932:\tTraining Loss - 0.0103\n",
      "Epoch 933:\tTraining Loss - 0.0068\n",
      "Epoch 934:\tTraining Loss - 0.0085\n",
      "Epoch 935:\tTraining Loss - 0.0143\n",
      "Epoch 936:\tTraining Loss - 0.0082\n",
      "Epoch 937:\tTraining Loss - 0.0089\n",
      "Epoch 938:\tTraining Loss - 0.0130\n",
      "Epoch 939:\tTraining Loss - 0.0064\n",
      "Epoch 940:\tTraining Loss - 0.0110\n",
      "Epoch 941:\tTraining Loss - 0.0119\n",
      "Epoch 942:\tTraining Loss - 0.0192\n",
      "Epoch 943:\tTraining Loss - 0.0164\n",
      "Epoch 944:\tTraining Loss - 0.0155\n",
      "Epoch 945:\tTraining Loss - 0.0151\n",
      "Epoch 946:\tTraining Loss - 0.0097\n",
      "Epoch 947:\tTraining Loss - 0.0158\n",
      "Epoch 948:\tTraining Loss - 0.0108\n",
      "Epoch 949:\tTraining Loss - 0.0193\n",
      "Epoch 950:\tTraining Loss - 0.0086\n",
      "Epoch 951:\tTraining Loss - 0.0114\n",
      "Epoch 952:\tTraining Loss - 0.0128\n",
      "Epoch 953:\tTraining Loss - 0.0111\n",
      "Epoch 954:\tTraining Loss - 0.0135\n",
      "Epoch 955:\tTraining Loss - 0.0114\n",
      "Epoch 956:\tTraining Loss - 0.0104\n",
      "Epoch 957:\tTraining Loss - 0.0084\n",
      "Epoch 958:\tTraining Loss - 0.0116\n",
      "Epoch 959:\tTraining Loss - 0.0088\n",
      "Epoch 960:\tTraining Loss - 0.0106\n",
      "Epoch 961:\tTraining Loss - 0.0166\n",
      "Epoch 962:\tTraining Loss - 0.0167\n",
      "Epoch 963:\tTraining Loss - 0.0101\n",
      "Epoch 964:\tTraining Loss - 0.0084\n",
      "Epoch 965:\tTraining Loss - 0.0078\n",
      "Epoch 966:\tTraining Loss - 0.0076\n",
      "Epoch 967:\tTraining Loss - 0.0075\n",
      "Epoch 968:\tTraining Loss - 0.0072\n",
      "Epoch 969:\tTraining Loss - 0.0107\n",
      "Epoch 970:\tTraining Loss - 0.0129\n",
      "Epoch 971:\tTraining Loss - 0.0071\n",
      "Epoch 972:\tTraining Loss - 0.0080\n",
      "Epoch 973:\tTraining Loss - 0.0115\n",
      "Epoch 974:\tTraining Loss - 0.0116\n",
      "Epoch 975:\tTraining Loss - 0.0142\n",
      "Epoch 976:\tTraining Loss - 0.0099\n",
      "Epoch 977:\tTraining Loss - 0.0071\n",
      "Epoch 978:\tTraining Loss - 0.0101\n",
      "Epoch 979:\tTraining Loss - 0.0080\n",
      "Epoch 980:\tTraining Loss - 0.0105\n",
      "Epoch 981:\tTraining Loss - 0.0094\n",
      "Epoch 982:\tTraining Loss - 0.0094\n",
      "Epoch 983:\tTraining Loss - 0.0096\n",
      "Epoch 984:\tTraining Loss - 0.0114\n",
      "Epoch 985:\tTraining Loss - 0.0083\n",
      "Epoch 986:\tTraining Loss - 0.0081\n",
      "Epoch 987:\tTraining Loss - 0.0102\n",
      "Epoch 988:\tTraining Loss - 0.0113\n",
      "Epoch 989:\tTraining Loss - 0.0087\n",
      "Epoch 990:\tTraining Loss - 0.0077\n",
      "Epoch 991:\tTraining Loss - 0.0118\n",
      "Epoch 992:\tTraining Loss - 0.0083\n",
      "Epoch 993:\tTraining Loss - 0.0088\n",
      "Epoch 994:\tTraining Loss - 0.0124\n",
      "Epoch 995:\tTraining Loss - 0.0095\n",
      "Epoch 996:\tTraining Loss - 0.0123\n",
      "Epoch 997:\tTraining Loss - 0.0084\n",
      "Epoch 998:\tTraining Loss - 0.0070\n",
      "Epoch 999:\tTraining Loss - 0.0087\n",
      "Epoch 1000:\tTraining Loss - 0.0103\n",
      "Epoch 1001:\tTraining Loss - 0.0140\n",
      "Epoch 1002:\tTraining Loss - 0.0136\n",
      "Epoch 1003:\tTraining Loss - 0.0102\n",
      "Epoch 1004:\tTraining Loss - 0.0108\n",
      "Epoch 1005:\tTraining Loss - 0.0112\n",
      "Epoch 1006:\tTraining Loss - 0.0102\n",
      "Epoch 1007:\tTraining Loss - 0.0079\n",
      "Epoch 1008:\tTraining Loss - 0.0115\n",
      "Epoch 1009:\tTraining Loss - 0.0137\n",
      "Epoch 1010:\tTraining Loss - 0.0110\n",
      "Epoch 1011:\tTraining Loss - 0.0098\n",
      "Epoch 1012:\tTraining Loss - 0.0103\n",
      "Epoch 1013:\tTraining Loss - 0.0080\n",
      "Epoch 1014:\tTraining Loss - 0.0101\n",
      "Epoch 1015:\tTraining Loss - 0.0099\n",
      "Epoch 1016:\tTraining Loss - 0.0083\n",
      "Epoch 1017:\tTraining Loss - 0.0104\n",
      "Epoch 1018:\tTraining Loss - 0.0105\n",
      "Epoch 1019:\tTraining Loss - 0.0087\n",
      "Epoch 1020:\tTraining Loss - 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1021:\tTraining Loss - 0.0099\n",
      "Epoch 1022:\tTraining Loss - 0.0112\n",
      "Epoch 1023:\tTraining Loss - 0.0121\n",
      "Epoch 1024:\tTraining Loss - 0.0084\n",
      "Epoch 1025:\tTraining Loss - 0.0102\n",
      "Epoch 1026:\tTraining Loss - 0.0118\n",
      "Epoch 1027:\tTraining Loss - 0.0076\n",
      "Epoch 1028:\tTraining Loss - 0.0153\n",
      "Epoch 1029:\tTraining Loss - 0.0068\n",
      "Epoch 1030:\tTraining Loss - 0.0097\n",
      "Epoch 1031:\tTraining Loss - 0.0098\n",
      "Epoch 1032:\tTraining Loss - 0.0089\n",
      "Epoch 1033:\tTraining Loss - 0.0100\n",
      "Epoch 1034:\tTraining Loss - 0.0086\n",
      "Epoch 1035:\tTraining Loss - 0.0060\n",
      "Epoch 1036:\tTraining Loss - 0.0078\n",
      "Epoch 1037:\tTraining Loss - 0.0093\n",
      "Epoch 1038:\tTraining Loss - 0.0068\n",
      "Epoch 1039:\tTraining Loss - 0.0082\n",
      "Epoch 1040:\tTraining Loss - 0.0082\n",
      "Epoch 1041:\tTraining Loss - 0.0104\n",
      "Epoch 1042:\tTraining Loss - 0.0093\n",
      "Epoch 1043:\tTraining Loss - 0.0125\n",
      "Epoch 1044:\tTraining Loss - 0.0085\n",
      "Epoch 1045:\tTraining Loss - 0.0117\n",
      "Epoch 1046:\tTraining Loss - 0.0165\n",
      "Epoch 1047:\tTraining Loss - 0.0083\n",
      "Epoch 1048:\tTraining Loss - 0.0095\n",
      "Epoch 1049:\tTraining Loss - 0.0096\n",
      "Epoch 1050:\tTraining Loss - 0.0095\n",
      "Epoch 1051:\tTraining Loss - 0.0082\n",
      "Epoch 1052:\tTraining Loss - 0.0086\n",
      "Epoch 1053:\tTraining Loss - 0.0093\n",
      "Epoch 1054:\tTraining Loss - 0.0103\n",
      "Epoch 1055:\tTraining Loss - 0.0116\n",
      "Epoch 1056:\tTraining Loss - 0.0070\n",
      "Epoch 1057:\tTraining Loss - 0.0067\n",
      "Epoch 1058:\tTraining Loss - 0.0066\n",
      "Epoch 1059:\tTraining Loss - 0.0090\n",
      "Epoch 1060:\tTraining Loss - 0.0071\n",
      "Epoch 1061:\tTraining Loss - 0.0095\n",
      "Epoch 1062:\tTraining Loss - 0.0082\n",
      "Epoch 1063:\tTraining Loss - 0.0101\n",
      "Epoch 1064:\tTraining Loss - 0.0154\n",
      "Epoch 1065:\tTraining Loss - 0.0064\n",
      "Epoch 1066:\tTraining Loss - 0.0120\n",
      "Epoch 1067:\tTraining Loss - 0.0082\n",
      "Epoch 1068:\tTraining Loss - 0.0097\n",
      "Epoch 1069:\tTraining Loss - 0.0134\n",
      "Epoch 1070:\tTraining Loss - 0.0063\n",
      "Epoch 1071:\tTraining Loss - 0.0053\n",
      "Epoch 1072:\tTraining Loss - 0.0083\n",
      "Epoch 1073:\tTraining Loss - 0.0095\n",
      "Epoch 1074:\tTraining Loss - 0.0082\n",
      "Epoch 1075:\tTraining Loss - 0.0093\n",
      "Epoch 1076:\tTraining Loss - 0.0093\n",
      "Epoch 1077:\tTraining Loss - 0.0070\n",
      "Epoch 1078:\tTraining Loss - 0.0142\n",
      "Epoch 1079:\tTraining Loss - 0.0076\n",
      "Epoch 1080:\tTraining Loss - 0.0105\n",
      "Epoch 1081:\tTraining Loss - 0.0095\n",
      "Epoch 1082:\tTraining Loss - 0.0075\n",
      "Epoch 1083:\tTraining Loss - 0.0114\n",
      "Epoch 1084:\tTraining Loss - 0.0074\n",
      "Epoch 1085:\tTraining Loss - 0.0060\n",
      "Epoch 1086:\tTraining Loss - 0.0087\n",
      "Epoch 1087:\tTraining Loss - 0.0063\n",
      "Epoch 1088:\tTraining Loss - 0.0127\n",
      "Epoch 1089:\tTraining Loss - 0.0149\n",
      "Epoch 1090:\tTraining Loss - 0.0087\n",
      "Epoch 1091:\tTraining Loss - 0.0093\n",
      "Epoch 1092:\tTraining Loss - 0.0097\n",
      "Epoch 1093:\tTraining Loss - 0.0073\n",
      "Epoch 1094:\tTraining Loss - 0.0107\n",
      "Epoch 1095:\tTraining Loss - 0.0061\n",
      "Epoch 1096:\tTraining Loss - 0.0070\n",
      "Epoch 1097:\tTraining Loss - 0.0081\n",
      "Epoch 1098:\tTraining Loss - 0.0077\n",
      "Epoch 1099:\tTraining Loss - 0.0112\n",
      "Epoch 1100:\tTraining Loss - 0.0127\n",
      "Epoch 1101:\tTraining Loss - 0.0087\n",
      "Epoch 1102:\tTraining Loss - 0.0069\n",
      "Epoch 1103:\tTraining Loss - 0.0081\n",
      "Epoch 1104:\tTraining Loss - 0.0078\n",
      "Epoch 1105:\tTraining Loss - 0.0062\n",
      "Epoch 1106:\tTraining Loss - 0.0097\n",
      "Epoch 1107:\tTraining Loss - 0.0074\n",
      "Epoch 1108:\tTraining Loss - 0.0085\n",
      "Epoch 1109:\tTraining Loss - 0.0084\n",
      "Epoch 1110:\tTraining Loss - 0.0092\n",
      "Epoch 1111:\tTraining Loss - 0.0075\n",
      "Epoch 1112:\tTraining Loss - 0.0085\n",
      "Epoch 1113:\tTraining Loss - 0.0106\n",
      "Epoch 1114:\tTraining Loss - 0.0128\n",
      "Epoch 1115:\tTraining Loss - 0.0095\n",
      "Epoch 1116:\tTraining Loss - 0.0078\n",
      "Epoch 1117:\tTraining Loss - 0.0094\n",
      "Epoch 1118:\tTraining Loss - 0.0099\n",
      "Epoch 1119:\tTraining Loss - 0.0112\n",
      "Epoch 1120:\tTraining Loss - 0.0079\n",
      "Epoch 1121:\tTraining Loss - 0.0077\n",
      "Epoch 1122:\tTraining Loss - 0.0092\n",
      "Epoch 1123:\tTraining Loss - 0.0081\n",
      "Epoch 1124:\tTraining Loss - 0.0083\n",
      "Epoch 1125:\tTraining Loss - 0.0155\n",
      "Epoch 1126:\tTraining Loss - 0.0101\n",
      "Epoch 1127:\tTraining Loss - 0.0098\n",
      "Epoch 1128:\tTraining Loss - 0.0085\n",
      "Epoch 1129:\tTraining Loss - 0.0063\n",
      "Epoch 1130:\tTraining Loss - 0.0081\n",
      "Epoch 1131:\tTraining Loss - 0.0088\n",
      "Epoch 1132:\tTraining Loss - 0.0163\n",
      "Epoch 1133:\tTraining Loss - 0.0091\n",
      "Epoch 1134:\tTraining Loss - 0.0114\n",
      "Epoch 1135:\tTraining Loss - 0.0093\n",
      "Epoch 1136:\tTraining Loss - 0.0079\n",
      "Epoch 1137:\tTraining Loss - 0.0075\n",
      "Epoch 1138:\tTraining Loss - 0.0063\n",
      "Epoch 1139:\tTraining Loss - 0.0090\n",
      "Epoch 1140:\tTraining Loss - 0.0069\n",
      "Epoch 1141:\tTraining Loss - 0.0083\n",
      "Epoch 1142:\tTraining Loss - 0.0143\n",
      "Epoch 1143:\tTraining Loss - 0.0095\n",
      "Epoch 1144:\tTraining Loss - 0.0163\n",
      "Epoch 1145:\tTraining Loss - 0.0079\n",
      "Epoch 1146:\tTraining Loss - 0.0076\n",
      "Epoch 1147:\tTraining Loss - 0.0081\n",
      "Epoch 1148:\tTraining Loss - 0.0111\n",
      "Epoch 1149:\tTraining Loss - 0.0134\n",
      "Epoch 1150:\tTraining Loss - 0.0127\n",
      "Epoch 1151:\tTraining Loss - 0.0082\n",
      "Epoch 1152:\tTraining Loss - 0.0096\n",
      "Epoch 1153:\tTraining Loss - 0.0133\n",
      "Epoch 1154:\tTraining Loss - 0.0094\n",
      "Epoch 1155:\tTraining Loss - 0.0097\n",
      "Epoch 1156:\tTraining Loss - 0.0063\n",
      "Epoch 1157:\tTraining Loss - 0.0069\n",
      "Epoch 1158:\tTraining Loss - 0.0084\n",
      "Epoch 1159:\tTraining Loss - 0.0088\n",
      "Epoch 1160:\tTraining Loss - 0.0081\n",
      "Epoch 1161:\tTraining Loss - 0.0102\n",
      "Epoch 1162:\tTraining Loss - 0.0086\n",
      "Epoch 1163:\tTraining Loss - 0.0055\n",
      "Epoch 1164:\tTraining Loss - 0.0113\n",
      "Epoch 1165:\tTraining Loss - 0.0078\n",
      "Epoch 1166:\tTraining Loss - 0.0122\n",
      "Epoch 1167:\tTraining Loss - 0.0117\n",
      "Epoch 1168:\tTraining Loss - 0.0067\n",
      "Epoch 1169:\tTraining Loss - 0.0070\n",
      "Epoch 1170:\tTraining Loss - 0.0093\n",
      "Epoch 1171:\tTraining Loss - 0.0067\n",
      "Epoch 1172:\tTraining Loss - 0.0110\n",
      "Epoch 1173:\tTraining Loss - 0.0098\n",
      "Epoch 1174:\tTraining Loss - 0.0081\n",
      "Epoch 1175:\tTraining Loss - 0.0089\n",
      "Epoch 1176:\tTraining Loss - 0.0122\n",
      "Epoch 1177:\tTraining Loss - 0.0075\n",
      "Epoch 1178:\tTraining Loss - 0.0098\n",
      "Epoch 1179:\tTraining Loss - 0.0114\n",
      "Epoch 1180:\tTraining Loss - 0.0082\n",
      "Epoch 1181:\tTraining Loss - 0.0087\n",
      "Epoch 1182:\tTraining Loss - 0.0096\n",
      "Epoch 1183:\tTraining Loss - 0.0091\n",
      "Epoch 1184:\tTraining Loss - 0.0083\n",
      "Epoch 1185:\tTraining Loss - 0.0085\n",
      "Epoch 1186:\tTraining Loss - 0.0068\n",
      "Epoch 1187:\tTraining Loss - 0.0118\n",
      "Epoch 1188:\tTraining Loss - 0.0103\n",
      "Epoch 1189:\tTraining Loss - 0.0092\n",
      "Epoch 1190:\tTraining Loss - 0.0090\n",
      "Epoch 1191:\tTraining Loss - 0.0077\n",
      "Epoch 1192:\tTraining Loss - 0.0147\n",
      "Epoch 1193:\tTraining Loss - 0.0080\n",
      "Epoch 1194:\tTraining Loss - 0.0085\n",
      "Epoch 1195:\tTraining Loss - 0.0080\n",
      "Epoch 1196:\tTraining Loss - 0.0101\n",
      "Epoch 1197:\tTraining Loss - 0.0115\n",
      "Epoch 1198:\tTraining Loss - 0.0087\n",
      "Epoch 1199:\tTraining Loss - 0.0114\n",
      "Epoch 1200:\tTraining Loss - 0.0090\n",
      "Epoch 1201:\tTraining Loss - 0.0083\n",
      "Epoch 1202:\tTraining Loss - 0.0092\n",
      "Epoch 1203:\tTraining Loss - 0.0088\n",
      "Epoch 1204:\tTraining Loss - 0.0112\n",
      "Epoch 1205:\tTraining Loss - 0.0056\n",
      "Epoch 1206:\tTraining Loss - 0.0089\n",
      "Epoch 1207:\tTraining Loss - 0.0060\n",
      "Epoch 1208:\tTraining Loss - 0.0091\n",
      "Epoch 1209:\tTraining Loss - 0.0102\n",
      "Epoch 1210:\tTraining Loss - 0.0074\n",
      "Epoch 1211:\tTraining Loss - 0.0078\n",
      "Epoch 1212:\tTraining Loss - 0.0102\n",
      "Epoch 1213:\tTraining Loss - 0.0143\n",
      "Epoch 1214:\tTraining Loss - 0.0093\n",
      "Epoch 1215:\tTraining Loss - 0.0066\n",
      "Epoch 1216:\tTraining Loss - 0.0096\n",
      "Epoch 1217:\tTraining Loss - 0.0078\n",
      "Epoch 1218:\tTraining Loss - 0.0109\n",
      "Epoch 1219:\tTraining Loss - 0.0095\n",
      "Epoch 1220:\tTraining Loss - 0.0063\n",
      "Epoch 1221:\tTraining Loss - 0.0085\n",
      "Epoch 1222:\tTraining Loss - 0.0085\n",
      "Epoch 1223:\tTraining Loss - 0.0099\n",
      "Epoch 1224:\tTraining Loss - 0.0075\n",
      "Epoch 1225:\tTraining Loss - 0.0141\n",
      "Epoch 1226:\tTraining Loss - 0.0084\n",
      "Epoch 1227:\tTraining Loss - 0.0078\n",
      "Epoch 1228:\tTraining Loss - 0.0074\n",
      "Epoch 1229:\tTraining Loss - 0.0094\n",
      "Epoch 1230:\tTraining Loss - 0.0080\n",
      "Epoch 1231:\tTraining Loss - 0.0061\n",
      "Epoch 1232:\tTraining Loss - 0.0139\n",
      "Epoch 1233:\tTraining Loss - 0.0104\n",
      "Epoch 1234:\tTraining Loss - 0.0131\n",
      "Epoch 1235:\tTraining Loss - 0.0094\n",
      "Epoch 1236:\tTraining Loss - 0.0064\n",
      "Epoch 1237:\tTraining Loss - 0.0083\n",
      "Epoch 1238:\tTraining Loss - 0.0083\n",
      "Epoch 1239:\tTraining Loss - 0.0138\n",
      "Epoch 1240:\tTraining Loss - 0.0063\n",
      "Epoch 1241:\tTraining Loss - 0.0105\n",
      "Epoch 1242:\tTraining Loss - 0.0115\n",
      "Epoch 1243:\tTraining Loss - 0.0100\n",
      "Epoch 1244:\tTraining Loss - 0.0083\n",
      "Epoch 1245:\tTraining Loss - 0.0100\n",
      "Epoch 1246:\tTraining Loss - 0.0084\n",
      "Epoch 1247:\tTraining Loss - 0.0064\n",
      "Epoch 1248:\tTraining Loss - 0.0090\n",
      "Epoch 1249:\tTraining Loss - 0.0084\n",
      "Epoch 1250:\tTraining Loss - 0.0093\n",
      "Epoch 1251:\tTraining Loss - 0.0090\n",
      "Epoch 1252:\tTraining Loss - 0.0091\n",
      "Epoch 1253:\tTraining Loss - 0.0093\n",
      "Epoch 1254:\tTraining Loss - 0.0101\n",
      "Epoch 1255:\tTraining Loss - 0.0063\n",
      "Epoch 1256:\tTraining Loss - 0.0078\n",
      "Epoch 1257:\tTraining Loss - 0.0103\n",
      "Epoch 1258:\tTraining Loss - 0.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1259:\tTraining Loss - 0.0088\n",
      "Epoch 1260:\tTraining Loss - 0.0085\n",
      "Epoch 1261:\tTraining Loss - 0.0080\n",
      "Epoch 1262:\tTraining Loss - 0.0053\n",
      "Epoch 1263:\tTraining Loss - 0.0099\n",
      "Epoch 1264:\tTraining Loss - 0.0088\n",
      "Epoch 1265:\tTraining Loss - 0.0134\n",
      "Epoch 1266:\tTraining Loss - 0.0082\n",
      "Epoch 1267:\tTraining Loss - 0.0097\n",
      "Epoch 1268:\tTraining Loss - 0.0084\n",
      "Epoch 1269:\tTraining Loss - 0.0078\n",
      "Epoch 1270:\tTraining Loss - 0.0095\n",
      "Epoch 1271:\tTraining Loss - 0.0082\n",
      "Epoch 1272:\tTraining Loss - 0.0075\n",
      "Epoch 1273:\tTraining Loss - 0.0081\n",
      "Epoch 1274:\tTraining Loss - 0.0088\n",
      "Epoch 1275:\tTraining Loss - 0.0071\n",
      "Epoch 1276:\tTraining Loss - 0.0078\n",
      "Epoch 1277:\tTraining Loss - 0.0067\n",
      "Epoch 1278:\tTraining Loss - 0.0076\n",
      "Epoch 1279:\tTraining Loss - 0.0079\n",
      "Epoch 1280:\tTraining Loss - 0.0099\n",
      "Epoch 1281:\tTraining Loss - 0.0109\n",
      "Epoch 1282:\tTraining Loss - 0.0092\n",
      "Epoch 1283:\tTraining Loss - 0.0109\n",
      "Epoch 1284:\tTraining Loss - 0.0080\n",
      "Epoch 1285:\tTraining Loss - 0.0062\n",
      "Epoch 1286:\tTraining Loss - 0.0088\n",
      "Epoch 1287:\tTraining Loss - 0.0089\n",
      "Epoch 1288:\tTraining Loss - 0.0100\n",
      "Epoch 1289:\tTraining Loss - 0.0104\n",
      "Epoch 1290:\tTraining Loss - 0.0064\n",
      "Epoch 1291:\tTraining Loss - 0.0059\n",
      "Epoch 1292:\tTraining Loss - 0.0084\n",
      "Epoch 1293:\tTraining Loss - 0.0064\n",
      "Epoch 1294:\tTraining Loss - 0.0093\n",
      "Epoch 1295:\tTraining Loss - 0.0080\n",
      "Epoch 1296:\tTraining Loss - 0.0067\n",
      "Epoch 1297:\tTraining Loss - 0.0092\n",
      "Epoch 1298:\tTraining Loss - 0.0086\n",
      "Epoch 1299:\tTraining Loss - 0.0098\n",
      "Epoch 1300:\tTraining Loss - 0.0089\n",
      "Epoch 1301:\tTraining Loss - 0.0095\n",
      "Epoch 1302:\tTraining Loss - 0.0100\n",
      "Epoch 1303:\tTraining Loss - 0.0075\n",
      "Epoch 1304:\tTraining Loss - 0.0147\n",
      "Epoch 1305:\tTraining Loss - 0.0056\n",
      "Epoch 1306:\tTraining Loss - 0.0089\n",
      "Epoch 1307:\tTraining Loss - 0.0100\n",
      "Epoch 1308:\tTraining Loss - 0.0105\n",
      "Epoch 1309:\tTraining Loss - 0.0103\n",
      "Epoch 1310:\tTraining Loss - 0.0137\n",
      "Epoch 1311:\tTraining Loss - 0.0089\n",
      "Epoch 1312:\tTraining Loss - 0.0085\n",
      "Epoch 1313:\tTraining Loss - 0.0113\n",
      "Epoch 1314:\tTraining Loss - 0.0065\n",
      "Epoch 1315:\tTraining Loss - 0.0091\n",
      "Epoch 1316:\tTraining Loss - 0.0110\n",
      "Epoch 1317:\tTraining Loss - 0.0084\n",
      "Epoch 1318:\tTraining Loss - 0.0086\n",
      "Epoch 1319:\tTraining Loss - 0.0102\n",
      "Epoch 1320:\tTraining Loss - 0.0107\n",
      "Epoch 1321:\tTraining Loss - 0.0085\n",
      "Epoch 1322:\tTraining Loss - 0.0070\n",
      "Epoch 1323:\tTraining Loss - 0.0080\n",
      "Epoch 1324:\tTraining Loss - 0.0138\n",
      "Epoch 1325:\tTraining Loss - 0.0090\n",
      "Epoch 1326:\tTraining Loss - 0.0078\n",
      "Epoch 1327:\tTraining Loss - 0.0078\n",
      "Epoch 1328:\tTraining Loss - 0.0081\n",
      "Epoch 1329:\tTraining Loss - 0.0056\n",
      "Epoch 1330:\tTraining Loss - 0.0103\n",
      "Epoch 1331:\tTraining Loss - 0.0099\n",
      "Epoch 1332:\tTraining Loss - 0.0122\n",
      "Epoch 1333:\tTraining Loss - 0.0079\n",
      "Epoch 1334:\tTraining Loss - 0.0102\n",
      "Epoch 1335:\tTraining Loss - 0.0072\n",
      "Epoch 1336:\tTraining Loss - 0.0081\n",
      "Epoch 1337:\tTraining Loss - 0.0070\n",
      "Epoch 1338:\tTraining Loss - 0.0102\n",
      "Epoch 1339:\tTraining Loss - 0.0102\n",
      "Epoch 1340:\tTraining Loss - 0.0095\n",
      "Epoch 1341:\tTraining Loss - 0.0114\n",
      "Epoch 1342:\tTraining Loss - 0.0130\n",
      "Epoch 1343:\tTraining Loss - 0.0094\n",
      "Epoch 1344:\tTraining Loss - 0.0088\n",
      "Epoch 1345:\tTraining Loss - 0.0087\n",
      "Epoch 1346:\tTraining Loss - 0.0085\n",
      "Epoch 1347:\tTraining Loss - 0.0096\n",
      "Epoch 1348:\tTraining Loss - 0.0089\n",
      "Epoch 1349:\tTraining Loss - 0.0139\n",
      "Epoch 1350:\tTraining Loss - 0.0093\n",
      "Epoch 1351:\tTraining Loss - 0.0094\n",
      "Epoch 1352:\tTraining Loss - 0.0087\n",
      "Epoch 1353:\tTraining Loss - 0.0090\n",
      "Epoch 1354:\tTraining Loss - 0.0059\n",
      "Epoch 1355:\tTraining Loss - 0.0062\n",
      "Epoch 1356:\tTraining Loss - 0.0109\n",
      "Epoch 1357:\tTraining Loss - 0.0065\n",
      "Epoch 1358:\tTraining Loss - 0.0078\n",
      "Epoch 1359:\tTraining Loss - 0.0096\n",
      "Epoch 1360:\tTraining Loss - 0.0096\n",
      "Epoch 1361:\tTraining Loss - 0.0099\n",
      "Epoch 1362:\tTraining Loss - 0.0096\n",
      "Epoch 1363:\tTraining Loss - 0.0062\n",
      "Epoch 1364:\tTraining Loss - 0.0062\n",
      "Epoch 1365:\tTraining Loss - 0.0076\n",
      "Epoch 1366:\tTraining Loss - 0.0088\n",
      "Epoch 1367:\tTraining Loss - 0.0069\n",
      "Epoch 1368:\tTraining Loss - 0.0125\n",
      "Epoch 1369:\tTraining Loss - 0.0111\n",
      "Epoch 1370:\tTraining Loss - 0.0095\n",
      "Epoch 1371:\tTraining Loss - 0.0077\n",
      "Epoch 1372:\tTraining Loss - 0.0090\n",
      "Epoch 1373:\tTraining Loss - 0.0102\n",
      "Epoch 1374:\tTraining Loss - 0.0098\n",
      "Epoch 1375:\tTraining Loss - 0.0072\n",
      "Epoch 1376:\tTraining Loss - 0.0088\n",
      "Epoch 1377:\tTraining Loss - 0.0080\n",
      "Epoch 1378:\tTraining Loss - 0.0111\n",
      "Epoch 1379:\tTraining Loss - 0.0087\n",
      "Epoch 1380:\tTraining Loss - 0.0095\n",
      "Epoch 1381:\tTraining Loss - 0.0060\n",
      "Epoch 1382:\tTraining Loss - 0.0131\n",
      "Epoch 1383:\tTraining Loss - 0.0066\n",
      "Epoch 1384:\tTraining Loss - 0.0092\n",
      "Epoch 1385:\tTraining Loss - 0.0082\n",
      "Epoch 1386:\tTraining Loss - 0.0055\n",
      "Epoch 1387:\tTraining Loss - 0.0085\n",
      "Epoch 1388:\tTraining Loss - 0.0130\n",
      "Epoch 1389:\tTraining Loss - 0.0055\n",
      "Epoch 1390:\tTraining Loss - 0.0070\n",
      "Epoch 1391:\tTraining Loss - 0.0082\n",
      "Epoch 1392:\tTraining Loss - 0.0077\n",
      "Epoch 1393:\tTraining Loss - 0.0085\n",
      "Epoch 1394:\tTraining Loss - 0.0077\n",
      "Epoch 1395:\tTraining Loss - 0.0062\n",
      "Epoch 1396:\tTraining Loss - 0.0077\n",
      "Epoch 1397:\tTraining Loss - 0.0091\n",
      "Epoch 1398:\tTraining Loss - 0.0062\n",
      "Epoch 1399:\tTraining Loss - 0.0107\n",
      "Epoch 1400:\tTraining Loss - 0.0090\n",
      "Epoch 1401:\tTraining Loss - 0.0114\n",
      "Epoch 1402:\tTraining Loss - 0.0079\n",
      "Epoch 1403:\tTraining Loss - 0.0071\n",
      "Epoch 1404:\tTraining Loss - 0.0124\n",
      "Epoch 1405:\tTraining Loss - 0.0073\n",
      "Epoch 1406:\tTraining Loss - 0.0061\n",
      "Epoch 1407:\tTraining Loss - 0.0067\n",
      "Epoch 1408:\tTraining Loss - 0.0098\n",
      "Epoch 1409:\tTraining Loss - 0.0078\n",
      "Epoch 1410:\tTraining Loss - 0.0081\n",
      "Epoch 1411:\tTraining Loss - 0.0070\n",
      "Epoch 1412:\tTraining Loss - 0.0087\n",
      "Epoch 1413:\tTraining Loss - 0.0059\n",
      "Epoch 1414:\tTraining Loss - 0.0088\n",
      "Epoch 1415:\tTraining Loss - 0.0072\n",
      "Epoch 1416:\tTraining Loss - 0.0063\n",
      "Epoch 1417:\tTraining Loss - 0.0096\n",
      "Epoch 1418:\tTraining Loss - 0.0070\n",
      "Epoch 1419:\tTraining Loss - 0.0073\n",
      "Epoch 1420:\tTraining Loss - 0.0069\n",
      "Epoch 1421:\tTraining Loss - 0.0111\n",
      "Epoch 1422:\tTraining Loss - 0.0076\n",
      "Epoch 1423:\tTraining Loss - 0.0086\n",
      "Epoch 1424:\tTraining Loss - 0.0097\n",
      "Epoch 1425:\tTraining Loss - 0.0069\n",
      "Epoch 1426:\tTraining Loss - 0.0095\n",
      "Epoch 1427:\tTraining Loss - 0.0109\n",
      "Epoch 1428:\tTraining Loss - 0.0083\n",
      "Epoch 1429:\tTraining Loss - 0.0086\n",
      "Epoch 1430:\tTraining Loss - 0.0099\n",
      "Epoch 1431:\tTraining Loss - 0.0057\n",
      "Epoch 1432:\tTraining Loss - 0.0103\n",
      "Epoch 1433:\tTraining Loss - 0.0102\n",
      "Epoch 1434:\tTraining Loss - 0.0067\n",
      "Epoch 1435:\tTraining Loss - 0.0125\n",
      "Epoch 1436:\tTraining Loss - 0.0060\n",
      "Epoch 1437:\tTraining Loss - 0.0103\n",
      "Epoch 1438:\tTraining Loss - 0.0067\n",
      "Epoch 1439:\tTraining Loss - 0.0066\n",
      "Epoch 1440:\tTraining Loss - 0.0067\n",
      "Epoch 1441:\tTraining Loss - 0.0064\n",
      "Epoch 1442:\tTraining Loss - 0.0056\n",
      "Epoch 1443:\tTraining Loss - 0.0075\n",
      "Epoch 1444:\tTraining Loss - 0.0097\n",
      "Epoch 1445:\tTraining Loss - 0.0078\n",
      "Epoch 1446:\tTraining Loss - 0.0113\n",
      "Epoch 1447:\tTraining Loss - 0.0101\n",
      "Epoch 1448:\tTraining Loss - 0.0079\n",
      "Epoch 1449:\tTraining Loss - 0.0061\n",
      "Epoch 1450:\tTraining Loss - 0.0096\n",
      "Epoch 1451:\tTraining Loss - 0.0085\n",
      "Epoch 1452:\tTraining Loss - 0.0100\n",
      "Epoch 1453:\tTraining Loss - 0.0099\n",
      "Epoch 1454:\tTraining Loss - 0.0097\n",
      "Epoch 1455:\tTraining Loss - 0.0070\n",
      "Epoch 1456:\tTraining Loss - 0.0055\n",
      "Epoch 1457:\tTraining Loss - 0.0057\n",
      "Epoch 1458:\tTraining Loss - 0.0062\n",
      "Epoch 1459:\tTraining Loss - 0.0130\n",
      "Epoch 1460:\tTraining Loss - 0.0075\n",
      "Epoch 1461:\tTraining Loss - 0.0072\n",
      "Epoch 1462:\tTraining Loss - 0.0100\n",
      "Epoch 1463:\tTraining Loss - 0.0054\n",
      "Epoch 1464:\tTraining Loss - 0.0080\n",
      "Epoch 1465:\tTraining Loss - 0.0072\n",
      "Epoch 1466:\tTraining Loss - 0.0112\n",
      "Epoch 1467:\tTraining Loss - 0.0111\n",
      "Epoch 1468:\tTraining Loss - 0.0092\n",
      "Epoch 1469:\tTraining Loss - 0.0062\n",
      "Epoch 1470:\tTraining Loss - 0.0116\n",
      "Epoch 1471:\tTraining Loss - 0.0092\n",
      "Epoch 1472:\tTraining Loss - 0.0106\n",
      "Epoch 1473:\tTraining Loss - 0.0062\n",
      "Epoch 1474:\tTraining Loss - 0.0109\n",
      "Epoch 1475:\tTraining Loss - 0.0078\n",
      "Epoch 1476:\tTraining Loss - 0.0065\n",
      "Epoch 1477:\tTraining Loss - 0.0051\n",
      "Epoch 1478:\tTraining Loss - 0.0093\n",
      "Epoch 1479:\tTraining Loss - 0.0064\n",
      "Epoch 1480:\tTraining Loss - 0.0070\n",
      "Epoch 1481:\tTraining Loss - 0.0108\n",
      "Epoch 1482:\tTraining Loss - 0.0096\n",
      "Epoch 1483:\tTraining Loss - 0.0068\n",
      "Epoch 1484:\tTraining Loss - 0.0105\n",
      "Epoch 1485:\tTraining Loss - 0.0069\n",
      "Epoch 1486:\tTraining Loss - 0.0074\n",
      "Epoch 1487:\tTraining Loss - 0.0091\n",
      "Epoch 1488:\tTraining Loss - 0.0058\n",
      "Epoch 1489:\tTraining Loss - 0.0075\n",
      "Epoch 1490:\tTraining Loss - 0.0073\n",
      "Epoch 1491:\tTraining Loss - 0.0072\n",
      "Epoch 1492:\tTraining Loss - 0.0071\n",
      "Epoch 1493:\tTraining Loss - 0.0072\n",
      "Epoch 1494:\tTraining Loss - 0.0106\n",
      "Epoch 1495:\tTraining Loss - 0.0091\n",
      "Epoch 1496:\tTraining Loss - 0.0084\n",
      "Epoch 1497:\tTraining Loss - 0.0133\n",
      "Epoch 1498:\tTraining Loss - 0.0055\n",
      "Epoch 1499:\tTraining Loss - 0.0086\n",
      "Epoch 1500:\tTraining Loss - 0.0114\n",
      "Epoch 1501:\tTraining Loss - 0.0065\n",
      "Epoch 1502:\tTraining Loss - 0.0069\n",
      "Epoch 1503:\tTraining Loss - 0.0076\n",
      "Epoch 1504:\tTraining Loss - 0.0082\n",
      "Epoch 1505:\tTraining Loss - 0.0125\n",
      "Epoch 1506:\tTraining Loss - 0.0105\n",
      "Epoch 1507:\tTraining Loss - 0.0084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1508:\tTraining Loss - 0.0088\n",
      "Epoch 1509:\tTraining Loss - 0.0068\n",
      "Epoch 1510:\tTraining Loss - 0.0068\n",
      "Epoch 1511:\tTraining Loss - 0.0073\n",
      "Epoch 1512:\tTraining Loss - 0.0079\n",
      "Epoch 1513:\tTraining Loss - 0.0065\n",
      "Epoch 1514:\tTraining Loss - 0.0093\n",
      "Epoch 1515:\tTraining Loss - 0.0062\n",
      "Epoch 1516:\tTraining Loss - 0.0081\n",
      "Epoch 1517:\tTraining Loss - 0.0050\n",
      "Epoch 1518:\tTraining Loss - 0.0109\n",
      "Epoch 1519:\tTraining Loss - 0.0060\n",
      "Epoch 1520:\tTraining Loss - 0.0101\n",
      "Epoch 1521:\tTraining Loss - 0.0109\n",
      "Epoch 1522:\tTraining Loss - 0.0093\n",
      "Epoch 1523:\tTraining Loss - 0.0099\n",
      "Epoch 1524:\tTraining Loss - 0.0091\n",
      "Epoch 1525:\tTraining Loss - 0.0100\n",
      "Epoch 1526:\tTraining Loss - 0.0069\n",
      "Epoch 1527:\tTraining Loss - 0.0112\n",
      "Epoch 1528:\tTraining Loss - 0.0081\n",
      "Epoch 1529:\tTraining Loss - 0.0101\n",
      "Epoch 1530:\tTraining Loss - 0.0080\n",
      "Epoch 1531:\tTraining Loss - 0.0066\n",
      "Epoch 1532:\tTraining Loss - 0.0081\n",
      "Epoch 1533:\tTraining Loss - 0.0085\n",
      "Epoch 1534:\tTraining Loss - 0.0061\n",
      "Epoch 1535:\tTraining Loss - 0.0066\n",
      "Epoch 1536:\tTraining Loss - 0.0052\n",
      "Epoch 1537:\tTraining Loss - 0.0072\n",
      "Epoch 1538:\tTraining Loss - 0.0083\n",
      "Epoch 1539:\tTraining Loss - 0.0100\n",
      "Epoch 1540:\tTraining Loss - 0.0074\n",
      "Epoch 1541:\tTraining Loss - 0.0055\n",
      "Epoch 1542:\tTraining Loss - 0.0064\n",
      "Epoch 1543:\tTraining Loss - 0.0094\n",
      "Epoch 1544:\tTraining Loss - 0.0065\n",
      "Epoch 1545:\tTraining Loss - 0.0076\n",
      "Epoch 1546:\tTraining Loss - 0.0090\n",
      "Epoch 1547:\tTraining Loss - 0.0087\n",
      "Epoch 1548:\tTraining Loss - 0.0058\n",
      "Epoch 1549:\tTraining Loss - 0.0066\n",
      "Epoch 1550:\tTraining Loss - 0.0097\n",
      "Epoch 1551:\tTraining Loss - 0.0073\n",
      "Epoch 1552:\tTraining Loss - 0.0091\n",
      "Epoch 1553:\tTraining Loss - 0.0119\n",
      "Epoch 1554:\tTraining Loss - 0.0092\n",
      "Epoch 1555:\tTraining Loss - 0.0114\n",
      "Epoch 1556:\tTraining Loss - 0.0124\n",
      "Epoch 1557:\tTraining Loss - 0.0093\n",
      "Epoch 1558:\tTraining Loss - 0.0093\n",
      "Epoch 1559:\tTraining Loss - 0.0078\n",
      "Epoch 1560:\tTraining Loss - 0.0054\n",
      "Epoch 1561:\tTraining Loss - 0.0069\n",
      "Epoch 1562:\tTraining Loss - 0.0073\n",
      "Epoch 1563:\tTraining Loss - 0.0081\n",
      "Epoch 1564:\tTraining Loss - 0.0084\n",
      "Epoch 1565:\tTraining Loss - 0.0064\n",
      "Epoch 1566:\tTraining Loss - 0.0079\n",
      "Epoch 1567:\tTraining Loss - 0.0066\n",
      "Epoch 1568:\tTraining Loss - 0.0091\n",
      "Epoch 1569:\tTraining Loss - 0.0095\n",
      "Epoch 1570:\tTraining Loss - 0.0095\n",
      "Epoch 1571:\tTraining Loss - 0.0066\n",
      "Epoch 1572:\tTraining Loss - 0.0082\n",
      "Epoch 1573:\tTraining Loss - 0.0094\n",
      "Epoch 1574:\tTraining Loss - 0.0082\n",
      "Epoch 1575:\tTraining Loss - 0.0091\n",
      "Epoch 1576:\tTraining Loss - 0.0085\n",
      "Epoch 1577:\tTraining Loss - 0.0095\n",
      "Epoch 1578:\tTraining Loss - 0.0088\n",
      "Epoch 1579:\tTraining Loss - 0.0098\n",
      "Epoch 1580:\tTraining Loss - 0.0058\n",
      "Epoch 1581:\tTraining Loss - 0.0088\n",
      "Epoch 1582:\tTraining Loss - 0.0077\n",
      "Epoch 1583:\tTraining Loss - 0.0091\n",
      "Epoch 1584:\tTraining Loss - 0.0108\n",
      "Epoch 1585:\tTraining Loss - 0.0078\n",
      "Epoch 1586:\tTraining Loss - 0.0100\n",
      "Epoch 1587:\tTraining Loss - 0.0087\n",
      "Epoch 1588:\tTraining Loss - 0.0072\n",
      "Epoch 1589:\tTraining Loss - 0.0071\n",
      "Epoch 1590:\tTraining Loss - 0.0120\n",
      "Epoch 1591:\tTraining Loss - 0.0067\n",
      "Epoch 1592:\tTraining Loss - 0.0070\n",
      "Epoch 1593:\tTraining Loss - 0.0069\n",
      "Epoch 1594:\tTraining Loss - 0.0129\n",
      "Epoch 1595:\tTraining Loss - 0.0097\n",
      "Epoch 1596:\tTraining Loss - 0.0055\n",
      "Epoch 1597:\tTraining Loss - 0.0082\n",
      "Epoch 1598:\tTraining Loss - 0.0056\n",
      "Epoch 1599:\tTraining Loss - 0.0090\n",
      "Epoch 1600:\tTraining Loss - 0.0073\n",
      "Epoch 1601:\tTraining Loss - 0.0095\n",
      "Epoch 1602:\tTraining Loss - 0.0081\n",
      "Epoch 1603:\tTraining Loss - 0.0100\n",
      "Epoch 1604:\tTraining Loss - 0.0063\n",
      "Epoch 1605:\tTraining Loss - 0.0073\n",
      "Epoch 1606:\tTraining Loss - 0.0091\n",
      "Epoch 1607:\tTraining Loss - 0.0114\n",
      "Epoch 1608:\tTraining Loss - 0.0079\n",
      "Epoch 1609:\tTraining Loss - 0.0060\n",
      "Epoch 1610:\tTraining Loss - 0.0090\n",
      "Epoch 1611:\tTraining Loss - 0.0089\n",
      "Epoch 1612:\tTraining Loss - 0.0059\n",
      "Epoch 1613:\tTraining Loss - 0.0060\n",
      "Epoch 1614:\tTraining Loss - 0.0118\n",
      "Epoch 1615:\tTraining Loss - 0.0123\n",
      "Epoch 1616:\tTraining Loss - 0.0080\n",
      "Epoch 1617:\tTraining Loss - 0.0089\n",
      "Epoch 1618:\tTraining Loss - 0.0091\n",
      "Epoch 1619:\tTraining Loss - 0.0082\n",
      "Epoch 1620:\tTraining Loss - 0.0095\n",
      "Epoch 1621:\tTraining Loss - 0.0081\n",
      "Epoch 1622:\tTraining Loss - 0.0069\n",
      "Epoch 1623:\tTraining Loss - 0.0086\n",
      "Epoch 1624:\tTraining Loss - 0.0117\n",
      "Epoch 1625:\tTraining Loss - 0.0058\n",
      "Epoch 1626:\tTraining Loss - 0.0085\n",
      "Epoch 1627:\tTraining Loss - 0.0037\n",
      "Epoch 1628:\tTraining Loss - 0.0076\n",
      "Epoch 1629:\tTraining Loss - 0.0069\n",
      "Epoch 1630:\tTraining Loss - 0.0063\n",
      "Epoch 1631:\tTraining Loss - 0.0063\n",
      "Epoch 1632:\tTraining Loss - 0.0116\n",
      "Epoch 1633:\tTraining Loss - 0.0078\n",
      "Epoch 1634:\tTraining Loss - 0.0063\n",
      "Epoch 1635:\tTraining Loss - 0.0072\n",
      "Epoch 1636:\tTraining Loss - 0.0065\n",
      "Epoch 1637:\tTraining Loss - 0.0089\n",
      "Epoch 1638:\tTraining Loss - 0.0084\n",
      "Epoch 1639:\tTraining Loss - 0.0103\n",
      "Epoch 1640:\tTraining Loss - 0.0079\n",
      "Epoch 1641:\tTraining Loss - 0.0093\n",
      "Epoch 1642:\tTraining Loss - 0.0097\n",
      "Epoch 1643:\tTraining Loss - 0.0084\n",
      "Epoch 1644:\tTraining Loss - 0.0078\n",
      "Epoch 1645:\tTraining Loss - 0.0061\n",
      "Epoch 1646:\tTraining Loss - 0.0054\n",
      "Epoch 1647:\tTraining Loss - 0.0082\n",
      "Epoch 1648:\tTraining Loss - 0.0066\n",
      "Epoch 1649:\tTraining Loss - 0.0073\n",
      "Epoch 1650:\tTraining Loss - 0.0118\n",
      "Epoch 1651:\tTraining Loss - 0.0058\n",
      "Epoch 1652:\tTraining Loss - 0.0134\n",
      "Epoch 1653:\tTraining Loss - 0.0069\n",
      "Epoch 1654:\tTraining Loss - 0.0072\n",
      "Epoch 1655:\tTraining Loss - 0.0096\n",
      "Epoch 1656:\tTraining Loss - 0.0092\n",
      "Epoch 1657:\tTraining Loss - 0.0076\n",
      "Epoch 1658:\tTraining Loss - 0.0142\n",
      "Epoch 1659:\tTraining Loss - 0.0066\n",
      "Epoch 1660:\tTraining Loss - 0.0084\n",
      "Epoch 1661:\tTraining Loss - 0.0067\n",
      "Epoch 1662:\tTraining Loss - 0.0089\n",
      "Epoch 1663:\tTraining Loss - 0.0099\n",
      "Epoch 1664:\tTraining Loss - 0.0079\n",
      "Epoch 1665:\tTraining Loss - 0.0071\n",
      "Epoch 1666:\tTraining Loss - 0.0063\n",
      "Epoch 1667:\tTraining Loss - 0.0094\n",
      "Epoch 1668:\tTraining Loss - 0.0079\n",
      "Epoch 1669:\tTraining Loss - 0.0096\n",
      "Epoch 1670:\tTraining Loss - 0.0069\n",
      "Epoch 1671:\tTraining Loss - 0.0131\n",
      "Epoch 1672:\tTraining Loss - 0.0080\n",
      "Epoch 1673:\tTraining Loss - 0.0082\n",
      "Epoch 1674:\tTraining Loss - 0.0088\n",
      "Epoch 1675:\tTraining Loss - 0.0116\n",
      "Epoch 1676:\tTraining Loss - 0.0069\n",
      "Epoch 1677:\tTraining Loss - 0.0102\n",
      "Epoch 1678:\tTraining Loss - 0.0083\n",
      "Epoch 1679:\tTraining Loss - 0.0069\n",
      "Epoch 1680:\tTraining Loss - 0.0117\n",
      "Epoch 1681:\tTraining Loss - 0.0110\n",
      "Epoch 1682:\tTraining Loss - 0.0064\n",
      "Epoch 1683:\tTraining Loss - 0.0093\n",
      "Epoch 1684:\tTraining Loss - 0.0094\n",
      "Epoch 1685:\tTraining Loss - 0.0122\n",
      "Epoch 1686:\tTraining Loss - 0.0081\n",
      "Epoch 1687:\tTraining Loss - 0.0100\n",
      "Epoch 1688:\tTraining Loss - 0.0093\n",
      "Epoch 1689:\tTraining Loss - 0.0085\n",
      "Epoch 1690:\tTraining Loss - 0.0074\n",
      "Epoch 1691:\tTraining Loss - 0.0068\n",
      "Epoch 1692:\tTraining Loss - 0.0097\n",
      "Epoch 1693:\tTraining Loss - 0.0096\n",
      "Epoch 1694:\tTraining Loss - 0.0070\n",
      "Epoch 1695:\tTraining Loss - 0.0079\n",
      "Epoch 1696:\tTraining Loss - 0.0092\n",
      "Epoch 1697:\tTraining Loss - 0.0131\n",
      "Epoch 1698:\tTraining Loss - 0.0069\n",
      "Epoch 1699:\tTraining Loss - 0.0095\n",
      "Epoch 1700:\tTraining Loss - 0.0087\n",
      "Epoch 1701:\tTraining Loss - 0.0076\n",
      "Epoch 1702:\tTraining Loss - 0.0094\n",
      "Epoch 1703:\tTraining Loss - 0.0100\n",
      "Epoch 1704:\tTraining Loss - 0.0068\n",
      "Epoch 1705:\tTraining Loss - 0.0109\n",
      "Epoch 1706:\tTraining Loss - 0.0096\n",
      "Epoch 1707:\tTraining Loss - 0.0119\n",
      "Epoch 1708:\tTraining Loss - 0.0081\n",
      "Epoch 1709:\tTraining Loss - 0.0065\n",
      "Epoch 1710:\tTraining Loss - 0.0084\n",
      "Epoch 1711:\tTraining Loss - 0.0055\n",
      "Epoch 1712:\tTraining Loss - 0.0068\n",
      "Epoch 1713:\tTraining Loss - 0.0055\n",
      "Epoch 1714:\tTraining Loss - 0.0074\n",
      "Epoch 1715:\tTraining Loss - 0.0068\n",
      "Epoch 1716:\tTraining Loss - 0.0066\n",
      "Epoch 1717:\tTraining Loss - 0.0063\n",
      "Epoch 1718:\tTraining Loss - 0.0073\n",
      "Epoch 1719:\tTraining Loss - 0.0073\n",
      "Epoch 1720:\tTraining Loss - 0.0128\n",
      "Epoch 1721:\tTraining Loss - 0.0092\n",
      "Epoch 1722:\tTraining Loss - 0.0053\n",
      "Epoch 1723:\tTraining Loss - 0.0064\n",
      "Epoch 1724:\tTraining Loss - 0.0051\n",
      "Epoch 1725:\tTraining Loss - 0.0094\n",
      "Epoch 1726:\tTraining Loss - 0.0048\n",
      "Epoch 1727:\tTraining Loss - 0.0042\n",
      "Epoch 1728:\tTraining Loss - 0.0065\n",
      "Epoch 1729:\tTraining Loss - 0.0069\n",
      "Epoch 1730:\tTraining Loss - 0.0099\n",
      "Epoch 1731:\tTraining Loss - 0.0116\n",
      "Epoch 1732:\tTraining Loss - 0.0059\n",
      "Epoch 1733:\tTraining Loss - 0.0087\n",
      "Epoch 1734:\tTraining Loss - 0.0065\n",
      "Epoch 1735:\tTraining Loss - 0.0031\n",
      "Epoch 1736:\tTraining Loss - 0.0052\n",
      "Epoch 1737:\tTraining Loss - 0.0052\n",
      "Epoch 1738:\tTraining Loss - 0.0071\n",
      "Epoch 1739:\tTraining Loss - 0.0092\n",
      "Epoch 1740:\tTraining Loss - 0.0088\n",
      "Epoch 1741:\tTraining Loss - 0.0080\n",
      "Epoch 1742:\tTraining Loss - 0.0084\n",
      "Epoch 1743:\tTraining Loss - 0.0117\n",
      "Epoch 1744:\tTraining Loss - 0.0081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1745:\tTraining Loss - 0.0073\n",
      "Epoch 1746:\tTraining Loss - 0.0057\n",
      "Epoch 1747:\tTraining Loss - 0.0092\n",
      "Epoch 1748:\tTraining Loss - 0.0069\n",
      "Epoch 1749:\tTraining Loss - 0.0079\n",
      "Epoch 1750:\tTraining Loss - 0.0061\n",
      "Epoch 1751:\tTraining Loss - 0.0068\n",
      "Epoch 1752:\tTraining Loss - 0.0074\n",
      "Epoch 1753:\tTraining Loss - 0.0057\n",
      "Epoch 1754:\tTraining Loss - 0.0093\n",
      "Epoch 1755:\tTraining Loss - 0.0075\n",
      "Epoch 1756:\tTraining Loss - 0.0088\n",
      "Epoch 1757:\tTraining Loss - 0.0064\n",
      "Epoch 1758:\tTraining Loss - 0.0073\n",
      "Epoch 1759:\tTraining Loss - 0.0053\n",
      "Epoch 1760:\tTraining Loss - 0.0062\n",
      "Epoch 1761:\tTraining Loss - 0.0053\n",
      "Epoch 1762:\tTraining Loss - 0.0107\n",
      "Epoch 1763:\tTraining Loss - 0.0072\n",
      "Epoch 1764:\tTraining Loss - 0.0067\n",
      "Epoch 1765:\tTraining Loss - 0.0066\n",
      "Epoch 1766:\tTraining Loss - 0.0066\n",
      "Epoch 1767:\tTraining Loss - 0.0066\n",
      "Epoch 1768:\tTraining Loss - 0.0070\n",
      "Epoch 1769:\tTraining Loss - 0.0114\n",
      "Epoch 1770:\tTraining Loss - 0.0048\n",
      "Epoch 1771:\tTraining Loss - 0.0075\n",
      "Epoch 1772:\tTraining Loss - 0.0099\n",
      "Epoch 1773:\tTraining Loss - 0.0066\n",
      "Epoch 1774:\tTraining Loss - 0.0069\n",
      "Epoch 1775:\tTraining Loss - 0.0071\n",
      "Epoch 1776:\tTraining Loss - 0.0064\n",
      "Epoch 1777:\tTraining Loss - 0.0055\n",
      "Epoch 1778:\tTraining Loss - 0.0134\n",
      "Epoch 1779:\tTraining Loss - 0.0091\n",
      "Epoch 1780:\tTraining Loss - 0.0056\n",
      "Epoch 1781:\tTraining Loss - 0.0063\n",
      "Epoch 1782:\tTraining Loss - 0.0050\n",
      "Epoch 1783:\tTraining Loss - 0.0092\n",
      "Epoch 1784:\tTraining Loss - 0.0085\n",
      "Epoch 1785:\tTraining Loss - 0.0052\n",
      "Epoch 1786:\tTraining Loss - 0.0078\n",
      "Epoch 1787:\tTraining Loss - 0.0118\n",
      "Epoch 1788:\tTraining Loss - 0.0048\n",
      "Epoch 1789:\tTraining Loss - 0.0096\n",
      "Epoch 1790:\tTraining Loss - 0.0100\n",
      "Epoch 1791:\tTraining Loss - 0.0085\n",
      "Epoch 1792:\tTraining Loss - 0.0052\n",
      "Epoch 1793:\tTraining Loss - 0.0076\n",
      "Epoch 1794:\tTraining Loss - 0.0081\n",
      "Epoch 1795:\tTraining Loss - 0.0080\n",
      "Epoch 1796:\tTraining Loss - 0.0077\n",
      "Epoch 1797:\tTraining Loss - 0.0090\n",
      "Epoch 1798:\tTraining Loss - 0.0069\n",
      "Epoch 1799:\tTraining Loss - 0.0106\n",
      "Epoch 1800:\tTraining Loss - 0.0082\n",
      "Epoch 1801:\tTraining Loss - 0.0075\n",
      "Epoch 1802:\tTraining Loss - 0.0083\n",
      "Epoch 1803:\tTraining Loss - 0.0082\n",
      "Epoch 1804:\tTraining Loss - 0.0085\n",
      "Epoch 1805:\tTraining Loss - 0.0121\n",
      "Epoch 1806:\tTraining Loss - 0.0076\n",
      "Epoch 1807:\tTraining Loss - 0.0074\n",
      "Epoch 1808:\tTraining Loss - 0.0084\n",
      "Epoch 1809:\tTraining Loss - 0.0074\n",
      "Epoch 1810:\tTraining Loss - 0.0094\n",
      "Epoch 1811:\tTraining Loss - 0.0129\n",
      "Epoch 1812:\tTraining Loss - 0.0054\n",
      "Epoch 1813:\tTraining Loss - 0.0030\n",
      "Epoch 1814:\tTraining Loss - 0.0071\n",
      "Epoch 1815:\tTraining Loss - 0.0044\n",
      "Epoch 1816:\tTraining Loss - 0.0076\n",
      "Epoch 1817:\tTraining Loss - 0.0075\n",
      "Epoch 1818:\tTraining Loss - 0.0057\n",
      "Epoch 1819:\tTraining Loss - 0.0049\n",
      "Epoch 1820:\tTraining Loss - 0.0083\n",
      "Epoch 1821:\tTraining Loss - 0.0077\n",
      "Epoch 1822:\tTraining Loss - 0.0074\n",
      "Epoch 1823:\tTraining Loss - 0.0091\n",
      "Epoch 1824:\tTraining Loss - 0.0073\n",
      "Epoch 1825:\tTraining Loss - 0.0105\n",
      "Epoch 1826:\tTraining Loss - 0.0071\n",
      "Epoch 1827:\tTraining Loss - 0.0069\n",
      "Epoch 1828:\tTraining Loss - 0.0072\n",
      "Epoch 1829:\tTraining Loss - 0.0050\n",
      "Epoch 1830:\tTraining Loss - 0.0077\n",
      "Epoch 1831:\tTraining Loss - 0.0108\n",
      "Epoch 1832:\tTraining Loss - 0.0060\n",
      "Epoch 1833:\tTraining Loss - 0.0085\n",
      "Epoch 1834:\tTraining Loss - 0.0071\n",
      "Epoch 1835:\tTraining Loss - 0.0073\n",
      "Epoch 1836:\tTraining Loss - 0.0071\n",
      "Epoch 1837:\tTraining Loss - 0.0049\n",
      "Epoch 1838:\tTraining Loss - 0.0060\n",
      "Epoch 1839:\tTraining Loss - 0.0080\n",
      "Epoch 1840:\tTraining Loss - 0.0082\n",
      "Epoch 1841:\tTraining Loss - 0.0082\n",
      "Epoch 1842:\tTraining Loss - 0.0050\n",
      "Epoch 1843:\tTraining Loss - 0.0080\n",
      "Epoch 1844:\tTraining Loss - 0.0095\n",
      "Epoch 1845:\tTraining Loss - 0.0052\n",
      "Epoch 1846:\tTraining Loss - 0.0051\n",
      "Epoch 1847:\tTraining Loss - 0.0083\n",
      "Epoch 1848:\tTraining Loss - 0.0045\n",
      "Epoch 1849:\tTraining Loss - 0.0095\n",
      "Epoch 1850:\tTraining Loss - 0.0065\n",
      "Epoch 1851:\tTraining Loss - 0.0101\n",
      "Epoch 1852:\tTraining Loss - 0.0108\n",
      "Epoch 1853:\tTraining Loss - 0.0077\n",
      "Epoch 1854:\tTraining Loss - 0.0048\n",
      "Epoch 1855:\tTraining Loss - 0.0068\n",
      "Epoch 1856:\tTraining Loss - 0.0098\n",
      "Epoch 1857:\tTraining Loss - 0.0118\n",
      "Epoch 1858:\tTraining Loss - 0.0085\n",
      "Epoch 1859:\tTraining Loss - 0.0109\n",
      "Epoch 1860:\tTraining Loss - 0.0063\n",
      "Epoch 1861:\tTraining Loss - 0.0069\n",
      "Epoch 1862:\tTraining Loss - 0.0117\n",
      "Epoch 1863:\tTraining Loss - 0.0067\n",
      "Epoch 1864:\tTraining Loss - 0.0045\n",
      "Epoch 1865:\tTraining Loss - 0.0041\n",
      "Epoch 1866:\tTraining Loss - 0.0074\n",
      "Epoch 1867:\tTraining Loss - 0.0079\n",
      "Epoch 1868:\tTraining Loss - 0.0084\n",
      "Epoch 1869:\tTraining Loss - 0.0049\n",
      "Epoch 1870:\tTraining Loss - 0.0073\n",
      "Epoch 1871:\tTraining Loss - 0.0085\n",
      "Epoch 1872:\tTraining Loss - 0.0068\n",
      "Epoch 1873:\tTraining Loss - 0.0063\n",
      "Epoch 1874:\tTraining Loss - 0.0082\n",
      "Epoch 1875:\tTraining Loss - 0.0074\n",
      "Epoch 1876:\tTraining Loss - 0.0049\n",
      "Epoch 1877:\tTraining Loss - 0.0042\n",
      "Epoch 1878:\tTraining Loss - 0.0072\n",
      "Epoch 1879:\tTraining Loss - 0.0108\n",
      "Epoch 1880:\tTraining Loss - 0.0081\n",
      "Epoch 1881:\tTraining Loss - 0.0101\n",
      "Epoch 1882:\tTraining Loss - 0.0037\n",
      "Epoch 1883:\tTraining Loss - 0.0083\n",
      "Epoch 1884:\tTraining Loss - 0.0083\n",
      "Epoch 1885:\tTraining Loss - 0.0069\n",
      "Epoch 1886:\tTraining Loss - 0.0121\n",
      "Epoch 1887:\tTraining Loss - 0.0161\n",
      "Epoch 1888:\tTraining Loss - 0.0078\n",
      "Epoch 1889:\tTraining Loss - 0.0103\n",
      "Epoch 1890:\tTraining Loss - 0.0083\n",
      "Epoch 1891:\tTraining Loss - 0.0075\n",
      "Epoch 1892:\tTraining Loss - 0.0065\n",
      "Epoch 1893:\tTraining Loss - 0.0071\n",
      "Epoch 1894:\tTraining Loss - 0.0056\n",
      "Epoch 1895:\tTraining Loss - 0.0087\n",
      "Epoch 1896:\tTraining Loss - 0.0070\n",
      "Epoch 1897:\tTraining Loss - 0.0093\n",
      "Epoch 1898:\tTraining Loss - 0.0046\n",
      "Epoch 1899:\tTraining Loss - 0.0071\n",
      "Epoch 1900:\tTraining Loss - 0.0070\n",
      "Epoch 1901:\tTraining Loss - 0.0068\n",
      "Epoch 1902:\tTraining Loss - 0.0065\n",
      "Epoch 1903:\tTraining Loss - 0.0072\n",
      "Epoch 1904:\tTraining Loss - 0.0044\n",
      "Epoch 1905:\tTraining Loss - 0.0117\n",
      "Epoch 1906:\tTraining Loss - 0.0065\n",
      "Epoch 1907:\tTraining Loss - 0.0072\n",
      "Epoch 1908:\tTraining Loss - 0.0085\n",
      "Epoch 1909:\tTraining Loss - 0.0038\n",
      "Epoch 1910:\tTraining Loss - 0.0079\n",
      "Epoch 1911:\tTraining Loss - 0.0048\n",
      "Epoch 1912:\tTraining Loss - 0.0102\n",
      "Epoch 1913:\tTraining Loss - 0.0063\n",
      "Epoch 1914:\tTraining Loss - 0.0048\n",
      "Epoch 1915:\tTraining Loss - 0.0102\n",
      "Epoch 1916:\tTraining Loss - 0.0085\n",
      "Epoch 1917:\tTraining Loss - 0.0064\n",
      "Epoch 1918:\tTraining Loss - 0.0044\n",
      "Epoch 1919:\tTraining Loss - 0.0082\n",
      "Epoch 1920:\tTraining Loss - 0.0068\n",
      "Epoch 1921:\tTraining Loss - 0.0072\n",
      "Epoch 1922:\tTraining Loss - 0.0097\n",
      "Epoch 1923:\tTraining Loss - 0.0062\n",
      "Epoch 1924:\tTraining Loss - 0.0057\n",
      "Epoch 1925:\tTraining Loss - 0.0041\n",
      "Epoch 1926:\tTraining Loss - 0.0078\n",
      "Epoch 1927:\tTraining Loss - 0.0079\n",
      "Epoch 1928:\tTraining Loss - 0.0056\n",
      "Epoch 1929:\tTraining Loss - 0.0102\n",
      "Epoch 1930:\tTraining Loss - 0.0080\n",
      "Epoch 1931:\tTraining Loss - 0.0069\n",
      "Epoch 1932:\tTraining Loss - 0.0082\n",
      "Epoch 1933:\tTraining Loss - 0.0121\n",
      "Epoch 1934:\tTraining Loss - 0.0053\n",
      "Epoch 1935:\tTraining Loss - 0.0057\n",
      "Epoch 1936:\tTraining Loss - 0.0065\n",
      "Epoch 1937:\tTraining Loss - 0.0061\n",
      "Epoch 1938:\tTraining Loss - 0.0102\n",
      "Epoch 1939:\tTraining Loss - 0.0121\n",
      "Epoch 1940:\tTraining Loss - 0.0071\n",
      "Epoch 1941:\tTraining Loss - 0.0079\n",
      "Epoch 1942:\tTraining Loss - 0.0049\n",
      "Epoch 1943:\tTraining Loss - 0.0069\n",
      "Epoch 1944:\tTraining Loss - 0.0072\n",
      "Epoch 1945:\tTraining Loss - 0.0051\n",
      "Epoch 1946:\tTraining Loss - 0.0059\n",
      "Epoch 1947:\tTraining Loss - 0.0042\n",
      "Epoch 1948:\tTraining Loss - 0.0082\n",
      "Epoch 1949:\tTraining Loss - 0.0050\n",
      "Epoch 1950:\tTraining Loss - 0.0076\n",
      "Epoch 1951:\tTraining Loss - 0.0070\n",
      "Epoch 1952:\tTraining Loss - 0.0098\n",
      "Epoch 1953:\tTraining Loss - 0.0050\n",
      "Epoch 1954:\tTraining Loss - 0.0083\n",
      "Epoch 1955:\tTraining Loss - 0.0058\n",
      "Epoch 1956:\tTraining Loss - 0.0081\n",
      "Epoch 1957:\tTraining Loss - 0.0086\n",
      "Epoch 1958:\tTraining Loss - 0.0072\n",
      "Epoch 1959:\tTraining Loss - 0.0066\n",
      "Epoch 1960:\tTraining Loss - 0.0051\n",
      "Epoch 1961:\tTraining Loss - 0.0040\n",
      "Epoch 1962:\tTraining Loss - 0.0082\n",
      "Epoch 1963:\tTraining Loss - 0.0076\n",
      "Epoch 1964:\tTraining Loss - 0.0054\n",
      "Epoch 1965:\tTraining Loss - 0.0065\n",
      "Epoch 1966:\tTraining Loss - 0.0045\n",
      "Epoch 1967:\tTraining Loss - 0.0054\n",
      "Epoch 1968:\tTraining Loss - 0.0042\n",
      "Epoch 1969:\tTraining Loss - 0.0088\n",
      "Epoch 1970:\tTraining Loss - 0.0107\n",
      "Epoch 1971:\tTraining Loss - 0.0095\n",
      "Epoch 1972:\tTraining Loss - 0.0048\n",
      "Epoch 1973:\tTraining Loss - 0.0061\n",
      "Epoch 1974:\tTraining Loss - 0.0081\n",
      "Epoch 1975:\tTraining Loss - 0.0088\n",
      "Epoch 1976:\tTraining Loss - 0.0119\n",
      "Epoch 1977:\tTraining Loss - 0.0099\n",
      "Epoch 1978:\tTraining Loss - 0.0078\n",
      "Epoch 1979:\tTraining Loss - 0.0114\n",
      "Epoch 1980:\tTraining Loss - 0.0081\n",
      "Epoch 1981:\tTraining Loss - 0.0097\n",
      "Epoch 1982:\tTraining Loss - 0.0056\n",
      "Epoch 1983:\tTraining Loss - 0.0120\n",
      "Epoch 1984:\tTraining Loss - 0.0099\n",
      "Epoch 1985:\tTraining Loss - 0.0083\n",
      "Epoch 1986:\tTraining Loss - 0.0086\n",
      "Epoch 1987:\tTraining Loss - 0.0043\n",
      "Epoch 1988:\tTraining Loss - 0.0058\n",
      "Epoch 1989:\tTraining Loss - 0.0120\n",
      "Epoch 1990:\tTraining Loss - 0.0085\n",
      "Epoch 1991:\tTraining Loss - 0.0057\n",
      "Epoch 1992:\tTraining Loss - 0.0095\n",
      "Epoch 1993:\tTraining Loss - 0.0089\n",
      "Epoch 1994:\tTraining Loss - 0.0108\n",
      "Epoch 1995:\tTraining Loss - 0.0058\n",
      "Epoch 1996:\tTraining Loss - 0.0112\n",
      "Epoch 1997:\tTraining Loss - 0.0082\n",
      "Epoch 1998:\tTraining Loss - 0.0060\n",
      "Epoch 1999:\tTraining Loss - 0.0072\n",
      "Epoch 2000:\tTraining Loss - 0.0075\n",
      "Epoch 2001:\tTraining Loss - 0.0073\n",
      "Epoch 2002:\tTraining Loss - 0.0039\n",
      "Epoch 2003:\tTraining Loss - 0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2004:\tTraining Loss - 0.0066\n",
      "Epoch 2005:\tTraining Loss - 0.0078\n",
      "Epoch 2006:\tTraining Loss - 0.0082\n",
      "Epoch 2007:\tTraining Loss - 0.0077\n",
      "Epoch 2008:\tTraining Loss - 0.0111\n",
      "Epoch 2009:\tTraining Loss - 0.0031\n",
      "Epoch 2010:\tTraining Loss - 0.0057\n",
      "Epoch 2011:\tTraining Loss - 0.0060\n",
      "Epoch 2012:\tTraining Loss - 0.0072\n",
      "Epoch 2013:\tTraining Loss - 0.0080\n",
      "Epoch 2014:\tTraining Loss - 0.0068\n",
      "Epoch 2015:\tTraining Loss - 0.0056\n",
      "Epoch 2016:\tTraining Loss - 0.0063\n",
      "Epoch 2017:\tTraining Loss - 0.0075\n",
      "Epoch 2018:\tTraining Loss - 0.0081\n",
      "Epoch 2019:\tTraining Loss - 0.0072\n",
      "Epoch 2020:\tTraining Loss - 0.0079\n",
      "Epoch 2021:\tTraining Loss - 0.0039\n",
      "Epoch 2022:\tTraining Loss - 0.0118\n",
      "Epoch 2023:\tTraining Loss - 0.0067\n",
      "Epoch 2024:\tTraining Loss - 0.0063\n",
      "Epoch 2025:\tTraining Loss - 0.0086\n",
      "Epoch 2026:\tTraining Loss - 0.0065\n",
      "Epoch 2027:\tTraining Loss - 0.0084\n",
      "Epoch 2028:\tTraining Loss - 0.0086\n",
      "Epoch 2029:\tTraining Loss - 0.0057\n",
      "Epoch 2030:\tTraining Loss - 0.0037\n",
      "Epoch 2031:\tTraining Loss - 0.0063\n",
      "Epoch 2032:\tTraining Loss - 0.0055\n",
      "Epoch 2033:\tTraining Loss - 0.0109\n",
      "Epoch 2034:\tTraining Loss - 0.0068\n",
      "Epoch 2035:\tTraining Loss - 0.0077\n",
      "Epoch 2036:\tTraining Loss - 0.0055\n",
      "Epoch 2037:\tTraining Loss - 0.0083\n",
      "Epoch 2038:\tTraining Loss - 0.0051\n",
      "Epoch 2039:\tTraining Loss - 0.0059\n",
      "Epoch 2040:\tTraining Loss - 0.0057\n",
      "Epoch 2041:\tTraining Loss - 0.0063\n",
      "Epoch 2042:\tTraining Loss - 0.0091\n",
      "Epoch 2043:\tTraining Loss - 0.0056\n",
      "Epoch 2044:\tTraining Loss - 0.0073\n",
      "Epoch 2045:\tTraining Loss - 0.0065\n",
      "Epoch 2046:\tTraining Loss - 0.0112\n",
      "Epoch 2047:\tTraining Loss - 0.0065\n",
      "Epoch 2048:\tTraining Loss - 0.0074\n",
      "Epoch 2049:\tTraining Loss - 0.0048\n",
      "Epoch 2050:\tTraining Loss - 0.0069\n",
      "Epoch 2051:\tTraining Loss - 0.0079\n",
      "Epoch 2052:\tTraining Loss - 0.0057\n",
      "Epoch 2053:\tTraining Loss - 0.0077\n",
      "Epoch 2054:\tTraining Loss - 0.0080\n",
      "Epoch 2055:\tTraining Loss - 0.0035\n",
      "Epoch 2056:\tTraining Loss - 0.0098\n",
      "Epoch 2057:\tTraining Loss - 0.0040\n",
      "Epoch 2058:\tTraining Loss - 0.0059\n",
      "Epoch 2059:\tTraining Loss - 0.0075\n",
      "Epoch 2060:\tTraining Loss - 0.0070\n",
      "Epoch 2061:\tTraining Loss - 0.0054\n",
      "Epoch 2062:\tTraining Loss - 0.0052\n",
      "Epoch 2063:\tTraining Loss - 0.0057\n",
      "Epoch 2064:\tTraining Loss - 0.0068\n",
      "Epoch 2065:\tTraining Loss - 0.0061\n",
      "Epoch 2066:\tTraining Loss - 0.0032\n",
      "Epoch 2067:\tTraining Loss - 0.0063\n",
      "Epoch 2068:\tTraining Loss - 0.0063\n",
      "Epoch 2069:\tTraining Loss - 0.0079\n",
      "Epoch 2070:\tTraining Loss - 0.0070\n",
      "Epoch 2071:\tTraining Loss - 0.0055\n",
      "Epoch 2072:\tTraining Loss - 0.0135\n",
      "Epoch 2073:\tTraining Loss - 0.0041\n",
      "Epoch 2074:\tTraining Loss - 0.0053\n",
      "Epoch 2075:\tTraining Loss - 0.0089\n",
      "Epoch 2076:\tTraining Loss - 0.0092\n",
      "Epoch 2077:\tTraining Loss - 0.0057\n",
      "Epoch 2078:\tTraining Loss - 0.0054\n",
      "Epoch 2079:\tTraining Loss - 0.0050\n",
      "Epoch 2080:\tTraining Loss - 0.0072\n",
      "Epoch 2081:\tTraining Loss - 0.0121\n",
      "Epoch 2082:\tTraining Loss - 0.0061\n",
      "Epoch 2083:\tTraining Loss - 0.0053\n",
      "Epoch 2084:\tTraining Loss - 0.0062\n",
      "Epoch 2085:\tTraining Loss - 0.0061\n",
      "Epoch 2086:\tTraining Loss - 0.0126\n",
      "Epoch 2087:\tTraining Loss - 0.0054\n",
      "Epoch 2088:\tTraining Loss - 0.0058\n",
      "Epoch 2089:\tTraining Loss - 0.0074\n",
      "Epoch 2090:\tTraining Loss - 0.0056\n",
      "Epoch 2091:\tTraining Loss - 0.0064\n",
      "Epoch 2092:\tTraining Loss - 0.0059\n",
      "Epoch 2093:\tTraining Loss - 0.0102\n",
      "Epoch 2094:\tTraining Loss - 0.0076\n",
      "Epoch 2095:\tTraining Loss - 0.0036\n",
      "Epoch 2096:\tTraining Loss - 0.0054\n",
      "Epoch 2097:\tTraining Loss - 0.0063\n",
      "Epoch 2098:\tTraining Loss - 0.0071\n",
      "Epoch 2099:\tTraining Loss - 0.0082\n",
      "Epoch 2100:\tTraining Loss - 0.0049\n",
      "Epoch 2101:\tTraining Loss - 0.0108\n",
      "Epoch 2102:\tTraining Loss - 0.0071\n",
      "Epoch 2103:\tTraining Loss - 0.0065\n",
      "Epoch 2104:\tTraining Loss - 0.0054\n",
      "Epoch 2105:\tTraining Loss - 0.0044\n",
      "Epoch 2106:\tTraining Loss - 0.0090\n",
      "Epoch 2107:\tTraining Loss - 0.0064\n",
      "Epoch 2108:\tTraining Loss - 0.0077\n",
      "Epoch 2109:\tTraining Loss - 0.0056\n",
      "Epoch 2110:\tTraining Loss - 0.0068\n",
      "Epoch 2111:\tTraining Loss - 0.0082\n",
      "Epoch 2112:\tTraining Loss - 0.0062\n",
      "Epoch 2113:\tTraining Loss - 0.0058\n",
      "Epoch 2114:\tTraining Loss - 0.0076\n",
      "Epoch 2115:\tTraining Loss - 0.0063\n",
      "Epoch 2116:\tTraining Loss - 0.0039\n",
      "Epoch 2117:\tTraining Loss - 0.0050\n",
      "Epoch 2118:\tTraining Loss - 0.0034\n",
      "Epoch 2119:\tTraining Loss - 0.0060\n",
      "Epoch 2120:\tTraining Loss - 0.0051\n",
      "Epoch 2121:\tTraining Loss - 0.0057\n",
      "Epoch 2122:\tTraining Loss - 0.0068\n",
      "Epoch 2123:\tTraining Loss - 0.0086\n",
      "Epoch 2124:\tTraining Loss - 0.0095\n",
      "Epoch 2125:\tTraining Loss - 0.0059\n",
      "Epoch 2126:\tTraining Loss - 0.0069\n",
      "Epoch 2127:\tTraining Loss - 0.0072\n",
      "Epoch 2128:\tTraining Loss - 0.0050\n",
      "Epoch 2129:\tTraining Loss - 0.0084\n",
      "Epoch 2130:\tTraining Loss - 0.0043\n",
      "Epoch 2131:\tTraining Loss - 0.0060\n",
      "Epoch 2132:\tTraining Loss - 0.0061\n",
      "Epoch 2133:\tTraining Loss - 0.0071\n",
      "Epoch 2134:\tTraining Loss - 0.0079\n",
      "Epoch 2135:\tTraining Loss - 0.0077\n",
      "Epoch 2136:\tTraining Loss - 0.0052\n",
      "Epoch 2137:\tTraining Loss - 0.0096\n",
      "Epoch 2138:\tTraining Loss - 0.0053\n",
      "Epoch 2139:\tTraining Loss - 0.0069\n",
      "Epoch 2140:\tTraining Loss - 0.0058\n",
      "Epoch 2141:\tTraining Loss - 0.0064\n",
      "Epoch 2142:\tTraining Loss - 0.0069\n",
      "Epoch 2143:\tTraining Loss - 0.0056\n",
      "Epoch 2144:\tTraining Loss - 0.0050\n",
      "Epoch 2145:\tTraining Loss - 0.0089\n",
      "Epoch 2146:\tTraining Loss - 0.0066\n",
      "Epoch 2147:\tTraining Loss - 0.0038\n",
      "Epoch 2148:\tTraining Loss - 0.0042\n",
      "Epoch 2149:\tTraining Loss - 0.0066\n",
      "Epoch 2150:\tTraining Loss - 0.0056\n",
      "Epoch 2151:\tTraining Loss - 0.0094\n",
      "Epoch 2152:\tTraining Loss - 0.0060\n",
      "Epoch 2153:\tTraining Loss - 0.0129\n",
      "Epoch 2154:\tTraining Loss - 0.0065\n",
      "Epoch 2155:\tTraining Loss - 0.0052\n",
      "Epoch 2156:\tTraining Loss - 0.0061\n",
      "Epoch 2157:\tTraining Loss - 0.0072\n",
      "Epoch 2158:\tTraining Loss - 0.0059\n",
      "Epoch 2159:\tTraining Loss - 0.0062\n",
      "Epoch 2160:\tTraining Loss - 0.0053\n",
      "Epoch 2161:\tTraining Loss - 0.0087\n",
      "Epoch 2162:\tTraining Loss - 0.0037\n",
      "Epoch 2163:\tTraining Loss - 0.0064\n",
      "Epoch 2164:\tTraining Loss - 0.0048\n",
      "Epoch 2165:\tTraining Loss - 0.0092\n",
      "Epoch 2166:\tTraining Loss - 0.0063\n",
      "Epoch 2167:\tTraining Loss - 0.0082\n",
      "Epoch 2168:\tTraining Loss - 0.0071\n",
      "Epoch 2169:\tTraining Loss - 0.0113\n",
      "Epoch 2170:\tTraining Loss - 0.0049\n",
      "Epoch 2171:\tTraining Loss - 0.0082\n",
      "Epoch 2172:\tTraining Loss - 0.0061\n",
      "Epoch 2173:\tTraining Loss - 0.0093\n",
      "Epoch 2174:\tTraining Loss - 0.0072\n",
      "Epoch 2175:\tTraining Loss - 0.0089\n",
      "Epoch 2176:\tTraining Loss - 0.0069\n",
      "Epoch 2177:\tTraining Loss - 0.0084\n",
      "Epoch 2178:\tTraining Loss - 0.0048\n",
      "Epoch 2179:\tTraining Loss - 0.0170\n",
      "Epoch 2180:\tTraining Loss - 0.0064\n",
      "Epoch 2181:\tTraining Loss - 0.0059\n",
      "Epoch 2182:\tTraining Loss - 0.0026\n",
      "Epoch 2183:\tTraining Loss - 0.0118\n",
      "Epoch 2184:\tTraining Loss - 0.0084\n",
      "Epoch 2185:\tTraining Loss - 0.0101\n",
      "Epoch 2186:\tTraining Loss - 0.0093\n",
      "Epoch 2187:\tTraining Loss - 0.0114\n",
      "Epoch 2188:\tTraining Loss - 0.0048\n",
      "Epoch 2189:\tTraining Loss - 0.0042\n",
      "Epoch 2190:\tTraining Loss - 0.0067\n",
      "Epoch 2191:\tTraining Loss - 0.0048\n",
      "Epoch 2192:\tTraining Loss - 0.0068\n",
      "Epoch 2193:\tTraining Loss - 0.0048\n",
      "Epoch 2194:\tTraining Loss - 0.0097\n",
      "Epoch 2195:\tTraining Loss - 0.0118\n",
      "Epoch 2196:\tTraining Loss - 0.0083\n",
      "Epoch 2197:\tTraining Loss - 0.0063\n",
      "Epoch 2198:\tTraining Loss - 0.0091\n",
      "Epoch 2199:\tTraining Loss - 0.0061\n",
      "Epoch 2200:\tTraining Loss - 0.0062\n",
      "Epoch 2201:\tTraining Loss - 0.0046\n",
      "Epoch 2202:\tTraining Loss - 0.0062\n",
      "Epoch 2203:\tTraining Loss - 0.0068\n",
      "Epoch 2204:\tTraining Loss - 0.0062\n",
      "Epoch 2205:\tTraining Loss - 0.0060\n",
      "Epoch 2206:\tTraining Loss - 0.0054\n",
      "Epoch 2207:\tTraining Loss - 0.0067\n",
      "Epoch 2208:\tTraining Loss - 0.0049\n",
      "Epoch 2209:\tTraining Loss - 0.0057\n",
      "Epoch 2210:\tTraining Loss - 0.0052\n",
      "Epoch 2211:\tTraining Loss - 0.0137\n",
      "Epoch 2212:\tTraining Loss - 0.0064\n",
      "Epoch 2213:\tTraining Loss - 0.0040\n",
      "Epoch 2214:\tTraining Loss - 0.0060\n",
      "Epoch 2215:\tTraining Loss - 0.0074\n",
      "Epoch 2216:\tTraining Loss - 0.0068\n",
      "Epoch 2217:\tTraining Loss - 0.0039\n",
      "Epoch 2218:\tTraining Loss - 0.0039\n",
      "Epoch 2219:\tTraining Loss - 0.0079\n",
      "Epoch 2220:\tTraining Loss - 0.0077\n",
      "Epoch 2221:\tTraining Loss - 0.0093\n",
      "Epoch 2222:\tTraining Loss - 0.0040\n",
      "Epoch 2223:\tTraining Loss - 0.0052\n",
      "Epoch 2224:\tTraining Loss - 0.0047\n",
      "Epoch 2225:\tTraining Loss - 0.0026\n",
      "Epoch 2226:\tTraining Loss - 0.0070\n",
      "Epoch 2227:\tTraining Loss - 0.0048\n",
      "Epoch 2228:\tTraining Loss - 0.0087\n",
      "Epoch 2229:\tTraining Loss - 0.0059\n",
      "Epoch 2230:\tTraining Loss - 0.0076\n",
      "Epoch 2231:\tTraining Loss - 0.0048\n",
      "Epoch 2232:\tTraining Loss - 0.0138\n",
      "Epoch 2233:\tTraining Loss - 0.0074\n",
      "Epoch 2234:\tTraining Loss - 0.0040\n",
      "Epoch 2235:\tTraining Loss - 0.0121\n",
      "Epoch 2236:\tTraining Loss - 0.0078\n",
      "Epoch 2237:\tTraining Loss - 0.0044\n",
      "Epoch 2238:\tTraining Loss - 0.0061\n",
      "Epoch 2239:\tTraining Loss - 0.0067\n",
      "Epoch 2240:\tTraining Loss - 0.0071\n",
      "Epoch 2241:\tTraining Loss - 0.0085\n",
      "Epoch 2242:\tTraining Loss - 0.0075\n",
      "Epoch 2243:\tTraining Loss - 0.0075\n",
      "Epoch 2244:\tTraining Loss - 0.0090\n",
      "Epoch 2245:\tTraining Loss - 0.0101\n",
      "Epoch 2246:\tTraining Loss - 0.0067\n",
      "Epoch 2247:\tTraining Loss - 0.0027\n",
      "Epoch 2248:\tTraining Loss - 0.0105\n",
      "Epoch 2249:\tTraining Loss - 0.0068\n",
      "Epoch 2250:\tTraining Loss - 0.0080\n",
      "Epoch 2251:\tTraining Loss - 0.0076\n",
      "Epoch 2252:\tTraining Loss - 0.0052\n",
      "Epoch 2253:\tTraining Loss - 0.0069\n",
      "Epoch 2254:\tTraining Loss - 0.0058\n",
      "Epoch 2255:\tTraining Loss - 0.0069\n",
      "Epoch 2256:\tTraining Loss - 0.0110\n",
      "Epoch 2257:\tTraining Loss - 0.0065\n",
      "Epoch 2258:\tTraining Loss - 0.0087\n",
      "Epoch 2259:\tTraining Loss - 0.0069\n",
      "Epoch 2260:\tTraining Loss - 0.0071\n",
      "Epoch 2261:\tTraining Loss - 0.0099\n",
      "Epoch 2262:\tTraining Loss - 0.0096\n",
      "Epoch 2263:\tTraining Loss - 0.0086\n",
      "Epoch 2264:\tTraining Loss - 0.0075\n",
      "Epoch 2265:\tTraining Loss - 0.0083\n",
      "Epoch 2266:\tTraining Loss - 0.0050\n",
      "Epoch 2267:\tTraining Loss - 0.0112\n",
      "Epoch 2268:\tTraining Loss - 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2269:\tTraining Loss - 0.0074\n",
      "Epoch 2270:\tTraining Loss - 0.0042\n",
      "Epoch 2271:\tTraining Loss - 0.0079\n",
      "Epoch 2272:\tTraining Loss - 0.0077\n",
      "Epoch 2273:\tTraining Loss - 0.0062\n",
      "Epoch 2274:\tTraining Loss - 0.0063\n",
      "Epoch 2275:\tTraining Loss - 0.0107\n",
      "Epoch 2276:\tTraining Loss - 0.0050\n",
      "Epoch 2277:\tTraining Loss - 0.0122\n",
      "Epoch 2278:\tTraining Loss - 0.0076\n",
      "Epoch 2279:\tTraining Loss - 0.0076\n",
      "Epoch 2280:\tTraining Loss - 0.0098\n",
      "Epoch 2281:\tTraining Loss - 0.0057\n",
      "Epoch 2282:\tTraining Loss - 0.0040\n",
      "Epoch 2283:\tTraining Loss - 0.0057\n",
      "Epoch 2284:\tTraining Loss - 0.0062\n",
      "Epoch 2285:\tTraining Loss - 0.0109\n",
      "Epoch 2286:\tTraining Loss - 0.0035\n",
      "Epoch 2287:\tTraining Loss - 0.0071\n",
      "Epoch 2288:\tTraining Loss - 0.0074\n",
      "Epoch 2289:\tTraining Loss - 0.0079\n",
      "Epoch 2290:\tTraining Loss - 0.0054\n",
      "Epoch 2291:\tTraining Loss - 0.0076\n",
      "Epoch 2292:\tTraining Loss - 0.0082\n",
      "Epoch 2293:\tTraining Loss - 0.0059\n",
      "Epoch 2294:\tTraining Loss - 0.0051\n",
      "Epoch 2295:\tTraining Loss - 0.0071\n",
      "Epoch 2296:\tTraining Loss - 0.0061\n",
      "Epoch 2297:\tTraining Loss - 0.0067\n",
      "Epoch 2298:\tTraining Loss - 0.0073\n",
      "Epoch 2299:\tTraining Loss - 0.0042\n",
      "Epoch 2300:\tTraining Loss - 0.0076\n",
      "Epoch 2301:\tTraining Loss - 0.0085\n",
      "Epoch 2302:\tTraining Loss - 0.0054\n",
      "Epoch 2303:\tTraining Loss - 0.0098\n",
      "Epoch 2304:\tTraining Loss - 0.0080\n",
      "Epoch 2305:\tTraining Loss - 0.0104\n",
      "Epoch 2306:\tTraining Loss - 0.0038\n",
      "Epoch 2307:\tTraining Loss - 0.0053\n",
      "Epoch 2308:\tTraining Loss - 0.0072\n",
      "Epoch 2309:\tTraining Loss - 0.0073\n",
      "Epoch 2310:\tTraining Loss - 0.0068\n",
      "Epoch 2311:\tTraining Loss - 0.0056\n",
      "Epoch 2312:\tTraining Loss - 0.0039\n",
      "Epoch 2313:\tTraining Loss - 0.0048\n",
      "Epoch 2314:\tTraining Loss - 0.0060\n",
      "Epoch 2315:\tTraining Loss - 0.0061\n",
      "Epoch 2316:\tTraining Loss - 0.0056\n",
      "Epoch 2317:\tTraining Loss - 0.0066\n",
      "Epoch 2318:\tTraining Loss - 0.0033\n",
      "Epoch 2319:\tTraining Loss - 0.0049\n",
      "Epoch 2320:\tTraining Loss - 0.0091\n",
      "Epoch 2321:\tTraining Loss - 0.0056\n",
      "Epoch 2322:\tTraining Loss - 0.0099\n",
      "Epoch 2323:\tTraining Loss - 0.0051\n",
      "Epoch 2324:\tTraining Loss - 0.0068\n",
      "Epoch 2325:\tTraining Loss - 0.0090\n",
      "Epoch 2326:\tTraining Loss - 0.0083\n",
      "Epoch 2327:\tTraining Loss - 0.0068\n",
      "Epoch 2328:\tTraining Loss - 0.0053\n",
      "Epoch 2329:\tTraining Loss - 0.0049\n",
      "Epoch 2330:\tTraining Loss - 0.0067\n",
      "Epoch 2331:\tTraining Loss - 0.0056\n",
      "Epoch 2332:\tTraining Loss - 0.0136\n",
      "Epoch 2333:\tTraining Loss - 0.0055\n",
      "Epoch 2334:\tTraining Loss - 0.0109\n",
      "Epoch 2335:\tTraining Loss - 0.0092\n",
      "Epoch 2336:\tTraining Loss - 0.0078\n",
      "Epoch 2337:\tTraining Loss - 0.0087\n",
      "Epoch 2338:\tTraining Loss - 0.0085\n",
      "Epoch 2339:\tTraining Loss - 0.0120\n",
      "Epoch 2340:\tTraining Loss - 0.0029\n",
      "Epoch 2341:\tTraining Loss - 0.0062\n",
      "Epoch 2342:\tTraining Loss - 0.0058\n",
      "Epoch 2343:\tTraining Loss - 0.0042\n",
      "Epoch 2344:\tTraining Loss - 0.0080\n",
      "Epoch 2345:\tTraining Loss - 0.0086\n",
      "Epoch 2346:\tTraining Loss - 0.0046\n",
      "Epoch 2347:\tTraining Loss - 0.0080\n",
      "Epoch 2348:\tTraining Loss - 0.0040\n",
      "Epoch 2349:\tTraining Loss - 0.0063\n",
      "Epoch 2350:\tTraining Loss - 0.0055\n",
      "Epoch 2351:\tTraining Loss - 0.0084\n",
      "Epoch 2352:\tTraining Loss - 0.0091\n",
      "Epoch 2353:\tTraining Loss - 0.0060\n",
      "Epoch 2354:\tTraining Loss - 0.0054\n",
      "Epoch 2355:\tTraining Loss - 0.0141\n",
      "Epoch 2356:\tTraining Loss - 0.0088\n",
      "Epoch 2357:\tTraining Loss - 0.0093\n",
      "Epoch 2358:\tTraining Loss - 0.0072\n",
      "Epoch 2359:\tTraining Loss - 0.0076\n",
      "Epoch 2360:\tTraining Loss - 0.0075\n",
      "Epoch 2361:\tTraining Loss - 0.0055\n",
      "Epoch 2362:\tTraining Loss - 0.0061\n",
      "Epoch 2363:\tTraining Loss - 0.0098\n",
      "Epoch 2364:\tTraining Loss - 0.0071\n",
      "Epoch 2365:\tTraining Loss - 0.0060\n",
      "Epoch 2366:\tTraining Loss - 0.0073\n",
      "Epoch 2367:\tTraining Loss - 0.0062\n",
      "Epoch 2368:\tTraining Loss - 0.0073\n",
      "Epoch 2369:\tTraining Loss - 0.0042\n",
      "Epoch 2370:\tTraining Loss - 0.0107\n",
      "Epoch 2371:\tTraining Loss - 0.0066\n",
      "Epoch 2372:\tTraining Loss - 0.0097\n",
      "Epoch 2373:\tTraining Loss - 0.0067\n",
      "Epoch 2374:\tTraining Loss - 0.0060\n",
      "Epoch 2375:\tTraining Loss - 0.0046\n",
      "Epoch 2376:\tTraining Loss - 0.0044\n",
      "Epoch 2377:\tTraining Loss - 0.0056\n",
      "Epoch 2378:\tTraining Loss - 0.0072\n",
      "Epoch 2379:\tTraining Loss - 0.0097\n",
      "Epoch 2380:\tTraining Loss - 0.0056\n",
      "Epoch 2381:\tTraining Loss - 0.0044\n",
      "Epoch 2382:\tTraining Loss - 0.0075\n",
      "Epoch 2383:\tTraining Loss - 0.0076\n",
      "Epoch 2384:\tTraining Loss - 0.0034\n",
      "Epoch 2385:\tTraining Loss - 0.0061\n",
      "Epoch 2386:\tTraining Loss - 0.0092\n",
      "Epoch 2387:\tTraining Loss - 0.0093\n",
      "Epoch 2388:\tTraining Loss - 0.0048\n",
      "Epoch 2389:\tTraining Loss - 0.0050\n",
      "Epoch 2390:\tTraining Loss - 0.0083\n",
      "Epoch 2391:\tTraining Loss - 0.0089\n",
      "Epoch 2392:\tTraining Loss - 0.0056\n",
      "Epoch 2393:\tTraining Loss - 0.0070\n",
      "Epoch 2394:\tTraining Loss - 0.0074\n",
      "Epoch 2395:\tTraining Loss - 0.0051\n",
      "Epoch 2396:\tTraining Loss - 0.0040\n",
      "Epoch 2397:\tTraining Loss - 0.0054\n",
      "Epoch 2398:\tTraining Loss - 0.0114\n",
      "Epoch 2399:\tTraining Loss - 0.0093\n",
      "Epoch 2400:\tTraining Loss - 0.0081\n",
      "Epoch 2401:\tTraining Loss - 0.0055\n",
      "Epoch 2402:\tTraining Loss - 0.0066\n",
      "Epoch 2403:\tTraining Loss - 0.0052\n",
      "Epoch 2404:\tTraining Loss - 0.0042\n",
      "Epoch 2405:\tTraining Loss - 0.0059\n",
      "Epoch 2406:\tTraining Loss - 0.0064\n",
      "Epoch 2407:\tTraining Loss - 0.0056\n",
      "Epoch 2408:\tTraining Loss - 0.0071\n",
      "Epoch 2409:\tTraining Loss - 0.0084\n",
      "Epoch 2410:\tTraining Loss - 0.0072\n",
      "Epoch 2411:\tTraining Loss - 0.0067\n",
      "Epoch 2412:\tTraining Loss - 0.0081\n",
      "Epoch 2413:\tTraining Loss - 0.0051\n",
      "Epoch 2414:\tTraining Loss - 0.0066\n",
      "Epoch 2415:\tTraining Loss - 0.0056\n",
      "Epoch 2416:\tTraining Loss - 0.0089\n",
      "Epoch 2417:\tTraining Loss - 0.0073\n",
      "Epoch 2418:\tTraining Loss - 0.0079\n",
      "Epoch 2419:\tTraining Loss - 0.0077\n",
      "Epoch 2420:\tTraining Loss - 0.0080\n",
      "Epoch 2421:\tTraining Loss - 0.0083\n",
      "Epoch 2422:\tTraining Loss - 0.0074\n",
      "Epoch 2423:\tTraining Loss - 0.0052\n",
      "Epoch 2424:\tTraining Loss - 0.0043\n",
      "Epoch 2425:\tTraining Loss - 0.0057\n",
      "Epoch 2426:\tTraining Loss - 0.0061\n",
      "Epoch 2427:\tTraining Loss - 0.0071\n",
      "Epoch 2428:\tTraining Loss - 0.0057\n",
      "Epoch 2429:\tTraining Loss - 0.0058\n",
      "Epoch 2430:\tTraining Loss - 0.0061\n",
      "Epoch 2431:\tTraining Loss - 0.0092\n",
      "Epoch 2432:\tTraining Loss - 0.0070\n",
      "Epoch 2433:\tTraining Loss - 0.0052\n",
      "Epoch 2434:\tTraining Loss - 0.0065\n",
      "Epoch 2435:\tTraining Loss - 0.0077\n",
      "Epoch 2436:\tTraining Loss - 0.0043\n",
      "Epoch 2437:\tTraining Loss - 0.0059\n",
      "Epoch 2438:\tTraining Loss - 0.0074\n",
      "Epoch 2439:\tTraining Loss - 0.0075\n",
      "Epoch 2440:\tTraining Loss - 0.0039\n",
      "Epoch 2441:\tTraining Loss - 0.0038\n",
      "Epoch 2442:\tTraining Loss - 0.0113\n",
      "Epoch 2443:\tTraining Loss - 0.0047\n",
      "Epoch 2444:\tTraining Loss - 0.0057\n",
      "Epoch 2445:\tTraining Loss - 0.0044\n",
      "Epoch 2446:\tTraining Loss - 0.0074\n",
      "Epoch 2447:\tTraining Loss - 0.0083\n",
      "Epoch 2448:\tTraining Loss - 0.0117\n",
      "Epoch 2449:\tTraining Loss - 0.0060\n",
      "Epoch 2450:\tTraining Loss - 0.0052\n",
      "Epoch 2451:\tTraining Loss - 0.0105\n",
      "Epoch 2452:\tTraining Loss - 0.0056\n",
      "Epoch 2453:\tTraining Loss - 0.0065\n",
      "Epoch 2454:\tTraining Loss - 0.0056\n",
      "Epoch 2455:\tTraining Loss - 0.0116\n",
      "Epoch 2456:\tTraining Loss - 0.0078\n",
      "Epoch 2457:\tTraining Loss - 0.0051\n",
      "Epoch 2458:\tTraining Loss - 0.0057\n",
      "Epoch 2459:\tTraining Loss - 0.0060\n",
      "Epoch 2460:\tTraining Loss - 0.0063\n",
      "Epoch 2461:\tTraining Loss - 0.0080\n",
      "Epoch 2462:\tTraining Loss - 0.0090\n",
      "Epoch 2463:\tTraining Loss - 0.0031\n",
      "Epoch 2464:\tTraining Loss - 0.0061\n",
      "Epoch 2465:\tTraining Loss - 0.0086\n",
      "Epoch 2466:\tTraining Loss - 0.0075\n",
      "Epoch 2467:\tTraining Loss - 0.0051\n",
      "Epoch 2468:\tTraining Loss - 0.0053\n",
      "Epoch 2469:\tTraining Loss - 0.0108\n",
      "Epoch 2470:\tTraining Loss - 0.0051\n",
      "Epoch 2471:\tTraining Loss - 0.0047\n",
      "Epoch 2472:\tTraining Loss - 0.0040\n",
      "Epoch 2473:\tTraining Loss - 0.0067\n",
      "Epoch 2474:\tTraining Loss - 0.0035\n",
      "Epoch 2475:\tTraining Loss - 0.0131\n",
      "Epoch 2476:\tTraining Loss - 0.0080\n",
      "Epoch 2477:\tTraining Loss - 0.0062\n",
      "Epoch 2478:\tTraining Loss - 0.0053\n",
      "Epoch 2479:\tTraining Loss - 0.0073\n",
      "Epoch 2480:\tTraining Loss - 0.0059\n",
      "Epoch 2481:\tTraining Loss - 0.0066\n",
      "Epoch 2482:\tTraining Loss - 0.0050\n",
      "Epoch 2483:\tTraining Loss - 0.0050\n",
      "Epoch 2484:\tTraining Loss - 0.0063\n",
      "Epoch 2485:\tTraining Loss - 0.0068\n",
      "Epoch 2486:\tTraining Loss - 0.0038\n",
      "Epoch 2487:\tTraining Loss - 0.0066\n",
      "Epoch 2488:\tTraining Loss - 0.0105\n",
      "Epoch 2489:\tTraining Loss - 0.0079\n",
      "Epoch 2490:\tTraining Loss - 0.0054\n",
      "Epoch 2491:\tTraining Loss - 0.0040\n",
      "Epoch 2492:\tTraining Loss - 0.0062\n",
      "Epoch 2493:\tTraining Loss - 0.0060\n",
      "Epoch 2494:\tTraining Loss - 0.0031\n",
      "Epoch 2495:\tTraining Loss - 0.0040\n",
      "Epoch 2496:\tTraining Loss - 0.0084\n",
      "Epoch 2497:\tTraining Loss - 0.0078\n",
      "Epoch 2498:\tTraining Loss - 0.0108\n",
      "Epoch 2499:\tTraining Loss - 0.0065\n",
      "Epoch 2500:\tTraining Loss - 0.0083\n",
      "Epoch 2501:\tTraining Loss - 0.0046\n",
      "Epoch 2502:\tTraining Loss - 0.0044\n",
      "Epoch 2503:\tTraining Loss - 0.0127\n",
      "Epoch 2504:\tTraining Loss - 0.0042\n",
      "Epoch 2505:\tTraining Loss - 0.0096\n",
      "Epoch 2506:\tTraining Loss - 0.0107\n",
      "Epoch 2507:\tTraining Loss - 0.0098\n",
      "Epoch 2508:\tTraining Loss - 0.0080\n",
      "Epoch 2509:\tTraining Loss - 0.0025\n",
      "Epoch 2510:\tTraining Loss - 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2511:\tTraining Loss - 0.0058\n",
      "Epoch 2512:\tTraining Loss - 0.0056\n",
      "Epoch 2513:\tTraining Loss - 0.0037\n",
      "Epoch 2514:\tTraining Loss - 0.0100\n",
      "Epoch 2515:\tTraining Loss - 0.0084\n",
      "Epoch 2516:\tTraining Loss - 0.0098\n",
      "Epoch 2517:\tTraining Loss - 0.0048\n",
      "Epoch 2518:\tTraining Loss - 0.0043\n",
      "Epoch 2519:\tTraining Loss - 0.0080\n",
      "Epoch 2520:\tTraining Loss - 0.0049\n",
      "Epoch 2521:\tTraining Loss - 0.0063\n",
      "Epoch 2522:\tTraining Loss - 0.0083\n",
      "Epoch 2523:\tTraining Loss - 0.0099\n",
      "Epoch 2524:\tTraining Loss - 0.0070\n",
      "Epoch 2525:\tTraining Loss - 0.0059\n",
      "Epoch 2526:\tTraining Loss - 0.0070\n",
      "Epoch 2527:\tTraining Loss - 0.0046\n",
      "Epoch 2528:\tTraining Loss - 0.0066\n",
      "Epoch 2529:\tTraining Loss - 0.0062\n",
      "Epoch 2530:\tTraining Loss - 0.0043\n",
      "Epoch 2531:\tTraining Loss - 0.0058\n",
      "Epoch 2532:\tTraining Loss - 0.0053\n",
      "Epoch 2533:\tTraining Loss - 0.0058\n",
      "Epoch 2534:\tTraining Loss - 0.0061\n",
      "Epoch 2535:\tTraining Loss - 0.0065\n",
      "Epoch 2536:\tTraining Loss - 0.0072\n",
      "Epoch 2537:\tTraining Loss - 0.0063\n",
      "Epoch 2538:\tTraining Loss - 0.0029\n",
      "Epoch 2539:\tTraining Loss - 0.0063\n",
      "Epoch 2540:\tTraining Loss - 0.0062\n",
      "Epoch 2541:\tTraining Loss - 0.0052\n",
      "Epoch 2542:\tTraining Loss - 0.0058\n",
      "Epoch 2543:\tTraining Loss - 0.0068\n",
      "Epoch 2544:\tTraining Loss - 0.0059\n",
      "Epoch 2545:\tTraining Loss - 0.0100\n",
      "Epoch 2546:\tTraining Loss - 0.0063\n",
      "Epoch 2547:\tTraining Loss - 0.0086\n",
      "Epoch 2548:\tTraining Loss - 0.0077\n",
      "Epoch 2549:\tTraining Loss - 0.0045\n",
      "Epoch 2550:\tTraining Loss - 0.0039\n",
      "Epoch 2551:\tTraining Loss - 0.0067\n",
      "Epoch 2552:\tTraining Loss - 0.0056\n",
      "Epoch 2553:\tTraining Loss - 0.0066\n",
      "Epoch 2554:\tTraining Loss - 0.0093\n",
      "Epoch 2555:\tTraining Loss - 0.0061\n",
      "Epoch 2556:\tTraining Loss - 0.0029\n",
      "Epoch 2557:\tTraining Loss - 0.0075\n",
      "Epoch 2558:\tTraining Loss - 0.0080\n",
      "Epoch 2559:\tTraining Loss - 0.0039\n",
      "Epoch 2560:\tTraining Loss - 0.0038\n",
      "Epoch 2561:\tTraining Loss - 0.0071\n",
      "Epoch 2562:\tTraining Loss - 0.0072\n",
      "Epoch 2563:\tTraining Loss - 0.0090\n",
      "Epoch 2564:\tTraining Loss - 0.0053\n",
      "Epoch 2565:\tTraining Loss - 0.0035\n",
      "Epoch 2566:\tTraining Loss - 0.0085\n",
      "Epoch 2567:\tTraining Loss - 0.0083\n",
      "Epoch 2568:\tTraining Loss - 0.0045\n",
      "Epoch 2569:\tTraining Loss - 0.0066\n",
      "Epoch 2570:\tTraining Loss - 0.0030\n",
      "Epoch 2571:\tTraining Loss - 0.0050\n",
      "Epoch 2572:\tTraining Loss - 0.0096\n",
      "Epoch 2573:\tTraining Loss - 0.0088\n",
      "Epoch 2574:\tTraining Loss - 0.0088\n",
      "Epoch 2575:\tTraining Loss - 0.0096\n",
      "Epoch 2576:\tTraining Loss - 0.0078\n",
      "Epoch 2577:\tTraining Loss - 0.0079\n",
      "Epoch 2578:\tTraining Loss - 0.0060\n",
      "Epoch 2579:\tTraining Loss - 0.0063\n",
      "Epoch 2580:\tTraining Loss - 0.0079\n",
      "Epoch 2581:\tTraining Loss - 0.0078\n",
      "Epoch 2582:\tTraining Loss - 0.0070\n",
      "Epoch 2583:\tTraining Loss - 0.0042\n",
      "Epoch 2584:\tTraining Loss - 0.0050\n",
      "Epoch 2585:\tTraining Loss - 0.0051\n",
      "Epoch 2586:\tTraining Loss - 0.0037\n",
      "Epoch 2587:\tTraining Loss - 0.0060\n",
      "Epoch 2588:\tTraining Loss - 0.0058\n",
      "Epoch 2589:\tTraining Loss - 0.0098\n",
      "Epoch 2590:\tTraining Loss - 0.0075\n",
      "Epoch 2591:\tTraining Loss - 0.0039\n",
      "Epoch 2592:\tTraining Loss - 0.0080\n",
      "Epoch 2593:\tTraining Loss - 0.0084\n",
      "Epoch 2594:\tTraining Loss - 0.0070\n",
      "Epoch 2595:\tTraining Loss - 0.0060\n",
      "Epoch 2596:\tTraining Loss - 0.0093\n",
      "Epoch 2597:\tTraining Loss - 0.0052\n",
      "Epoch 2598:\tTraining Loss - 0.0106\n",
      "Epoch 2599:\tTraining Loss - 0.0104\n",
      "Epoch 2600:\tTraining Loss - 0.0086\n",
      "Epoch 2601:\tTraining Loss - 0.0076\n",
      "Epoch 2602:\tTraining Loss - 0.0082\n",
      "Epoch 2603:\tTraining Loss - 0.0081\n",
      "Epoch 2604:\tTraining Loss - 0.0053\n",
      "Epoch 2605:\tTraining Loss - 0.0041\n",
      "Epoch 2606:\tTraining Loss - 0.0072\n",
      "Epoch 2607:\tTraining Loss - 0.0047\n",
      "Epoch 2608:\tTraining Loss - 0.0056\n",
      "Epoch 2609:\tTraining Loss - 0.0051\n",
      "Epoch 2610:\tTraining Loss - 0.0037\n",
      "Epoch 2611:\tTraining Loss - 0.0071\n",
      "Epoch 2612:\tTraining Loss - 0.0083\n",
      "Epoch 2613:\tTraining Loss - 0.0091\n",
      "Epoch 2614:\tTraining Loss - 0.0049\n",
      "Epoch 2615:\tTraining Loss - 0.0057\n",
      "Epoch 2616:\tTraining Loss - 0.0086\n",
      "Epoch 2617:\tTraining Loss - 0.0056\n",
      "Epoch 2618:\tTraining Loss - 0.0086\n",
      "Epoch 2619:\tTraining Loss - 0.0046\n",
      "Epoch 2620:\tTraining Loss - 0.0073\n",
      "Epoch 2621:\tTraining Loss - 0.0108\n",
      "Epoch 2622:\tTraining Loss - 0.0052\n",
      "Epoch 2623:\tTraining Loss - 0.0066\n",
      "Epoch 2624:\tTraining Loss - 0.0058\n",
      "Epoch 2625:\tTraining Loss - 0.0039\n",
      "Epoch 2626:\tTraining Loss - 0.0125\n",
      "Epoch 2627:\tTraining Loss - 0.0141\n",
      "Epoch 2628:\tTraining Loss - 0.0048\n",
      "Epoch 2629:\tTraining Loss - 0.0085\n",
      "Epoch 2630:\tTraining Loss - 0.0054\n",
      "Epoch 2631:\tTraining Loss - 0.0035\n",
      "Epoch 2632:\tTraining Loss - 0.0102\n",
      "Epoch 2633:\tTraining Loss - 0.0061\n",
      "Epoch 2634:\tTraining Loss - 0.0039\n",
      "Epoch 2635:\tTraining Loss - 0.0081\n",
      "Epoch 2636:\tTraining Loss - 0.0032\n",
      "Epoch 2637:\tTraining Loss - 0.0048\n",
      "Epoch 2638:\tTraining Loss - 0.0026\n",
      "Epoch 2639:\tTraining Loss - 0.0039\n",
      "Epoch 2640:\tTraining Loss - 0.0049\n",
      "Epoch 2641:\tTraining Loss - 0.0044\n",
      "Epoch 2642:\tTraining Loss - 0.0042\n",
      "Epoch 2643:\tTraining Loss - 0.0077\n",
      "Epoch 2644:\tTraining Loss - 0.0085\n",
      "Epoch 2645:\tTraining Loss - 0.0074\n",
      "Epoch 2646:\tTraining Loss - 0.0095\n",
      "Epoch 2647:\tTraining Loss - 0.0052\n",
      "Epoch 2648:\tTraining Loss - 0.0059\n",
      "Epoch 2649:\tTraining Loss - 0.0056\n",
      "Epoch 2650:\tTraining Loss - 0.0086\n",
      "Epoch 2651:\tTraining Loss - 0.0048\n",
      "Epoch 2652:\tTraining Loss - 0.0070\n",
      "Epoch 2653:\tTraining Loss - 0.0070\n",
      "Epoch 2654:\tTraining Loss - 0.0114\n",
      "Epoch 2655:\tTraining Loss - 0.0074\n",
      "Epoch 2656:\tTraining Loss - 0.0079\n",
      "Epoch 2657:\tTraining Loss - 0.0062\n",
      "Epoch 2658:\tTraining Loss - 0.0039\n",
      "Epoch 2659:\tTraining Loss - 0.0051\n",
      "Epoch 2660:\tTraining Loss - 0.0076\n",
      "Epoch 2661:\tTraining Loss - 0.0059\n",
      "Epoch 2662:\tTraining Loss - 0.0053\n",
      "Epoch 2663:\tTraining Loss - 0.0035\n",
      "Epoch 2664:\tTraining Loss - 0.0074\n",
      "Epoch 2665:\tTraining Loss - 0.0115\n",
      "Epoch 2666:\tTraining Loss - 0.0058\n",
      "Epoch 2667:\tTraining Loss - 0.0077\n",
      "Epoch 2668:\tTraining Loss - 0.0052\n",
      "Epoch 2669:\tTraining Loss - 0.0045\n",
      "Epoch 2670:\tTraining Loss - 0.0037\n",
      "Epoch 2671:\tTraining Loss - 0.0044\n",
      "Epoch 2672:\tTraining Loss - 0.0077\n",
      "Epoch 2673:\tTraining Loss - 0.0088\n",
      "Epoch 2674:\tTraining Loss - 0.0083\n",
      "Epoch 2675:\tTraining Loss - 0.0136\n",
      "Epoch 2676:\tTraining Loss - 0.0072\n",
      "Epoch 2677:\tTraining Loss - 0.0072\n",
      "Epoch 2678:\tTraining Loss - 0.0069\n",
      "Epoch 2679:\tTraining Loss - 0.0045\n",
      "Epoch 2680:\tTraining Loss - 0.0057\n",
      "Epoch 2681:\tTraining Loss - 0.0051\n",
      "Epoch 2682:\tTraining Loss - 0.0067\n",
      "Epoch 2683:\tTraining Loss - 0.0067\n",
      "Epoch 2684:\tTraining Loss - 0.0041\n",
      "Epoch 2685:\tTraining Loss - 0.0052\n",
      "Epoch 2686:\tTraining Loss - 0.0067\n",
      "Epoch 2687:\tTraining Loss - 0.0052\n",
      "Epoch 2688:\tTraining Loss - 0.0092\n",
      "Epoch 2689:\tTraining Loss - 0.0081\n",
      "Epoch 2690:\tTraining Loss - 0.0079\n",
      "Epoch 2691:\tTraining Loss - 0.0047\n",
      "Epoch 2692:\tTraining Loss - 0.0042\n",
      "Epoch 2693:\tTraining Loss - 0.0056\n",
      "Epoch 2694:\tTraining Loss - 0.0050\n",
      "Epoch 2695:\tTraining Loss - 0.0077\n",
      "Epoch 2696:\tTraining Loss - 0.0082\n",
      "Epoch 2697:\tTraining Loss - 0.0107\n",
      "Epoch 2698:\tTraining Loss - 0.0055\n",
      "Epoch 2699:\tTraining Loss - 0.0081\n",
      "Epoch 2700:\tTraining Loss - 0.0103\n",
      "Epoch 2701:\tTraining Loss - 0.0075\n",
      "Epoch 2702:\tTraining Loss - 0.0038\n",
      "Epoch 2703:\tTraining Loss - 0.0048\n",
      "Epoch 2704:\tTraining Loss - 0.0062\n",
      "Epoch 2705:\tTraining Loss - 0.0056\n",
      "Epoch 2706:\tTraining Loss - 0.0062\n",
      "Epoch 2707:\tTraining Loss - 0.0050\n",
      "Epoch 2708:\tTraining Loss - 0.0067\n",
      "Epoch 2709:\tTraining Loss - 0.0066\n",
      "Epoch 2710:\tTraining Loss - 0.0041\n",
      "Epoch 2711:\tTraining Loss - 0.0082\n",
      "Epoch 2712:\tTraining Loss - 0.0085\n",
      "Epoch 2713:\tTraining Loss - 0.0065\n",
      "Epoch 2714:\tTraining Loss - 0.0060\n",
      "Epoch 2715:\tTraining Loss - 0.0040\n",
      "Epoch 2716:\tTraining Loss - 0.0081\n",
      "Epoch 2717:\tTraining Loss - 0.0070\n",
      "Epoch 2718:\tTraining Loss - 0.0075\n",
      "Epoch 2719:\tTraining Loss - 0.0052\n",
      "Epoch 2720:\tTraining Loss - 0.0065\n",
      "Epoch 2721:\tTraining Loss - 0.0070\n",
      "Epoch 2722:\tTraining Loss - 0.0103\n",
      "Epoch 2723:\tTraining Loss - 0.0056\n",
      "Epoch 2724:\tTraining Loss - 0.0060\n",
      "Epoch 2725:\tTraining Loss - 0.0081\n",
      "Epoch 2726:\tTraining Loss - 0.0088\n",
      "Epoch 2727:\tTraining Loss - 0.0088\n",
      "Epoch 2728:\tTraining Loss - 0.0056\n",
      "Epoch 2729:\tTraining Loss - 0.0069\n",
      "Epoch 2730:\tTraining Loss - 0.0051\n",
      "Epoch 2731:\tTraining Loss - 0.0075\n",
      "Epoch 2732:\tTraining Loss - 0.0046\n",
      "Epoch 2733:\tTraining Loss - 0.0052\n",
      "Epoch 2734:\tTraining Loss - 0.0105\n",
      "Epoch 2735:\tTraining Loss - 0.0076\n",
      "Epoch 2736:\tTraining Loss - 0.0073\n",
      "Epoch 2737:\tTraining Loss - 0.0063\n",
      "Epoch 2738:\tTraining Loss - 0.0037\n",
      "Epoch 2739:\tTraining Loss - 0.0077\n",
      "Epoch 2740:\tTraining Loss - 0.0042\n",
      "Epoch 2741:\tTraining Loss - 0.0037\n",
      "Epoch 2742:\tTraining Loss - 0.0066\n",
      "Epoch 2743:\tTraining Loss - 0.0046\n",
      "Epoch 2744:\tTraining Loss - 0.0042\n",
      "Epoch 2745:\tTraining Loss - 0.0036\n",
      "Epoch 2746:\tTraining Loss - 0.0035\n",
      "Epoch 2747:\tTraining Loss - 0.0099\n",
      "Epoch 2748:\tTraining Loss - 0.0051\n",
      "Epoch 2749:\tTraining Loss - 0.0056\n",
      "Epoch 2750:\tTraining Loss - 0.0041\n",
      "Epoch 2751:\tTraining Loss - 0.0033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2752:\tTraining Loss - 0.0072\n",
      "Epoch 2753:\tTraining Loss - 0.0040\n",
      "Epoch 2754:\tTraining Loss - 0.0045\n",
      "Epoch 2755:\tTraining Loss - 0.0077\n",
      "Epoch 2756:\tTraining Loss - 0.0068\n",
      "Epoch 2757:\tTraining Loss - 0.0037\n",
      "Epoch 2758:\tTraining Loss - 0.0025\n",
      "Epoch 2759:\tTraining Loss - 0.0106\n",
      "Epoch 2760:\tTraining Loss - 0.0033\n",
      "Epoch 2761:\tTraining Loss - 0.0072\n",
      "Epoch 2762:\tTraining Loss - 0.0027\n",
      "Epoch 2763:\tTraining Loss - 0.0080\n",
      "Epoch 2764:\tTraining Loss - 0.0065\n",
      "Epoch 2765:\tTraining Loss - 0.0050\n",
      "Epoch 2766:\tTraining Loss - 0.0111\n",
      "Epoch 2767:\tTraining Loss - 0.0034\n",
      "Epoch 2768:\tTraining Loss - 0.0091\n",
      "Epoch 2769:\tTraining Loss - 0.0076\n",
      "Epoch 2770:\tTraining Loss - 0.0059\n",
      "Epoch 2771:\tTraining Loss - 0.0041\n",
      "Epoch 2772:\tTraining Loss - 0.0040\n",
      "Epoch 2773:\tTraining Loss - 0.0072\n",
      "Epoch 2774:\tTraining Loss - 0.0067\n",
      "Epoch 2775:\tTraining Loss - 0.0055\n",
      "Epoch 2776:\tTraining Loss - 0.0062\n",
      "Epoch 2777:\tTraining Loss - 0.0062\n",
      "Epoch 2778:\tTraining Loss - 0.0078\n",
      "Epoch 2779:\tTraining Loss - 0.0049\n",
      "Epoch 2780:\tTraining Loss - 0.0074\n",
      "Epoch 2781:\tTraining Loss - 0.0106\n",
      "Epoch 2782:\tTraining Loss - 0.0050\n",
      "Epoch 2783:\tTraining Loss - 0.0063\n",
      "Epoch 2784:\tTraining Loss - 0.0050\n",
      "Epoch 2785:\tTraining Loss - 0.0080\n",
      "Epoch 2786:\tTraining Loss - 0.0059\n",
      "Epoch 2787:\tTraining Loss - 0.0058\n",
      "Epoch 2788:\tTraining Loss - 0.0043\n",
      "Epoch 2789:\tTraining Loss - 0.0059\n",
      "Epoch 2790:\tTraining Loss - 0.0047\n",
      "Epoch 2791:\tTraining Loss - 0.0067\n",
      "Epoch 2792:\tTraining Loss - 0.0063\n",
      "Epoch 2793:\tTraining Loss - 0.0047\n",
      "Epoch 2794:\tTraining Loss - 0.0082\n",
      "Epoch 2795:\tTraining Loss - 0.0058\n",
      "Epoch 2796:\tTraining Loss - 0.0041\n",
      "Epoch 2797:\tTraining Loss - 0.0088\n",
      "Epoch 2798:\tTraining Loss - 0.0034\n",
      "Epoch 2799:\tTraining Loss - 0.0041\n",
      "Epoch 2800:\tTraining Loss - 0.0040\n",
      "Epoch 2801:\tTraining Loss - 0.0071\n",
      "Epoch 2802:\tTraining Loss - 0.0062\n",
      "Epoch 2803:\tTraining Loss - 0.0046\n",
      "Epoch 2804:\tTraining Loss - 0.0051\n",
      "Epoch 2805:\tTraining Loss - 0.0105\n",
      "Epoch 2806:\tTraining Loss - 0.0044\n",
      "Epoch 2807:\tTraining Loss - 0.0053\n",
      "Epoch 2808:\tTraining Loss - 0.0051\n",
      "Epoch 2809:\tTraining Loss - 0.0059\n",
      "Epoch 2810:\tTraining Loss - 0.0100\n",
      "Epoch 2811:\tTraining Loss - 0.0095\n",
      "Epoch 2812:\tTraining Loss - 0.0052\n",
      "Epoch 2813:\tTraining Loss - 0.0059\n",
      "Epoch 2814:\tTraining Loss - 0.0046\n",
      "Epoch 2815:\tTraining Loss - 0.0063\n",
      "Epoch 2816:\tTraining Loss - 0.0064\n",
      "Epoch 2817:\tTraining Loss - 0.0046\n",
      "Epoch 2818:\tTraining Loss - 0.0053\n",
      "Epoch 2819:\tTraining Loss - 0.0082\n",
      "Epoch 2820:\tTraining Loss - 0.0077\n",
      "Epoch 2821:\tTraining Loss - 0.0033\n",
      "Epoch 2822:\tTraining Loss - 0.0042\n",
      "Epoch 2823:\tTraining Loss - 0.0064\n",
      "Epoch 2824:\tTraining Loss - 0.0078\n",
      "Epoch 2825:\tTraining Loss - 0.0053\n",
      "Epoch 2826:\tTraining Loss - 0.0081\n",
      "Epoch 2827:\tTraining Loss - 0.0074\n",
      "Epoch 2828:\tTraining Loss - 0.0042\n",
      "Epoch 2829:\tTraining Loss - 0.0035\n",
      "Epoch 2830:\tTraining Loss - 0.0050\n",
      "Epoch 2831:\tTraining Loss - 0.0064\n",
      "Epoch 2832:\tTraining Loss - 0.0099\n",
      "Epoch 2833:\tTraining Loss - 0.0056\n",
      "Epoch 2834:\tTraining Loss - 0.0077\n",
      "Epoch 2835:\tTraining Loss - 0.0051\n",
      "Epoch 2836:\tTraining Loss - 0.0073\n",
      "Epoch 2837:\tTraining Loss - 0.0083\n",
      "Epoch 2838:\tTraining Loss - 0.0036\n",
      "Epoch 2839:\tTraining Loss - 0.0058\n",
      "Epoch 2840:\tTraining Loss - 0.0056\n",
      "Epoch 2841:\tTraining Loss - 0.0040\n",
      "Epoch 2842:\tTraining Loss - 0.0082\n",
      "Epoch 2843:\tTraining Loss - 0.0068\n",
      "Epoch 2844:\tTraining Loss - 0.0069\n",
      "Epoch 2845:\tTraining Loss - 0.0056\n",
      "Epoch 2846:\tTraining Loss - 0.0086\n",
      "Epoch 2847:\tTraining Loss - 0.0113\n",
      "Epoch 2848:\tTraining Loss - 0.0047\n",
      "Epoch 2849:\tTraining Loss - 0.0080\n",
      "Epoch 2850:\tTraining Loss - 0.0067\n",
      "Epoch 2851:\tTraining Loss - 0.0072\n",
      "Epoch 2852:\tTraining Loss - 0.0078\n",
      "Epoch 2853:\tTraining Loss - 0.0038\n",
      "Epoch 2854:\tTraining Loss - 0.0115\n",
      "Epoch 2855:\tTraining Loss - 0.0058\n",
      "Epoch 2856:\tTraining Loss - 0.0090\n",
      "Epoch 2857:\tTraining Loss - 0.0068\n",
      "Epoch 2858:\tTraining Loss - 0.0063\n",
      "Epoch 2859:\tTraining Loss - 0.0115\n",
      "Epoch 2860:\tTraining Loss - 0.0081\n",
      "Epoch 2861:\tTraining Loss - 0.0042\n",
      "Epoch 2862:\tTraining Loss - 0.0065\n",
      "Epoch 2863:\tTraining Loss - 0.0068\n",
      "Epoch 2864:\tTraining Loss - 0.0045\n",
      "Epoch 2865:\tTraining Loss - 0.0064\n",
      "Epoch 2866:\tTraining Loss - 0.0048\n",
      "Epoch 2867:\tTraining Loss - 0.0052\n",
      "Epoch 2868:\tTraining Loss - 0.0079\n",
      "Epoch 2869:\tTraining Loss - 0.0045\n",
      "Epoch 2870:\tTraining Loss - 0.0064\n",
      "Epoch 2871:\tTraining Loss - 0.0081\n",
      "Epoch 2872:\tTraining Loss - 0.0053\n",
      "Epoch 2873:\tTraining Loss - 0.0045\n",
      "Epoch 2874:\tTraining Loss - 0.0050\n",
      "Epoch 2875:\tTraining Loss - 0.0051\n",
      "Epoch 2876:\tTraining Loss - 0.0045\n",
      "Epoch 2877:\tTraining Loss - 0.0095\n",
      "Epoch 2878:\tTraining Loss - 0.0061\n",
      "Epoch 2879:\tTraining Loss - 0.0027\n",
      "Epoch 2880:\tTraining Loss - 0.0110\n",
      "Epoch 2881:\tTraining Loss - 0.0056\n",
      "Epoch 2882:\tTraining Loss - 0.0034\n",
      "Epoch 2883:\tTraining Loss - 0.0063\n",
      "Epoch 2884:\tTraining Loss - 0.0033\n",
      "Epoch 2885:\tTraining Loss - 0.0060\n",
      "Epoch 2886:\tTraining Loss - 0.0056\n",
      "Epoch 2887:\tTraining Loss - 0.0052\n",
      "Epoch 2888:\tTraining Loss - 0.0049\n",
      "Epoch 2889:\tTraining Loss - 0.0041\n",
      "Epoch 2890:\tTraining Loss - 0.0056\n",
      "Epoch 2891:\tTraining Loss - 0.0055\n",
      "Epoch 2892:\tTraining Loss - 0.0071\n",
      "Epoch 2893:\tTraining Loss - 0.0069\n",
      "Epoch 2894:\tTraining Loss - 0.0046\n",
      "Epoch 2895:\tTraining Loss - 0.0071\n",
      "Epoch 2896:\tTraining Loss - 0.0114\n",
      "Epoch 2897:\tTraining Loss - 0.0073\n",
      "Epoch 2898:\tTraining Loss - 0.0044\n",
      "Epoch 2899:\tTraining Loss - 0.0055\n",
      "Epoch 2900:\tTraining Loss - 0.0035\n",
      "Epoch 2901:\tTraining Loss - 0.0054\n",
      "Epoch 2902:\tTraining Loss - 0.0044\n",
      "Epoch 2903:\tTraining Loss - 0.0068\n",
      "Epoch 2904:\tTraining Loss - 0.0095\n",
      "Epoch 2905:\tTraining Loss - 0.0062\n",
      "Epoch 2906:\tTraining Loss - 0.0025\n",
      "Epoch 2907:\tTraining Loss - 0.0114\n",
      "Epoch 2908:\tTraining Loss - 0.0049\n",
      "Epoch 2909:\tTraining Loss - 0.0046\n",
      "Epoch 2910:\tTraining Loss - 0.0042\n",
      "Epoch 2911:\tTraining Loss - 0.0085\n",
      "Epoch 2912:\tTraining Loss - 0.0068\n",
      "Epoch 2913:\tTraining Loss - 0.0058\n",
      "Epoch 2914:\tTraining Loss - 0.0063\n",
      "Epoch 2915:\tTraining Loss - 0.0038\n",
      "Epoch 2916:\tTraining Loss - 0.0049\n",
      "Epoch 2917:\tTraining Loss - 0.0120\n",
      "Epoch 2918:\tTraining Loss - 0.0058\n",
      "Epoch 2919:\tTraining Loss - 0.0047\n",
      "Epoch 2920:\tTraining Loss - 0.0058\n",
      "Epoch 2921:\tTraining Loss - 0.0047\n",
      "Epoch 2922:\tTraining Loss - 0.0108\n",
      "Epoch 2923:\tTraining Loss - 0.0124\n",
      "Epoch 2924:\tTraining Loss - 0.0039\n",
      "Epoch 2925:\tTraining Loss - 0.0071\n",
      "Epoch 2926:\tTraining Loss - 0.0096\n",
      "Epoch 2927:\tTraining Loss - 0.0044\n",
      "Epoch 2928:\tTraining Loss - 0.0063\n",
      "Epoch 2929:\tTraining Loss - 0.0067\n",
      "Epoch 2930:\tTraining Loss - 0.0055\n",
      "Epoch 2931:\tTraining Loss - 0.0061\n",
      "Epoch 2932:\tTraining Loss - 0.0053\n",
      "Epoch 2933:\tTraining Loss - 0.0065\n",
      "Epoch 2934:\tTraining Loss - 0.0072\n",
      "Epoch 2935:\tTraining Loss - 0.0127\n",
      "Epoch 2936:\tTraining Loss - 0.0039\n",
      "Epoch 2937:\tTraining Loss - 0.0071\n",
      "Epoch 2938:\tTraining Loss - 0.0108\n",
      "Epoch 2939:\tTraining Loss - 0.0047\n",
      "Epoch 2940:\tTraining Loss - 0.0080\n",
      "Epoch 2941:\tTraining Loss - 0.0073\n",
      "Epoch 2942:\tTraining Loss - 0.0051\n",
      "Epoch 2943:\tTraining Loss - 0.0029\n",
      "Epoch 2944:\tTraining Loss - 0.0048\n",
      "Epoch 2945:\tTraining Loss - 0.0072\n",
      "Epoch 2946:\tTraining Loss - 0.0051\n",
      "Epoch 2947:\tTraining Loss - 0.0073\n",
      "Epoch 2948:\tTraining Loss - 0.0066\n",
      "Epoch 2949:\tTraining Loss - 0.0045\n",
      "Epoch 2950:\tTraining Loss - 0.0034\n",
      "Epoch 2951:\tTraining Loss - 0.0073\n",
      "Epoch 2952:\tTraining Loss - 0.0050\n",
      "Epoch 2953:\tTraining Loss - 0.0055\n",
      "Epoch 2954:\tTraining Loss - 0.0041\n",
      "Epoch 2955:\tTraining Loss - 0.0068\n",
      "Epoch 2956:\tTraining Loss - 0.0090\n",
      "Epoch 2957:\tTraining Loss - 0.0105\n",
      "Epoch 2958:\tTraining Loss - 0.0058\n",
      "Epoch 2959:\tTraining Loss - 0.0073\n",
      "Epoch 2960:\tTraining Loss - 0.0059\n",
      "Epoch 2961:\tTraining Loss - 0.0114\n",
      "Epoch 2962:\tTraining Loss - 0.0092\n",
      "Epoch 2963:\tTraining Loss - 0.0052\n",
      "Epoch 2964:\tTraining Loss - 0.0053\n",
      "Epoch 2965:\tTraining Loss - 0.0047\n",
      "Epoch 2966:\tTraining Loss - 0.0038\n",
      "Epoch 2967:\tTraining Loss - 0.0095\n",
      "Epoch 2968:\tTraining Loss - 0.0048\n",
      "Epoch 2969:\tTraining Loss - 0.0077\n",
      "Epoch 2970:\tTraining Loss - 0.0033\n",
      "Epoch 2971:\tTraining Loss - 0.0068\n",
      "Epoch 2972:\tTraining Loss - 0.0045\n",
      "Epoch 2973:\tTraining Loss - 0.0074\n",
      "Epoch 2974:\tTraining Loss - 0.0091\n",
      "Epoch 2975:\tTraining Loss - 0.0040\n",
      "Epoch 2976:\tTraining Loss - 0.0053\n",
      "Epoch 2977:\tTraining Loss - 0.0051\n",
      "Epoch 2978:\tTraining Loss - 0.0042\n",
      "Epoch 2979:\tTraining Loss - 0.0058\n",
      "Epoch 2980:\tTraining Loss - 0.0076\n",
      "Epoch 2981:\tTraining Loss - 0.0050\n",
      "Epoch 2982:\tTraining Loss - 0.0060\n",
      "Epoch 2983:\tTraining Loss - 0.0053\n",
      "Epoch 2984:\tTraining Loss - 0.0080\n",
      "Epoch 2985:\tTraining Loss - 0.0040\n",
      "Epoch 2986:\tTraining Loss - 0.0122\n",
      "Epoch 2987:\tTraining Loss - 0.0039\n",
      "Epoch 2988:\tTraining Loss - 0.0060\n",
      "Epoch 2989:\tTraining Loss - 0.0058\n",
      "Epoch 2990:\tTraining Loss - 0.0054\n",
      "Epoch 2991:\tTraining Loss - 0.0072\n",
      "Epoch 2992:\tTraining Loss - 0.0032\n",
      "Epoch 2993:\tTraining Loss - 0.0049\n",
      "Epoch 2994:\tTraining Loss - 0.0034\n",
      "Epoch 2995:\tTraining Loss - 0.0083\n",
      "Epoch 2996:\tTraining Loss - 0.0053\n",
      "Epoch 2997:\tTraining Loss - 0.0050\n",
      "Epoch 2998:\tTraining Loss - 0.0112\n",
      "Epoch 2999:\tTraining Loss - 0.0061\n",
      "Epoch 3000:\tTraining Loss - 0.0043\n",
      "Epoch 3001:\tTraining Loss - 0.0036\n",
      "Epoch 3002:\tTraining Loss - 0.0099\n",
      "Epoch 3003:\tTraining Loss - 0.0061\n",
      "Epoch 3004:\tTraining Loss - 0.0044\n",
      "Epoch 3005:\tTraining Loss - 0.0149\n",
      "Epoch 3006:\tTraining Loss - 0.0092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3007:\tTraining Loss - 0.0053\n",
      "Epoch 3008:\tTraining Loss - 0.0030\n",
      "Epoch 3009:\tTraining Loss - 0.0079\n",
      "Epoch 3010:\tTraining Loss - 0.0046\n",
      "Epoch 3011:\tTraining Loss - 0.0061\n",
      "Epoch 3012:\tTraining Loss - 0.0099\n",
      "Epoch 3013:\tTraining Loss - 0.0091\n",
      "Epoch 3014:\tTraining Loss - 0.0095\n",
      "Epoch 3015:\tTraining Loss - 0.0075\n",
      "Epoch 3016:\tTraining Loss - 0.0045\n",
      "Epoch 3017:\tTraining Loss - 0.0106\n",
      "Epoch 3018:\tTraining Loss - 0.0062\n",
      "Epoch 3019:\tTraining Loss - 0.0061\n",
      "Epoch 3020:\tTraining Loss - 0.0063\n",
      "Epoch 3021:\tTraining Loss - 0.0088\n",
      "Epoch 3022:\tTraining Loss - 0.0077\n",
      "Epoch 3023:\tTraining Loss - 0.0075\n",
      "Epoch 3024:\tTraining Loss - 0.0084\n",
      "Epoch 3025:\tTraining Loss - 0.0062\n",
      "Epoch 3026:\tTraining Loss - 0.0083\n",
      "Epoch 3027:\tTraining Loss - 0.0090\n",
      "Epoch 3028:\tTraining Loss - 0.0064\n",
      "Epoch 3029:\tTraining Loss - 0.0090\n",
      "Epoch 3030:\tTraining Loss - 0.0050\n",
      "Epoch 3031:\tTraining Loss - 0.0069\n",
      "Epoch 3032:\tTraining Loss - 0.0044\n",
      "Epoch 3033:\tTraining Loss - 0.0104\n",
      "Epoch 3034:\tTraining Loss - 0.0056\n",
      "Epoch 3035:\tTraining Loss - 0.0048\n",
      "Epoch 3036:\tTraining Loss - 0.0064\n",
      "Epoch 3037:\tTraining Loss - 0.0034\n",
      "Epoch 3038:\tTraining Loss - 0.0053\n",
      "Epoch 3039:\tTraining Loss - 0.0047\n",
      "Epoch 3040:\tTraining Loss - 0.0087\n",
      "Epoch 3041:\tTraining Loss - 0.0054\n",
      "Epoch 3042:\tTraining Loss - 0.0085\n",
      "Epoch 3043:\tTraining Loss - 0.0053\n",
      "Epoch 3044:\tTraining Loss - 0.0071\n",
      "Epoch 3045:\tTraining Loss - 0.0065\n",
      "Epoch 3046:\tTraining Loss - 0.0046\n",
      "Epoch 3047:\tTraining Loss - 0.0061\n",
      "Epoch 3048:\tTraining Loss - 0.0064\n",
      "Epoch 3049:\tTraining Loss - 0.0080\n",
      "Epoch 3050:\tTraining Loss - 0.0034\n",
      "Epoch 3051:\tTraining Loss - 0.0056\n",
      "Epoch 3052:\tTraining Loss - 0.0055\n",
      "Epoch 3053:\tTraining Loss - 0.0044\n",
      "Epoch 3054:\tTraining Loss - 0.0064\n",
      "Epoch 3055:\tTraining Loss - 0.0032\n",
      "Epoch 3056:\tTraining Loss - 0.0096\n",
      "Epoch 3057:\tTraining Loss - 0.0045\n",
      "Epoch 3058:\tTraining Loss - 0.0046\n",
      "Epoch 3059:\tTraining Loss - 0.0051\n",
      "Epoch 3060:\tTraining Loss - 0.0064\n",
      "Epoch 3061:\tTraining Loss - 0.0085\n",
      "Epoch 3062:\tTraining Loss - 0.0068\n",
      "Epoch 3063:\tTraining Loss - 0.0048\n",
      "Epoch 3064:\tTraining Loss - 0.0077\n",
      "Epoch 3065:\tTraining Loss - 0.0049\n",
      "Epoch 3066:\tTraining Loss - 0.0068\n",
      "Epoch 3067:\tTraining Loss - 0.0058\n",
      "Epoch 3068:\tTraining Loss - 0.0071\n",
      "Epoch 3069:\tTraining Loss - 0.0043\n",
      "Epoch 3070:\tTraining Loss - 0.0058\n",
      "Epoch 3071:\tTraining Loss - 0.0057\n",
      "Epoch 3072:\tTraining Loss - 0.0164\n",
      "Epoch 3073:\tTraining Loss - 0.0117\n",
      "Epoch 3074:\tTraining Loss - 0.0056\n",
      "Epoch 3075:\tTraining Loss - 0.0052\n",
      "Epoch 3076:\tTraining Loss - 0.0053\n",
      "Epoch 3077:\tTraining Loss - 0.0077\n",
      "Epoch 3078:\tTraining Loss - 0.0061\n",
      "Epoch 3079:\tTraining Loss - 0.0066\n",
      "Epoch 3080:\tTraining Loss - 0.0044\n",
      "Epoch 3081:\tTraining Loss - 0.0073\n",
      "Epoch 3082:\tTraining Loss - 0.0051\n",
      "Epoch 3083:\tTraining Loss - 0.0108\n",
      "Epoch 3084:\tTraining Loss - 0.0050\n",
      "Epoch 3085:\tTraining Loss - 0.0046\n",
      "Epoch 3086:\tTraining Loss - 0.0024\n",
      "Epoch 3087:\tTraining Loss - 0.0048\n",
      "Epoch 3088:\tTraining Loss - 0.0047\n",
      "Epoch 3089:\tTraining Loss - 0.0060\n",
      "Epoch 3090:\tTraining Loss - 0.0027\n",
      "Epoch 3091:\tTraining Loss - 0.0090\n",
      "Epoch 3092:\tTraining Loss - 0.0042\n",
      "Epoch 3093:\tTraining Loss - 0.0047\n",
      "Epoch 3094:\tTraining Loss - 0.0050\n",
      "Epoch 3095:\tTraining Loss - 0.0097\n",
      "Epoch 3096:\tTraining Loss - 0.0058\n",
      "Epoch 3097:\tTraining Loss - 0.0093\n",
      "Epoch 3098:\tTraining Loss - 0.0112\n",
      "Epoch 3099:\tTraining Loss - 0.0057\n",
      "Epoch 3100:\tTraining Loss - 0.0048\n",
      "Epoch 3101:\tTraining Loss - 0.0106\n",
      "Epoch 3102:\tTraining Loss - 0.0092\n",
      "Epoch 3103:\tTraining Loss - 0.0051\n",
      "Epoch 3104:\tTraining Loss - 0.0078\n",
      "Epoch 3105:\tTraining Loss - 0.0048\n",
      "Epoch 3106:\tTraining Loss - 0.0068\n",
      "Epoch 3107:\tTraining Loss - 0.0075\n",
      "Epoch 3108:\tTraining Loss - 0.0102\n",
      "Epoch 3109:\tTraining Loss - 0.0051\n",
      "Epoch 3110:\tTraining Loss - 0.0063\n",
      "Epoch 3111:\tTraining Loss - 0.0061\n",
      "Epoch 3112:\tTraining Loss - 0.0047\n",
      "Epoch 3113:\tTraining Loss - 0.0079\n",
      "Epoch 3114:\tTraining Loss - 0.0055\n",
      "Epoch 3115:\tTraining Loss - 0.0066\n",
      "Epoch 3116:\tTraining Loss - 0.0064\n",
      "Epoch 3117:\tTraining Loss - 0.0045\n",
      "Epoch 3118:\tTraining Loss - 0.0046\n",
      "Epoch 3119:\tTraining Loss - 0.0043\n",
      "Epoch 3120:\tTraining Loss - 0.0061\n",
      "Epoch 3121:\tTraining Loss - 0.0072\n",
      "Epoch 3122:\tTraining Loss - 0.0053\n",
      "Epoch 3123:\tTraining Loss - 0.0040\n",
      "Epoch 3124:\tTraining Loss - 0.0080\n",
      "Epoch 3125:\tTraining Loss - 0.0029\n",
      "Epoch 3126:\tTraining Loss - 0.0097\n",
      "Epoch 3127:\tTraining Loss - 0.0052\n",
      "Epoch 3128:\tTraining Loss - 0.0087\n",
      "Epoch 3129:\tTraining Loss - 0.0059\n",
      "Epoch 3130:\tTraining Loss - 0.0117\n",
      "Epoch 3131:\tTraining Loss - 0.0081\n",
      "Epoch 3132:\tTraining Loss - 0.0054\n",
      "Epoch 3133:\tTraining Loss - 0.0043\n",
      "Epoch 3134:\tTraining Loss - 0.0028\n",
      "Epoch 3135:\tTraining Loss - 0.0058\n",
      "Epoch 3136:\tTraining Loss - 0.0071\n",
      "Epoch 3137:\tTraining Loss - 0.0024\n",
      "Epoch 3138:\tTraining Loss - 0.0079\n",
      "Epoch 3139:\tTraining Loss - 0.0062\n",
      "Epoch 3140:\tTraining Loss - 0.0038\n",
      "Epoch 3141:\tTraining Loss - 0.0084\n",
      "Epoch 3142:\tTraining Loss - 0.0074\n",
      "Epoch 3143:\tTraining Loss - 0.0069\n",
      "Epoch 3144:\tTraining Loss - 0.0040\n",
      "Epoch 3145:\tTraining Loss - 0.0045\n",
      "Epoch 3146:\tTraining Loss - 0.0048\n",
      "Epoch 3147:\tTraining Loss - 0.0047\n",
      "Epoch 3148:\tTraining Loss - 0.0101\n",
      "Epoch 3149:\tTraining Loss - 0.0068\n",
      "Epoch 3150:\tTraining Loss - 0.0088\n",
      "Epoch 3151:\tTraining Loss - 0.0108\n",
      "Epoch 3152:\tTraining Loss - 0.0064\n",
      "Epoch 3153:\tTraining Loss - 0.0083\n",
      "Epoch 3154:\tTraining Loss - 0.0072\n",
      "Epoch 3155:\tTraining Loss - 0.0030\n",
      "Epoch 3156:\tTraining Loss - 0.0068\n",
      "Epoch 3157:\tTraining Loss - 0.0056\n",
      "Epoch 3158:\tTraining Loss - 0.0057\n",
      "Epoch 3159:\tTraining Loss - 0.0058\n",
      "Epoch 3160:\tTraining Loss - 0.0093\n",
      "Epoch 3161:\tTraining Loss - 0.0039\n",
      "Epoch 3162:\tTraining Loss - 0.0080\n",
      "Epoch 3163:\tTraining Loss - 0.0057\n",
      "Epoch 3164:\tTraining Loss - 0.0065\n",
      "Epoch 3165:\tTraining Loss - 0.0071\n",
      "Epoch 3166:\tTraining Loss - 0.0079\n",
      "Epoch 3167:\tTraining Loss - 0.0063\n",
      "Epoch 3168:\tTraining Loss - 0.0058\n",
      "Epoch 3169:\tTraining Loss - 0.0050\n",
      "Epoch 3170:\tTraining Loss - 0.0119\n",
      "Epoch 3171:\tTraining Loss - 0.0100\n",
      "Epoch 3172:\tTraining Loss - 0.0060\n",
      "Epoch 3173:\tTraining Loss - 0.0057\n",
      "Epoch 3174:\tTraining Loss - 0.0061\n",
      "Epoch 3175:\tTraining Loss - 0.0053\n",
      "Epoch 3176:\tTraining Loss - 0.0063\n",
      "Epoch 3177:\tTraining Loss - 0.0073\n",
      "Epoch 3178:\tTraining Loss - 0.0045\n",
      "Epoch 3179:\tTraining Loss - 0.0074\n",
      "Epoch 3180:\tTraining Loss - 0.0057\n",
      "Epoch 3181:\tTraining Loss - 0.0057\n",
      "Epoch 3182:\tTraining Loss - 0.0054\n",
      "Epoch 3183:\tTraining Loss - 0.0062\n",
      "Epoch 3184:\tTraining Loss - 0.0075\n",
      "Epoch 3185:\tTraining Loss - 0.0098\n",
      "Epoch 3186:\tTraining Loss - 0.0046\n",
      "Epoch 3187:\tTraining Loss - 0.0050\n",
      "Epoch 3188:\tTraining Loss - 0.0051\n",
      "Epoch 3189:\tTraining Loss - 0.0063\n",
      "Epoch 3190:\tTraining Loss - 0.0059\n",
      "Epoch 3191:\tTraining Loss - 0.0060\n",
      "Epoch 3192:\tTraining Loss - 0.0059\n",
      "Epoch 3193:\tTraining Loss - 0.0053\n",
      "Epoch 3194:\tTraining Loss - 0.0034\n",
      "Epoch 3195:\tTraining Loss - 0.0061\n",
      "Epoch 3196:\tTraining Loss - 0.0077\n",
      "Epoch 3197:\tTraining Loss - 0.0071\n",
      "Epoch 3198:\tTraining Loss - 0.0072\n",
      "Epoch 3199:\tTraining Loss - 0.0086\n",
      "Epoch 3200:\tTraining Loss - 0.0038\n",
      "Epoch 3201:\tTraining Loss - 0.0069\n",
      "Epoch 3202:\tTraining Loss - 0.0057\n",
      "Epoch 3203:\tTraining Loss - 0.0067\n",
      "Epoch 3204:\tTraining Loss - 0.0068\n",
      "Epoch 3205:\tTraining Loss - 0.0033\n",
      "Epoch 3206:\tTraining Loss - 0.0063\n",
      "Epoch 3207:\tTraining Loss - 0.0095\n",
      "Epoch 3208:\tTraining Loss - 0.0058\n",
      "Epoch 3209:\tTraining Loss - 0.0068\n",
      "Epoch 3210:\tTraining Loss - 0.0056\n",
      "Epoch 3211:\tTraining Loss - 0.0047\n",
      "Epoch 3212:\tTraining Loss - 0.0113\n",
      "Epoch 3213:\tTraining Loss - 0.0075\n",
      "Epoch 3214:\tTraining Loss - 0.0073\n",
      "Epoch 3215:\tTraining Loss - 0.0075\n",
      "Epoch 3216:\tTraining Loss - 0.0053\n",
      "Epoch 3217:\tTraining Loss - 0.0028\n",
      "Epoch 3218:\tTraining Loss - 0.0036\n",
      "Epoch 3219:\tTraining Loss - 0.0066\n",
      "Epoch 3220:\tTraining Loss - 0.0058\n",
      "Epoch 3221:\tTraining Loss - 0.0038\n",
      "Epoch 3222:\tTraining Loss - 0.0063\n",
      "Epoch 3223:\tTraining Loss - 0.0053\n",
      "Epoch 3224:\tTraining Loss - 0.0049\n",
      "Epoch 3225:\tTraining Loss - 0.0081\n",
      "Epoch 3226:\tTraining Loss - 0.0068\n",
      "Epoch 3227:\tTraining Loss - 0.0060\n",
      "Epoch 3228:\tTraining Loss - 0.0094\n",
      "Epoch 3229:\tTraining Loss - 0.0075\n",
      "Epoch 3230:\tTraining Loss - 0.0061\n",
      "Epoch 3231:\tTraining Loss - 0.0052\n",
      "Epoch 3232:\tTraining Loss - 0.0066\n",
      "Epoch 3233:\tTraining Loss - 0.0082\n",
      "Epoch 3234:\tTraining Loss - 0.0060\n",
      "Epoch 3235:\tTraining Loss - 0.0063\n",
      "Epoch 3236:\tTraining Loss - 0.0066\n",
      "Epoch 3237:\tTraining Loss - 0.0044\n",
      "Epoch 3238:\tTraining Loss - 0.0059\n",
      "Epoch 3239:\tTraining Loss - 0.0059\n",
      "Epoch 3240:\tTraining Loss - 0.0056\n",
      "Epoch 3241:\tTraining Loss - 0.0118\n",
      "Epoch 3242:\tTraining Loss - 0.0067\n",
      "Epoch 3243:\tTraining Loss - 0.0041\n",
      "Epoch 3244:\tTraining Loss - 0.0077\n",
      "Epoch 3245:\tTraining Loss - 0.0062\n",
      "Epoch 3246:\tTraining Loss - 0.0089\n",
      "Epoch 3247:\tTraining Loss - 0.0063\n",
      "Epoch 3248:\tTraining Loss - 0.0023\n",
      "Epoch 3249:\tTraining Loss - 0.0064\n",
      "Epoch 3250:\tTraining Loss - 0.0058\n",
      "Epoch 3251:\tTraining Loss - 0.0067\n",
      "Epoch 3252:\tTraining Loss - 0.0069\n",
      "Epoch 3253:\tTraining Loss - 0.0052\n",
      "Epoch 3254:\tTraining Loss - 0.0069\n",
      "Epoch 3255:\tTraining Loss - 0.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3256:\tTraining Loss - 0.0038\n",
      "Epoch 3257:\tTraining Loss - 0.0084\n",
      "Epoch 3258:\tTraining Loss - 0.0053\n",
      "Epoch 3259:\tTraining Loss - 0.0048\n",
      "Epoch 3260:\tTraining Loss - 0.0087\n",
      "Epoch 3261:\tTraining Loss - 0.0078\n",
      "Epoch 3262:\tTraining Loss - 0.0051\n",
      "Epoch 3263:\tTraining Loss - 0.0086\n",
      "Epoch 3264:\tTraining Loss - 0.0073\n",
      "Epoch 3265:\tTraining Loss - 0.0076\n",
      "Epoch 3266:\tTraining Loss - 0.0052\n",
      "Epoch 3267:\tTraining Loss - 0.0062\n",
      "Epoch 3268:\tTraining Loss - 0.0058\n",
      "Epoch 3269:\tTraining Loss - 0.0053\n",
      "Epoch 3270:\tTraining Loss - 0.0027\n",
      "Epoch 3271:\tTraining Loss - 0.0071\n",
      "Epoch 3272:\tTraining Loss - 0.0106\n",
      "Epoch 3273:\tTraining Loss - 0.0130\n",
      "Epoch 3274:\tTraining Loss - 0.0065\n",
      "Epoch 3275:\tTraining Loss - 0.0075\n",
      "Epoch 3276:\tTraining Loss - 0.0064\n",
      "Epoch 3277:\tTraining Loss - 0.0031\n",
      "Epoch 3278:\tTraining Loss - 0.0063\n",
      "Epoch 3279:\tTraining Loss - 0.0039\n",
      "Epoch 3280:\tTraining Loss - 0.0109\n",
      "Epoch 3281:\tTraining Loss - 0.0063\n",
      "Epoch 3282:\tTraining Loss - 0.0069\n",
      "Epoch 3283:\tTraining Loss - 0.0056\n",
      "Epoch 3284:\tTraining Loss - 0.0044\n",
      "Epoch 3285:\tTraining Loss - 0.0104\n",
      "Epoch 3286:\tTraining Loss - 0.0092\n",
      "Epoch 3287:\tTraining Loss - 0.0025\n",
      "Epoch 3288:\tTraining Loss - 0.0085\n",
      "Epoch 3289:\tTraining Loss - 0.0092\n",
      "Epoch 3290:\tTraining Loss - 0.0059\n",
      "Epoch 3291:\tTraining Loss - 0.0051\n",
      "Epoch 3292:\tTraining Loss - 0.0112\n",
      "Epoch 3293:\tTraining Loss - 0.0044\n",
      "Epoch 3294:\tTraining Loss - 0.0054\n",
      "Epoch 3295:\tTraining Loss - 0.0066\n",
      "Epoch 3296:\tTraining Loss - 0.0069\n",
      "Epoch 3297:\tTraining Loss - 0.0092\n",
      "Epoch 3298:\tTraining Loss - 0.0091\n",
      "Epoch 3299:\tTraining Loss - 0.0086\n",
      "Epoch 3300:\tTraining Loss - 0.0035\n",
      "Epoch 3301:\tTraining Loss - 0.0038\n",
      "Epoch 3302:\tTraining Loss - 0.0046\n",
      "Epoch 3303:\tTraining Loss - 0.0045\n",
      "Epoch 3304:\tTraining Loss - 0.0119\n",
      "Epoch 3305:\tTraining Loss - 0.0066\n",
      "Epoch 3306:\tTraining Loss - 0.0079\n",
      "Epoch 3307:\tTraining Loss - 0.0078\n",
      "Epoch 3308:\tTraining Loss - 0.0078\n",
      "Epoch 3309:\tTraining Loss - 0.0069\n",
      "Epoch 3310:\tTraining Loss - 0.0060\n",
      "Epoch 3311:\tTraining Loss - 0.0077\n",
      "Epoch 3312:\tTraining Loss - 0.0091\n",
      "Epoch 3313:\tTraining Loss - 0.0065\n",
      "Epoch 3314:\tTraining Loss - 0.0055\n",
      "Epoch 3315:\tTraining Loss - 0.0060\n",
      "Epoch 3316:\tTraining Loss - 0.0063\n",
      "Epoch 3317:\tTraining Loss - 0.0090\n",
      "Epoch 3318:\tTraining Loss - 0.0058\n",
      "Epoch 3319:\tTraining Loss - 0.0048\n",
      "Epoch 3320:\tTraining Loss - 0.0037\n",
      "Epoch 3321:\tTraining Loss - 0.0031\n",
      "Epoch 3322:\tTraining Loss - 0.0051\n",
      "Epoch 3323:\tTraining Loss - 0.0053\n",
      "Epoch 3324:\tTraining Loss - 0.0035\n",
      "Epoch 3325:\tTraining Loss - 0.0049\n",
      "Epoch 3326:\tTraining Loss - 0.0064\n",
      "Epoch 3327:\tTraining Loss - 0.0033\n",
      "Epoch 3328:\tTraining Loss - 0.0061\n",
      "Epoch 3329:\tTraining Loss - 0.0067\n",
      "Epoch 3330:\tTraining Loss - 0.0060\n",
      "Epoch 3331:\tTraining Loss - 0.0083\n",
      "Epoch 3332:\tTraining Loss - 0.0073\n",
      "Epoch 3333:\tTraining Loss - 0.0030\n",
      "Epoch 3334:\tTraining Loss - 0.0053\n",
      "Epoch 3335:\tTraining Loss - 0.0038\n",
      "Epoch 3336:\tTraining Loss - 0.0031\n",
      "Epoch 3337:\tTraining Loss - 0.0065\n",
      "Epoch 3338:\tTraining Loss - 0.0078\n",
      "Epoch 3339:\tTraining Loss - 0.0028\n",
      "Epoch 3340:\tTraining Loss - 0.0097\n",
      "Epoch 3341:\tTraining Loss - 0.0048\n",
      "Epoch 3342:\tTraining Loss - 0.0054\n",
      "Epoch 3343:\tTraining Loss - 0.0045\n",
      "Epoch 3344:\tTraining Loss - 0.0094\n",
      "Epoch 3345:\tTraining Loss - 0.0070\n",
      "Epoch 3346:\tTraining Loss - 0.0066\n",
      "Epoch 3347:\tTraining Loss - 0.0089\n",
      "Epoch 3348:\tTraining Loss - 0.0038\n",
      "Epoch 3349:\tTraining Loss - 0.0025\n",
      "Epoch 3350:\tTraining Loss - 0.0093\n",
      "Epoch 3351:\tTraining Loss - 0.0028\n",
      "Epoch 3352:\tTraining Loss - 0.0064\n",
      "Epoch 3353:\tTraining Loss - 0.0092\n",
      "Epoch 3354:\tTraining Loss - 0.0053\n",
      "Epoch 3355:\tTraining Loss - 0.0046\n",
      "Epoch 3356:\tTraining Loss - 0.0059\n",
      "Epoch 3357:\tTraining Loss - 0.0064\n",
      "Epoch 3358:\tTraining Loss - 0.0080\n",
      "Epoch 3359:\tTraining Loss - 0.0038\n",
      "Epoch 3360:\tTraining Loss - 0.0092\n",
      "Epoch 3361:\tTraining Loss - 0.0085\n",
      "Epoch 3362:\tTraining Loss - 0.0064\n",
      "Epoch 3363:\tTraining Loss - 0.0060\n",
      "Epoch 3364:\tTraining Loss - 0.0056\n",
      "Epoch 3365:\tTraining Loss - 0.0027\n",
      "Epoch 3366:\tTraining Loss - 0.0075\n",
      "Epoch 3367:\tTraining Loss - 0.0059\n",
      "Epoch 3368:\tTraining Loss - 0.0095\n",
      "Epoch 3369:\tTraining Loss - 0.0068\n",
      "Epoch 3370:\tTraining Loss - 0.0071\n",
      "Epoch 3371:\tTraining Loss - 0.0085\n",
      "Epoch 3372:\tTraining Loss - 0.0050\n",
      "Epoch 3373:\tTraining Loss - 0.0049\n",
      "Epoch 3374:\tTraining Loss - 0.0078\n",
      "Epoch 3375:\tTraining Loss - 0.0060\n",
      "Epoch 3376:\tTraining Loss - 0.0053\n",
      "Epoch 3377:\tTraining Loss - 0.0045\n",
      "Epoch 3378:\tTraining Loss - 0.0050\n",
      "Epoch 3379:\tTraining Loss - 0.0038\n",
      "Epoch 3380:\tTraining Loss - 0.0070\n",
      "Epoch 3381:\tTraining Loss - 0.0043\n",
      "Epoch 3382:\tTraining Loss - 0.0052\n",
      "Epoch 3383:\tTraining Loss - 0.0037\n",
      "Epoch 3384:\tTraining Loss - 0.0097\n",
      "Epoch 3385:\tTraining Loss - 0.0028\n",
      "Epoch 3386:\tTraining Loss - 0.0074\n",
      "Epoch 3387:\tTraining Loss - 0.0045\n",
      "Epoch 3388:\tTraining Loss - 0.0046\n",
      "Epoch 3389:\tTraining Loss - 0.0091\n",
      "Epoch 3390:\tTraining Loss - 0.0043\n",
      "Epoch 3391:\tTraining Loss - 0.0067\n",
      "Epoch 3392:\tTraining Loss - 0.0069\n",
      "Epoch 3393:\tTraining Loss - 0.0068\n",
      "Epoch 3394:\tTraining Loss - 0.0066\n",
      "Epoch 3395:\tTraining Loss - 0.0056\n",
      "Epoch 3396:\tTraining Loss - 0.0034\n",
      "Epoch 3397:\tTraining Loss - 0.0055\n",
      "Epoch 3398:\tTraining Loss - 0.0081\n",
      "Epoch 3399:\tTraining Loss - 0.0063\n",
      "Epoch 3400:\tTraining Loss - 0.0085\n",
      "Epoch 3401:\tTraining Loss - 0.0048\n",
      "Epoch 3402:\tTraining Loss - 0.0109\n",
      "Epoch 3403:\tTraining Loss - 0.0054\n",
      "Epoch 3404:\tTraining Loss - 0.0049\n",
      "Epoch 3405:\tTraining Loss - 0.0081\n",
      "Epoch 3406:\tTraining Loss - 0.0026\n",
      "Epoch 3407:\tTraining Loss - 0.0048\n",
      "Epoch 3408:\tTraining Loss - 0.0077\n",
      "Epoch 3409:\tTraining Loss - 0.0038\n",
      "Epoch 3410:\tTraining Loss - 0.0063\n",
      "Epoch 3411:\tTraining Loss - 0.0039\n",
      "Epoch 3412:\tTraining Loss - 0.0072\n",
      "Epoch 3413:\tTraining Loss - 0.0078\n",
      "Epoch 3414:\tTraining Loss - 0.0111\n",
      "Epoch 3415:\tTraining Loss - 0.0035\n",
      "Epoch 3416:\tTraining Loss - 0.0056\n",
      "Epoch 3417:\tTraining Loss - 0.0076\n",
      "Epoch 3418:\tTraining Loss - 0.0085\n",
      "Epoch 3419:\tTraining Loss - 0.0043\n",
      "Epoch 3420:\tTraining Loss - 0.0044\n",
      "Epoch 3421:\tTraining Loss - 0.0044\n",
      "Epoch 3422:\tTraining Loss - 0.0116\n",
      "Epoch 3423:\tTraining Loss - 0.0090\n",
      "Epoch 3424:\tTraining Loss - 0.0057\n",
      "Epoch 3425:\tTraining Loss - 0.0070\n",
      "Epoch 3426:\tTraining Loss - 0.0094\n",
      "Epoch 3427:\tTraining Loss - 0.0050\n",
      "Epoch 3428:\tTraining Loss - 0.0111\n",
      "Epoch 3429:\tTraining Loss - 0.0034\n",
      "Epoch 3430:\tTraining Loss - 0.0052\n",
      "Epoch 3431:\tTraining Loss - 0.0043\n",
      "Epoch 3432:\tTraining Loss - 0.0079\n",
      "Epoch 3433:\tTraining Loss - 0.0071\n",
      "Epoch 3434:\tTraining Loss - 0.0073\n",
      "Epoch 3435:\tTraining Loss - 0.0047\n",
      "Epoch 3436:\tTraining Loss - 0.0060\n",
      "Epoch 3437:\tTraining Loss - 0.0047\n",
      "Epoch 3438:\tTraining Loss - 0.0072\n",
      "Epoch 3439:\tTraining Loss - 0.0050\n",
      "Epoch 3440:\tTraining Loss - 0.0057\n",
      "Epoch 3441:\tTraining Loss - 0.0143\n",
      "Epoch 3442:\tTraining Loss - 0.0064\n",
      "Epoch 3443:\tTraining Loss - 0.0058\n",
      "Epoch 3444:\tTraining Loss - 0.0058\n",
      "Epoch 3445:\tTraining Loss - 0.0046\n",
      "Epoch 3446:\tTraining Loss - 0.0091\n",
      "Epoch 3447:\tTraining Loss - 0.0058\n",
      "Epoch 3448:\tTraining Loss - 0.0037\n",
      "Epoch 3449:\tTraining Loss - 0.0058\n",
      "Epoch 3450:\tTraining Loss - 0.0065\n",
      "Epoch 3451:\tTraining Loss - 0.0086\n",
      "Epoch 3452:\tTraining Loss - 0.0041\n",
      "Epoch 3453:\tTraining Loss - 0.0070\n",
      "Epoch 3454:\tTraining Loss - 0.0077\n",
      "Epoch 3455:\tTraining Loss - 0.0054\n",
      "Epoch 3456:\tTraining Loss - 0.0061\n",
      "Epoch 3457:\tTraining Loss - 0.0048\n",
      "Epoch 3458:\tTraining Loss - 0.0114\n",
      "Epoch 3459:\tTraining Loss - 0.0078\n",
      "Epoch 3460:\tTraining Loss - 0.0061\n",
      "Epoch 3461:\tTraining Loss - 0.0053\n",
      "Epoch 3462:\tTraining Loss - 0.0073\n",
      "Epoch 3463:\tTraining Loss - 0.0107\n",
      "Epoch 3464:\tTraining Loss - 0.0050\n",
      "Epoch 3465:\tTraining Loss - 0.0059\n",
      "Epoch 3466:\tTraining Loss - 0.0028\n",
      "Epoch 3467:\tTraining Loss - 0.0054\n",
      "Epoch 3468:\tTraining Loss - 0.0069\n",
      "Epoch 3469:\tTraining Loss - 0.0065\n",
      "Epoch 3470:\tTraining Loss - 0.0047\n",
      "Epoch 3471:\tTraining Loss - 0.0066\n",
      "Epoch 3472:\tTraining Loss - 0.0045\n",
      "Epoch 3473:\tTraining Loss - 0.0106\n",
      "Epoch 3474:\tTraining Loss - 0.0070\n",
      "Epoch 3475:\tTraining Loss - 0.0065\n",
      "Epoch 3476:\tTraining Loss - 0.0051\n",
      "Epoch 3477:\tTraining Loss - 0.0046\n",
      "Epoch 3478:\tTraining Loss - 0.0047\n",
      "Epoch 3479:\tTraining Loss - 0.0077\n",
      "Epoch 3480:\tTraining Loss - 0.0039\n",
      "Epoch 3481:\tTraining Loss - 0.0073\n",
      "Epoch 3482:\tTraining Loss - 0.0060\n",
      "Epoch 3483:\tTraining Loss - 0.0090\n",
      "Epoch 3484:\tTraining Loss - 0.0093\n",
      "Epoch 3485:\tTraining Loss - 0.0038\n",
      "Epoch 3486:\tTraining Loss - 0.0063\n",
      "Epoch 3487:\tTraining Loss - 0.0088\n",
      "Epoch 3488:\tTraining Loss - 0.0101\n",
      "Epoch 3489:\tTraining Loss - 0.0042\n",
      "Epoch 3490:\tTraining Loss - 0.0053\n",
      "Epoch 3491:\tTraining Loss - 0.0060\n",
      "Epoch 3492:\tTraining Loss - 0.0064\n",
      "Epoch 3493:\tTraining Loss - 0.0049\n",
      "Epoch 3494:\tTraining Loss - 0.0046\n",
      "Epoch 3495:\tTraining Loss - 0.0042\n",
      "Epoch 3496:\tTraining Loss - 0.0046\n",
      "Epoch 3497:\tTraining Loss - 0.0074\n",
      "Epoch 3498:\tTraining Loss - 0.0067\n",
      "Epoch 3499:\tTraining Loss - 0.0076\n",
      "Epoch 3500:\tTraining Loss - 0.0151\n",
      "Epoch 3501:\tTraining Loss - 0.0071\n",
      "Epoch 3502:\tTraining Loss - 0.0072\n",
      "Epoch 3503:\tTraining Loss - 0.0072\n",
      "Epoch 3504:\tTraining Loss - 0.0069\n",
      "Epoch 3505:\tTraining Loss - 0.0094\n",
      "Epoch 3506:\tTraining Loss - 0.0055\n",
      "Epoch 3507:\tTraining Loss - 0.0098\n",
      "Epoch 3508:\tTraining Loss - 0.0093\n",
      "Epoch 3509:\tTraining Loss - 0.0063\n",
      "Epoch 3510:\tTraining Loss - 0.0056\n",
      "Epoch 3511:\tTraining Loss - 0.0074\n",
      "Epoch 3512:\tTraining Loss - 0.0040\n",
      "Epoch 3513:\tTraining Loss - 0.0033\n",
      "Epoch 3514:\tTraining Loss - 0.0039\n",
      "Epoch 3515:\tTraining Loss - 0.0134\n",
      "Epoch 3516:\tTraining Loss - 0.0099\n",
      "Epoch 3517:\tTraining Loss - 0.0040\n",
      "Epoch 3518:\tTraining Loss - 0.0043\n",
      "Epoch 3519:\tTraining Loss - 0.0040\n",
      "Epoch 3520:\tTraining Loss - 0.0035\n",
      "Epoch 3521:\tTraining Loss - 0.0029\n",
      "Epoch 3522:\tTraining Loss - 0.0089\n",
      "Epoch 3523:\tTraining Loss - 0.0089\n",
      "Epoch 3524:\tTraining Loss - 0.0043\n",
      "Epoch 3525:\tTraining Loss - 0.0064\n",
      "Epoch 3526:\tTraining Loss - 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3527:\tTraining Loss - 0.0078\n",
      "Epoch 3528:\tTraining Loss - 0.0106\n",
      "Epoch 3529:\tTraining Loss - 0.0046\n",
      "Epoch 3530:\tTraining Loss - 0.0073\n",
      "Epoch 3531:\tTraining Loss - 0.0045\n",
      "Epoch 3532:\tTraining Loss - 0.0060\n",
      "Epoch 3533:\tTraining Loss - 0.0063\n",
      "Epoch 3534:\tTraining Loss - 0.0045\n",
      "Epoch 3535:\tTraining Loss - 0.0070\n",
      "Epoch 3536:\tTraining Loss - 0.0052\n",
      "Epoch 3537:\tTraining Loss - 0.0103\n",
      "Epoch 3538:\tTraining Loss - 0.0063\n",
      "Epoch 3539:\tTraining Loss - 0.0031\n",
      "Epoch 3540:\tTraining Loss - 0.0082\n",
      "Epoch 3541:\tTraining Loss - 0.0042\n",
      "Epoch 3542:\tTraining Loss - 0.0085\n",
      "Epoch 3543:\tTraining Loss - 0.0091\n",
      "Epoch 3544:\tTraining Loss - 0.0087\n",
      "Epoch 3545:\tTraining Loss - 0.0054\n",
      "Epoch 3546:\tTraining Loss - 0.0031\n",
      "Epoch 3547:\tTraining Loss - 0.0099\n",
      "Epoch 3548:\tTraining Loss - 0.0094\n",
      "Epoch 3549:\tTraining Loss - 0.0065\n",
      "Epoch 3550:\tTraining Loss - 0.0061\n",
      "Epoch 3551:\tTraining Loss - 0.0048\n",
      "Epoch 3552:\tTraining Loss - 0.0078\n",
      "Epoch 3553:\tTraining Loss - 0.0049\n",
      "Epoch 3554:\tTraining Loss - 0.0048\n",
      "Epoch 3555:\tTraining Loss - 0.0067\n",
      "Epoch 3556:\tTraining Loss - 0.0106\n",
      "Epoch 3557:\tTraining Loss - 0.0053\n",
      "Epoch 3558:\tTraining Loss - 0.0075\n",
      "Epoch 3559:\tTraining Loss - 0.0071\n",
      "Epoch 3560:\tTraining Loss - 0.0048\n",
      "Epoch 3561:\tTraining Loss - 0.0050\n",
      "Epoch 3562:\tTraining Loss - 0.0074\n",
      "Epoch 3563:\tTraining Loss - 0.0065\n",
      "Epoch 3564:\tTraining Loss - 0.0059\n",
      "Epoch 3565:\tTraining Loss - 0.0069\n",
      "Epoch 3566:\tTraining Loss - 0.0053\n",
      "Epoch 3567:\tTraining Loss - 0.0061\n",
      "Epoch 3568:\tTraining Loss - 0.0097\n",
      "Epoch 3569:\tTraining Loss - 0.0067\n",
      "Epoch 3570:\tTraining Loss - 0.0044\n",
      "Epoch 3571:\tTraining Loss - 0.0099\n",
      "Epoch 3572:\tTraining Loss - 0.0062\n",
      "Epoch 3573:\tTraining Loss - 0.0068\n",
      "Epoch 3574:\tTraining Loss - 0.0090\n",
      "Epoch 3575:\tTraining Loss - 0.0062\n",
      "Epoch 3576:\tTraining Loss - 0.0054\n",
      "Epoch 3577:\tTraining Loss - 0.0037\n",
      "Epoch 3578:\tTraining Loss - 0.0051\n",
      "Epoch 3579:\tTraining Loss - 0.0048\n",
      "Epoch 3580:\tTraining Loss - 0.0045\n",
      "Epoch 3581:\tTraining Loss - 0.0065\n",
      "Epoch 3582:\tTraining Loss - 0.0055\n",
      "Epoch 3583:\tTraining Loss - 0.0065\n",
      "Epoch 3584:\tTraining Loss - 0.0100\n",
      "Epoch 3585:\tTraining Loss - 0.0044\n",
      "Epoch 3586:\tTraining Loss - 0.0043\n",
      "Epoch 3587:\tTraining Loss - 0.0051\n",
      "Epoch 3588:\tTraining Loss - 0.0105\n",
      "Epoch 3589:\tTraining Loss - 0.0043\n",
      "Epoch 3590:\tTraining Loss - 0.0076\n",
      "Epoch 3591:\tTraining Loss - 0.0095\n",
      "Epoch 3592:\tTraining Loss - 0.0054\n",
      "Epoch 3593:\tTraining Loss - 0.0049\n",
      "Epoch 3594:\tTraining Loss - 0.0050\n",
      "Epoch 3595:\tTraining Loss - 0.0044\n",
      "Epoch 3596:\tTraining Loss - 0.0031\n",
      "Epoch 3597:\tTraining Loss - 0.0078\n",
      "Epoch 3598:\tTraining Loss - 0.0056\n",
      "Epoch 3599:\tTraining Loss - 0.0068\n",
      "Epoch 3600:\tTraining Loss - 0.0043\n",
      "Epoch 3601:\tTraining Loss - 0.0095\n",
      "Epoch 3602:\tTraining Loss - 0.0064\n",
      "Epoch 3603:\tTraining Loss - 0.0023\n",
      "Epoch 3604:\tTraining Loss - 0.0023\n",
      "Epoch 3605:\tTraining Loss - 0.0046\n",
      "Epoch 3606:\tTraining Loss - 0.0089\n",
      "Epoch 3607:\tTraining Loss - 0.0042\n",
      "Epoch 3608:\tTraining Loss - 0.0083\n",
      "Epoch 3609:\tTraining Loss - 0.0100\n",
      "Epoch 3610:\tTraining Loss - 0.0081\n",
      "Epoch 3611:\tTraining Loss - 0.0082\n",
      "Epoch 3612:\tTraining Loss - 0.0048\n",
      "Epoch 3613:\tTraining Loss - 0.0056\n",
      "Epoch 3614:\tTraining Loss - 0.0053\n",
      "Epoch 3615:\tTraining Loss - 0.0056\n",
      "Epoch 3616:\tTraining Loss - 0.0068\n",
      "Epoch 3617:\tTraining Loss - 0.0048\n",
      "Epoch 3618:\tTraining Loss - 0.0056\n",
      "Epoch 3619:\tTraining Loss - 0.0058\n",
      "Epoch 3620:\tTraining Loss - 0.0068\n",
      "Epoch 3621:\tTraining Loss - 0.0078\n",
      "Epoch 3622:\tTraining Loss - 0.0096\n",
      "Epoch 3623:\tTraining Loss - 0.0083\n",
      "Epoch 3624:\tTraining Loss - 0.0083\n",
      "Epoch 3625:\tTraining Loss - 0.0041\n",
      "Epoch 3626:\tTraining Loss - 0.0047\n",
      "Epoch 3627:\tTraining Loss - 0.0037\n",
      "Epoch 3628:\tTraining Loss - 0.0050\n",
      "Epoch 3629:\tTraining Loss - 0.0052\n",
      "Epoch 3630:\tTraining Loss - 0.0074\n",
      "Epoch 3631:\tTraining Loss - 0.0068\n",
      "Epoch 3632:\tTraining Loss - 0.0033\n",
      "Epoch 3633:\tTraining Loss - 0.0067\n",
      "Epoch 3634:\tTraining Loss - 0.0062\n",
      "Epoch 3635:\tTraining Loss - 0.0056\n",
      "Epoch 3636:\tTraining Loss - 0.0040\n",
      "Epoch 3637:\tTraining Loss - 0.0073\n",
      "Epoch 3638:\tTraining Loss - 0.0056\n",
      "Epoch 3639:\tTraining Loss - 0.0080\n",
      "Epoch 3640:\tTraining Loss - 0.0054\n",
      "Epoch 3641:\tTraining Loss - 0.0059\n",
      "Epoch 3642:\tTraining Loss - 0.0052\n",
      "Epoch 3643:\tTraining Loss - 0.0055\n",
      "Epoch 3644:\tTraining Loss - 0.0070\n",
      "Epoch 3645:\tTraining Loss - 0.0080\n",
      "Epoch 3646:\tTraining Loss - 0.0066\n",
      "Epoch 3647:\tTraining Loss - 0.0053\n",
      "Epoch 3648:\tTraining Loss - 0.0030\n",
      "Epoch 3649:\tTraining Loss - 0.0063\n",
      "Epoch 3650:\tTraining Loss - 0.0026\n",
      "Epoch 3651:\tTraining Loss - 0.0066\n",
      "Epoch 3652:\tTraining Loss - 0.0058\n",
      "Epoch 3653:\tTraining Loss - 0.0102\n",
      "Epoch 3654:\tTraining Loss - 0.0046\n",
      "Epoch 3655:\tTraining Loss - 0.0113\n",
      "Epoch 3656:\tTraining Loss - 0.0045\n",
      "Epoch 3657:\tTraining Loss - 0.0081\n",
      "Epoch 3658:\tTraining Loss - 0.0069\n",
      "Epoch 3659:\tTraining Loss - 0.0047\n",
      "Epoch 3660:\tTraining Loss - 0.0114\n",
      "Epoch 3661:\tTraining Loss - 0.0061\n",
      "Epoch 3662:\tTraining Loss - 0.0058\n",
      "Epoch 3663:\tTraining Loss - 0.0088\n",
      "Epoch 3664:\tTraining Loss - 0.0088\n",
      "Epoch 3665:\tTraining Loss - 0.0043\n",
      "Epoch 3666:\tTraining Loss - 0.0088\n",
      "Epoch 3667:\tTraining Loss - 0.0044\n",
      "Epoch 3668:\tTraining Loss - 0.0033\n",
      "Epoch 3669:\tTraining Loss - 0.0048\n",
      "Epoch 3670:\tTraining Loss - 0.0079\n",
      "Epoch 3671:\tTraining Loss - 0.0064\n",
      "Epoch 3672:\tTraining Loss - 0.0057\n",
      "Epoch 3673:\tTraining Loss - 0.0066\n",
      "Epoch 3674:\tTraining Loss - 0.0077\n",
      "Epoch 3675:\tTraining Loss - 0.0074\n",
      "Epoch 3676:\tTraining Loss - 0.0083\n",
      "Epoch 3677:\tTraining Loss - 0.0053\n",
      "Epoch 3678:\tTraining Loss - 0.0068\n",
      "Epoch 3679:\tTraining Loss - 0.0064\n",
      "Epoch 3680:\tTraining Loss - 0.0048\n",
      "Epoch 3681:\tTraining Loss - 0.0057\n",
      "Epoch 3682:\tTraining Loss - 0.0055\n",
      "Epoch 3683:\tTraining Loss - 0.0040\n",
      "Epoch 3684:\tTraining Loss - 0.0041\n",
      "Epoch 3685:\tTraining Loss - 0.0058\n",
      "Epoch 3686:\tTraining Loss - 0.0037\n",
      "Epoch 3687:\tTraining Loss - 0.0027\n",
      "Epoch 3688:\tTraining Loss - 0.0058\n",
      "Epoch 3689:\tTraining Loss - 0.0050\n",
      "Epoch 3690:\tTraining Loss - 0.0114\n",
      "Epoch 3691:\tTraining Loss - 0.0024\n",
      "Epoch 3692:\tTraining Loss - 0.0086\n",
      "Epoch 3693:\tTraining Loss - 0.0059\n",
      "Epoch 3694:\tTraining Loss - 0.0072\n",
      "Epoch 3695:\tTraining Loss - 0.0070\n",
      "Epoch 3696:\tTraining Loss - 0.0104\n",
      "Epoch 3697:\tTraining Loss - 0.0026\n",
      "Epoch 3698:\tTraining Loss - 0.0049\n",
      "Epoch 3699:\tTraining Loss - 0.0044\n",
      "Epoch 3700:\tTraining Loss - 0.0098\n",
      "Epoch 3701:\tTraining Loss - 0.0069\n",
      "Epoch 3702:\tTraining Loss - 0.0041\n",
      "Epoch 3703:\tTraining Loss - 0.0084\n",
      "Epoch 3704:\tTraining Loss - 0.0134\n",
      "Epoch 3705:\tTraining Loss - 0.0041\n",
      "Epoch 3706:\tTraining Loss - 0.0041\n",
      "Epoch 3707:\tTraining Loss - 0.0044\n",
      "Epoch 3708:\tTraining Loss - 0.0039\n",
      "Epoch 3709:\tTraining Loss - 0.0048\n",
      "Epoch 3710:\tTraining Loss - 0.0033\n",
      "Epoch 3711:\tTraining Loss - 0.0050\n",
      "Epoch 3712:\tTraining Loss - 0.0031\n",
      "Epoch 3713:\tTraining Loss - 0.0066\n",
      "Epoch 3714:\tTraining Loss - 0.0059\n",
      "Epoch 3715:\tTraining Loss - 0.0025\n",
      "Epoch 3716:\tTraining Loss - 0.0081\n",
      "Epoch 3717:\tTraining Loss - 0.0045\n",
      "Epoch 3718:\tTraining Loss - 0.0066\n",
      "Epoch 3719:\tTraining Loss - 0.0055\n",
      "Epoch 3720:\tTraining Loss - 0.0061\n",
      "Epoch 3721:\tTraining Loss - 0.0050\n",
      "Epoch 3722:\tTraining Loss - 0.0093\n",
      "Epoch 3723:\tTraining Loss - 0.0042\n",
      "Epoch 3724:\tTraining Loss - 0.0085\n",
      "Epoch 3725:\tTraining Loss - 0.0046\n",
      "Epoch 3726:\tTraining Loss - 0.0059\n",
      "Epoch 3727:\tTraining Loss - 0.0072\n",
      "Epoch 3728:\tTraining Loss - 0.0062\n",
      "Epoch 3729:\tTraining Loss - 0.0075\n",
      "Epoch 3730:\tTraining Loss - 0.0070\n",
      "Epoch 3731:\tTraining Loss - 0.0105\n",
      "Epoch 3732:\tTraining Loss - 0.0049\n",
      "Epoch 3733:\tTraining Loss - 0.0048\n",
      "Epoch 3734:\tTraining Loss - 0.0038\n",
      "Epoch 3735:\tTraining Loss - 0.0081\n",
      "Epoch 3736:\tTraining Loss - 0.0047\n",
      "Epoch 3737:\tTraining Loss - 0.0061\n",
      "Epoch 3738:\tTraining Loss - 0.0052\n",
      "Epoch 3739:\tTraining Loss - 0.0087\n",
      "Epoch 3740:\tTraining Loss - 0.0049\n",
      "Epoch 3741:\tTraining Loss - 0.0063\n",
      "Epoch 3742:\tTraining Loss - 0.0052\n",
      "Epoch 3743:\tTraining Loss - 0.0038\n",
      "Epoch 3744:\tTraining Loss - 0.0056\n",
      "Epoch 3745:\tTraining Loss - 0.0054\n",
      "Epoch 3746:\tTraining Loss - 0.0034\n",
      "Epoch 3747:\tTraining Loss - 0.0060\n",
      "Epoch 3748:\tTraining Loss - 0.0066\n",
      "Epoch 3749:\tTraining Loss - 0.0134\n",
      "Epoch 3750:\tTraining Loss - 0.0076\n",
      "Epoch 3751:\tTraining Loss - 0.0036\n",
      "Epoch 3752:\tTraining Loss - 0.0044\n",
      "Epoch 3753:\tTraining Loss - 0.0077\n",
      "Epoch 3754:\tTraining Loss - 0.0028\n",
      "Epoch 3755:\tTraining Loss - 0.0113\n",
      "Epoch 3756:\tTraining Loss - 0.0051\n",
      "Epoch 3757:\tTraining Loss - 0.0065\n",
      "Epoch 3758:\tTraining Loss - 0.0055\n",
      "Epoch 3759:\tTraining Loss - 0.0062\n",
      "Epoch 3760:\tTraining Loss - 0.0089\n",
      "Epoch 3761:\tTraining Loss - 0.0089\n",
      "Epoch 3762:\tTraining Loss - 0.0056\n",
      "Epoch 3763:\tTraining Loss - 0.0055\n",
      "Epoch 3764:\tTraining Loss - 0.0080\n",
      "Epoch 3765:\tTraining Loss - 0.0046\n",
      "Epoch 3766:\tTraining Loss - 0.0063\n",
      "Epoch 3767:\tTraining Loss - 0.0078\n",
      "Epoch 3768:\tTraining Loss - 0.0046\n",
      "Epoch 3769:\tTraining Loss - 0.0037\n",
      "Epoch 3770:\tTraining Loss - 0.0075\n",
      "Epoch 3771:\tTraining Loss - 0.0064\n",
      "Epoch 3772:\tTraining Loss - 0.0054\n",
      "Epoch 3773:\tTraining Loss - 0.0047\n",
      "Epoch 3774:\tTraining Loss - 0.0068\n",
      "Epoch 3775:\tTraining Loss - 0.0025\n",
      "Epoch 3776:\tTraining Loss - 0.0046\n",
      "Epoch 3777:\tTraining Loss - 0.0072\n",
      "Epoch 3778:\tTraining Loss - 0.0045\n",
      "Epoch 3779:\tTraining Loss - 0.0077\n",
      "Epoch 3780:\tTraining Loss - 0.0074\n",
      "Epoch 3781:\tTraining Loss - 0.0063\n",
      "Epoch 3782:\tTraining Loss - 0.0055\n",
      "Epoch 3783:\tTraining Loss - 0.0043\n",
      "Epoch 3784:\tTraining Loss - 0.0037\n",
      "Epoch 3785:\tTraining Loss - 0.0029\n",
      "Epoch 3786:\tTraining Loss - 0.0040\n",
      "Epoch 3787:\tTraining Loss - 0.0071\n",
      "Epoch 3788:\tTraining Loss - 0.0036\n",
      "Epoch 3789:\tTraining Loss - 0.0068\n",
      "Epoch 3790:\tTraining Loss - 0.0058\n",
      "Epoch 3791:\tTraining Loss - 0.0056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3792:\tTraining Loss - 0.0057\n",
      "Epoch 3793:\tTraining Loss - 0.0037\n",
      "Epoch 3794:\tTraining Loss - 0.0035\n",
      "Epoch 3795:\tTraining Loss - 0.0069\n",
      "Epoch 3796:\tTraining Loss - 0.0024\n",
      "Epoch 3797:\tTraining Loss - 0.0041\n",
      "Epoch 3798:\tTraining Loss - 0.0047\n",
      "Epoch 3799:\tTraining Loss - 0.0072\n",
      "Epoch 3800:\tTraining Loss - 0.0116\n",
      "Epoch 3801:\tTraining Loss - 0.0050\n",
      "Epoch 3802:\tTraining Loss - 0.0083\n",
      "Epoch 3803:\tTraining Loss - 0.0024\n",
      "Epoch 3804:\tTraining Loss - 0.0054\n",
      "Epoch 3805:\tTraining Loss - 0.0082\n",
      "Epoch 3806:\tTraining Loss - 0.0075\n",
      "Epoch 3807:\tTraining Loss - 0.0058\n",
      "Epoch 3808:\tTraining Loss - 0.0047\n",
      "Epoch 3809:\tTraining Loss - 0.0127\n",
      "Epoch 3810:\tTraining Loss - 0.0051\n",
      "Epoch 3811:\tTraining Loss - 0.0046\n",
      "Epoch 3812:\tTraining Loss - 0.0043\n",
      "Epoch 3813:\tTraining Loss - 0.0033\n",
      "Epoch 3814:\tTraining Loss - 0.0040\n",
      "Epoch 3815:\tTraining Loss - 0.0085\n",
      "Epoch 3816:\tTraining Loss - 0.0053\n",
      "Epoch 3817:\tTraining Loss - 0.0085\n",
      "Epoch 3818:\tTraining Loss - 0.0041\n",
      "Epoch 3819:\tTraining Loss - 0.0059\n",
      "Epoch 3820:\tTraining Loss - 0.0037\n",
      "Epoch 3821:\tTraining Loss - 0.0049\n",
      "Epoch 3822:\tTraining Loss - 0.0080\n",
      "Epoch 3823:\tTraining Loss - 0.0049\n",
      "Epoch 3824:\tTraining Loss - 0.0040\n",
      "Epoch 3825:\tTraining Loss - 0.0040\n",
      "Epoch 3826:\tTraining Loss - 0.0053\n",
      "Epoch 3827:\tTraining Loss - 0.0085\n",
      "Epoch 3828:\tTraining Loss - 0.0039\n",
      "Epoch 3829:\tTraining Loss - 0.0039\n",
      "Epoch 3830:\tTraining Loss - 0.0107\n",
      "Epoch 3831:\tTraining Loss - 0.0087\n",
      "Epoch 3832:\tTraining Loss - 0.0101\n",
      "Epoch 3833:\tTraining Loss - 0.0050\n",
      "Epoch 3834:\tTraining Loss - 0.0080\n",
      "Epoch 3835:\tTraining Loss - 0.0086\n",
      "Epoch 3836:\tTraining Loss - 0.0034\n",
      "Epoch 3837:\tTraining Loss - 0.0059\n",
      "Epoch 3838:\tTraining Loss - 0.0051\n",
      "Epoch 3839:\tTraining Loss - 0.0046\n",
      "Epoch 3840:\tTraining Loss - 0.0106\n",
      "Epoch 3841:\tTraining Loss - 0.0041\n",
      "Epoch 3842:\tTraining Loss - 0.0078\n",
      "Epoch 3843:\tTraining Loss - 0.0070\n",
      "Epoch 3844:\tTraining Loss - 0.0113\n",
      "Epoch 3845:\tTraining Loss - 0.0087\n",
      "Epoch 3846:\tTraining Loss - 0.0052\n",
      "Epoch 3847:\tTraining Loss - 0.0053\n",
      "Epoch 3848:\tTraining Loss - 0.0049\n",
      "Epoch 3849:\tTraining Loss - 0.0077\n",
      "Epoch 3850:\tTraining Loss - 0.0094\n",
      "Epoch 3851:\tTraining Loss - 0.0100\n",
      "Epoch 3852:\tTraining Loss - 0.0098\n",
      "Epoch 3853:\tTraining Loss - 0.0041\n",
      "Epoch 3854:\tTraining Loss - 0.0045\n",
      "Epoch 3855:\tTraining Loss - 0.0065\n",
      "Epoch 3856:\tTraining Loss - 0.0050\n",
      "Epoch 3857:\tTraining Loss - 0.0048\n",
      "Epoch 3858:\tTraining Loss - 0.0030\n",
      "Epoch 3859:\tTraining Loss - 0.0069\n",
      "Epoch 3860:\tTraining Loss - 0.0046\n",
      "Epoch 3861:\tTraining Loss - 0.0050\n",
      "Epoch 3862:\tTraining Loss - 0.0045\n",
      "Epoch 3863:\tTraining Loss - 0.0051\n",
      "Epoch 3864:\tTraining Loss - 0.0099\n",
      "Epoch 3865:\tTraining Loss - 0.0052\n",
      "Epoch 3866:\tTraining Loss - 0.0066\n",
      "Epoch 3867:\tTraining Loss - 0.0039\n",
      "Epoch 3868:\tTraining Loss - 0.0031\n",
      "Epoch 3869:\tTraining Loss - 0.0031\n",
      "Epoch 3870:\tTraining Loss - 0.0074\n",
      "Epoch 3871:\tTraining Loss - 0.0072\n",
      "Epoch 3872:\tTraining Loss - 0.0051\n",
      "Epoch 3873:\tTraining Loss - 0.0032\n",
      "Epoch 3874:\tTraining Loss - 0.0076\n",
      "Epoch 3875:\tTraining Loss - 0.0061\n",
      "Epoch 3876:\tTraining Loss - 0.0071\n",
      "Epoch 3877:\tTraining Loss - 0.0031\n",
      "Epoch 3878:\tTraining Loss - 0.0078\n",
      "Epoch 3879:\tTraining Loss - 0.0060\n",
      "Epoch 3880:\tTraining Loss - 0.0036\n",
      "Epoch 3881:\tTraining Loss - 0.0055\n",
      "Epoch 3882:\tTraining Loss - 0.0062\n",
      "Epoch 3883:\tTraining Loss - 0.0075\n",
      "Epoch 3884:\tTraining Loss - 0.0047\n",
      "Epoch 3885:\tTraining Loss - 0.0058\n",
      "Epoch 3886:\tTraining Loss - 0.0076\n",
      "Epoch 3887:\tTraining Loss - 0.0048\n",
      "Epoch 3888:\tTraining Loss - 0.0081\n",
      "Epoch 3889:\tTraining Loss - 0.0050\n",
      "Epoch 3890:\tTraining Loss - 0.0064\n",
      "Epoch 3891:\tTraining Loss - 0.0080\n",
      "Epoch 3892:\tTraining Loss - 0.0043\n",
      "Epoch 3893:\tTraining Loss - 0.0127\n",
      "Epoch 3894:\tTraining Loss - 0.0053\n",
      "Epoch 3895:\tTraining Loss - 0.0033\n",
      "Epoch 3896:\tTraining Loss - 0.0081\n",
      "Epoch 3897:\tTraining Loss - 0.0077\n",
      "Epoch 3898:\tTraining Loss - 0.0057\n",
      "Epoch 3899:\tTraining Loss - 0.0052\n",
      "Epoch 3900:\tTraining Loss - 0.0121\n",
      "Epoch 3901:\tTraining Loss - 0.0056\n",
      "Epoch 3902:\tTraining Loss - 0.0032\n",
      "Epoch 3903:\tTraining Loss - 0.0048\n",
      "Epoch 3904:\tTraining Loss - 0.0056\n",
      "Epoch 3905:\tTraining Loss - 0.0058\n",
      "Epoch 3906:\tTraining Loss - 0.0062\n",
      "Epoch 3907:\tTraining Loss - 0.0063\n",
      "Epoch 3908:\tTraining Loss - 0.0085\n",
      "Epoch 3909:\tTraining Loss - 0.0069\n",
      "Epoch 3910:\tTraining Loss - 0.0068\n",
      "Epoch 3911:\tTraining Loss - 0.0048\n",
      "Epoch 3912:\tTraining Loss - 0.0045\n",
      "Epoch 3913:\tTraining Loss - 0.0085\n",
      "Epoch 3914:\tTraining Loss - 0.0095\n",
      "Epoch 3915:\tTraining Loss - 0.0031\n",
      "Epoch 3916:\tTraining Loss - 0.0030\n",
      "Epoch 3917:\tTraining Loss - 0.0089\n",
      "Epoch 3918:\tTraining Loss - 0.0094\n",
      "Epoch 3919:\tTraining Loss - 0.0056\n",
      "Epoch 3920:\tTraining Loss - 0.0066\n",
      "Epoch 3921:\tTraining Loss - 0.0098\n",
      "Epoch 3922:\tTraining Loss - 0.0029\n",
      "Epoch 3923:\tTraining Loss - 0.0085\n",
      "Epoch 3924:\tTraining Loss - 0.0054\n",
      "Epoch 3925:\tTraining Loss - 0.0058\n",
      "Epoch 3926:\tTraining Loss - 0.0064\n",
      "Epoch 3927:\tTraining Loss - 0.0064\n",
      "Epoch 3928:\tTraining Loss - 0.0059\n",
      "Epoch 3929:\tTraining Loss - 0.0084\n",
      "Epoch 3930:\tTraining Loss - 0.0050\n",
      "Epoch 3931:\tTraining Loss - 0.0044\n",
      "Epoch 3932:\tTraining Loss - 0.0071\n",
      "Epoch 3933:\tTraining Loss - 0.0033\n",
      "Epoch 3934:\tTraining Loss - 0.0046\n",
      "Epoch 3935:\tTraining Loss - 0.0066\n",
      "Epoch 3936:\tTraining Loss - 0.0084\n",
      "Epoch 3937:\tTraining Loss - 0.0044\n",
      "Epoch 3938:\tTraining Loss - 0.0044\n",
      "Epoch 3939:\tTraining Loss - 0.0040\n",
      "Epoch 3940:\tTraining Loss - 0.0044\n",
      "Epoch 3941:\tTraining Loss - 0.0084\n",
      "Epoch 3942:\tTraining Loss - 0.0047\n",
      "Epoch 3943:\tTraining Loss - 0.0037\n",
      "Epoch 3944:\tTraining Loss - 0.0039\n",
      "Epoch 3945:\tTraining Loss - 0.0052\n",
      "Epoch 3946:\tTraining Loss - 0.0031\n",
      "Epoch 3947:\tTraining Loss - 0.0054\n",
      "Epoch 3948:\tTraining Loss - 0.0052\n",
      "Epoch 3949:\tTraining Loss - 0.0031\n",
      "Epoch 3950:\tTraining Loss - 0.0076\n",
      "Epoch 3951:\tTraining Loss - 0.0042\n",
      "Epoch 3952:\tTraining Loss - 0.0034\n",
      "Epoch 3953:\tTraining Loss - 0.0037\n",
      "Epoch 3954:\tTraining Loss - 0.0055\n",
      "Epoch 3955:\tTraining Loss - 0.0070\n",
      "Epoch 3956:\tTraining Loss - 0.0102\n",
      "Epoch 3957:\tTraining Loss - 0.0094\n",
      "Epoch 3958:\tTraining Loss - 0.0062\n",
      "Epoch 3959:\tTraining Loss - 0.0047\n",
      "Epoch 3960:\tTraining Loss - 0.0081\n",
      "Epoch 3961:\tTraining Loss - 0.0047\n",
      "Epoch 3962:\tTraining Loss - 0.0119\n",
      "Epoch 3963:\tTraining Loss - 0.0065\n",
      "Epoch 3964:\tTraining Loss - 0.0049\n",
      "Epoch 3965:\tTraining Loss - 0.0095\n",
      "Epoch 3966:\tTraining Loss - 0.0082\n",
      "Epoch 3967:\tTraining Loss - 0.0044\n",
      "Epoch 3968:\tTraining Loss - 0.0051\n",
      "Epoch 3969:\tTraining Loss - 0.0071\n",
      "Epoch 3970:\tTraining Loss - 0.0046\n",
      "Epoch 3971:\tTraining Loss - 0.0027\n",
      "Epoch 3972:\tTraining Loss - 0.0074\n",
      "Epoch 3973:\tTraining Loss - 0.0051\n",
      "Epoch 3974:\tTraining Loss - 0.0090\n",
      "Epoch 3975:\tTraining Loss - 0.0084\n",
      "Epoch 3976:\tTraining Loss - 0.0075\n",
      "Epoch 3977:\tTraining Loss - 0.0073\n",
      "Epoch 3978:\tTraining Loss - 0.0035\n",
      "Epoch 3979:\tTraining Loss - 0.0038\n",
      "Epoch 3980:\tTraining Loss - 0.0036\n",
      "Epoch 3981:\tTraining Loss - 0.0043\n",
      "Epoch 3982:\tTraining Loss - 0.0062\n",
      "Epoch 3983:\tTraining Loss - 0.0076\n",
      "Epoch 3984:\tTraining Loss - 0.0048\n",
      "Epoch 3985:\tTraining Loss - 0.0047\n",
      "Epoch 3986:\tTraining Loss - 0.0080\n",
      "Epoch 3987:\tTraining Loss - 0.0033\n",
      "Epoch 3988:\tTraining Loss - 0.0043\n",
      "Epoch 3989:\tTraining Loss - 0.0026\n",
      "Epoch 3990:\tTraining Loss - 0.0059\n",
      "Epoch 3991:\tTraining Loss - 0.0071\n",
      "Epoch 3992:\tTraining Loss - 0.0037\n",
      "Epoch 3993:\tTraining Loss - 0.0075\n",
      "Epoch 3994:\tTraining Loss - 0.0042\n",
      "Epoch 3995:\tTraining Loss - 0.0069\n",
      "Epoch 3996:\tTraining Loss - 0.0037\n",
      "Epoch 3997:\tTraining Loss - 0.0063\n",
      "Epoch 3998:\tTraining Loss - 0.0032\n",
      "Epoch 3999:\tTraining Loss - 0.0048\n",
      "Epoch 4000:\tTraining Loss - 0.0024\n",
      "Epoch 4001:\tTraining Loss - 0.0044\n",
      "Epoch 4002:\tTraining Loss - 0.0039\n",
      "Epoch 4003:\tTraining Loss - 0.0028\n",
      "Epoch 4004:\tTraining Loss - 0.0031\n",
      "Epoch 4005:\tTraining Loss - 0.0037\n",
      "Epoch 4006:\tTraining Loss - 0.0058\n",
      "Epoch 4007:\tTraining Loss - 0.0026\n",
      "Epoch 4008:\tTraining Loss - 0.0062\n",
      "Epoch 4009:\tTraining Loss - 0.0098\n",
      "Epoch 4010:\tTraining Loss - 0.0026\n",
      "Epoch 4011:\tTraining Loss - 0.0056\n",
      "Epoch 4012:\tTraining Loss - 0.0074\n",
      "Epoch 4013:\tTraining Loss - 0.0054\n",
      "Epoch 4014:\tTraining Loss - 0.0047\n",
      "Epoch 4015:\tTraining Loss - 0.0038\n",
      "Epoch 4016:\tTraining Loss - 0.0060\n",
      "Epoch 4017:\tTraining Loss - 0.0110\n",
      "Epoch 4018:\tTraining Loss - 0.0043\n",
      "Epoch 4019:\tTraining Loss - 0.0031\n",
      "Epoch 4020:\tTraining Loss - 0.0030\n",
      "Epoch 4021:\tTraining Loss - 0.0064\n",
      "Epoch 4022:\tTraining Loss - 0.0029\n",
      "Epoch 4023:\tTraining Loss - 0.0031\n",
      "Epoch 4024:\tTraining Loss - 0.0074\n",
      "Epoch 4025:\tTraining Loss - 0.0093\n",
      "Epoch 4026:\tTraining Loss - 0.0023\n",
      "Epoch 4027:\tTraining Loss - 0.0061\n",
      "Epoch 4028:\tTraining Loss - 0.0068\n",
      "Epoch 4029:\tTraining Loss - 0.0086\n",
      "Epoch 4030:\tTraining Loss - 0.0074\n",
      "Epoch 4031:\tTraining Loss - 0.0068\n",
      "Epoch 4032:\tTraining Loss - 0.0027\n",
      "Epoch 4033:\tTraining Loss - 0.0041\n",
      "Epoch 4034:\tTraining Loss - 0.0048\n",
      "Epoch 4035:\tTraining Loss - 0.0073\n",
      "Epoch 4036:\tTraining Loss - 0.0062\n",
      "Epoch 4037:\tTraining Loss - 0.0063\n",
      "Epoch 4038:\tTraining Loss - 0.0104\n",
      "Epoch 4039:\tTraining Loss - 0.0041\n",
      "Epoch 4040:\tTraining Loss - 0.0070\n",
      "Epoch 4041:\tTraining Loss - 0.0052\n",
      "Epoch 4042:\tTraining Loss - 0.0068\n",
      "Epoch 4043:\tTraining Loss - 0.0070\n",
      "Epoch 4044:\tTraining Loss - 0.0058\n",
      "Epoch 4045:\tTraining Loss - 0.0040\n",
      "Epoch 4046:\tTraining Loss - 0.0056\n",
      "Epoch 4047:\tTraining Loss - 0.0056\n",
      "Epoch 4048:\tTraining Loss - 0.0048\n",
      "Epoch 4049:\tTraining Loss - 0.0042\n",
      "Epoch 4050:\tTraining Loss - 0.0056\n",
      "Epoch 4051:\tTraining Loss - 0.0061\n",
      "Epoch 4052:\tTraining Loss - 0.0047\n",
      "Epoch 4053:\tTraining Loss - 0.0058\n",
      "Epoch 4054:\tTraining Loss - 0.0088\n",
      "Epoch 4055:\tTraining Loss - 0.0085\n",
      "Epoch 4056:\tTraining Loss - 0.0025\n",
      "Epoch 4057:\tTraining Loss - 0.0050\n",
      "Epoch 4058:\tTraining Loss - 0.0058\n",
      "Epoch 4059:\tTraining Loss - 0.0044\n",
      "Epoch 4060:\tTraining Loss - 0.0076\n",
      "Epoch 4061:\tTraining Loss - 0.0032\n",
      "Epoch 4062:\tTraining Loss - 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4063:\tTraining Loss - 0.0050\n",
      "Epoch 4064:\tTraining Loss - 0.0043\n",
      "Epoch 4065:\tTraining Loss - 0.0054\n",
      "Epoch 4066:\tTraining Loss - 0.0083\n",
      "Epoch 4067:\tTraining Loss - 0.0042\n",
      "Epoch 4068:\tTraining Loss - 0.0063\n",
      "Epoch 4069:\tTraining Loss - 0.0050\n",
      "Epoch 4070:\tTraining Loss - 0.0071\n",
      "Epoch 4071:\tTraining Loss - 0.0096\n",
      "Epoch 4072:\tTraining Loss - 0.0074\n",
      "Epoch 4073:\tTraining Loss - 0.0064\n",
      "Epoch 4074:\tTraining Loss - 0.0070\n",
      "Epoch 4075:\tTraining Loss - 0.0059\n",
      "Epoch 4076:\tTraining Loss - 0.0088\n",
      "Epoch 4077:\tTraining Loss - 0.0079\n",
      "Epoch 4078:\tTraining Loss - 0.0072\n",
      "Epoch 4079:\tTraining Loss - 0.0088\n",
      "Epoch 4080:\tTraining Loss - 0.0048\n",
      "Epoch 4081:\tTraining Loss - 0.0070\n",
      "Epoch 4082:\tTraining Loss - 0.0055\n",
      "Epoch 4083:\tTraining Loss - 0.0060\n",
      "Epoch 4084:\tTraining Loss - 0.0056\n",
      "Epoch 4085:\tTraining Loss - 0.0035\n",
      "Epoch 4086:\tTraining Loss - 0.0042\n",
      "Epoch 4087:\tTraining Loss - 0.0045\n",
      "Epoch 4088:\tTraining Loss - 0.0032\n",
      "Epoch 4089:\tTraining Loss - 0.0064\n",
      "Epoch 4090:\tTraining Loss - 0.0068\n",
      "Epoch 4091:\tTraining Loss - 0.0120\n",
      "Epoch 4092:\tTraining Loss - 0.0105\n",
      "Epoch 4093:\tTraining Loss - 0.0117\n",
      "Epoch 4094:\tTraining Loss - 0.0073\n",
      "Epoch 4095:\tTraining Loss - 0.0051\n",
      "Epoch 4096:\tTraining Loss - 0.0081\n",
      "Epoch 4097:\tTraining Loss - 0.0039\n",
      "Epoch 4098:\tTraining Loss - 0.0030\n",
      "Epoch 4099:\tTraining Loss - 0.0046\n",
      "Epoch 4100:\tTraining Loss - 0.0045\n",
      "Epoch 4101:\tTraining Loss - 0.0110\n",
      "Epoch 4102:\tTraining Loss - 0.0073\n",
      "Epoch 4103:\tTraining Loss - 0.0080\n",
      "Epoch 4104:\tTraining Loss - 0.0069\n",
      "Epoch 4105:\tTraining Loss - 0.0048\n",
      "Epoch 4106:\tTraining Loss - 0.0078\n",
      "Epoch 4107:\tTraining Loss - 0.0066\n",
      "Epoch 4108:\tTraining Loss - 0.0029\n",
      "Epoch 4109:\tTraining Loss - 0.0043\n",
      "Epoch 4110:\tTraining Loss - 0.0066\n",
      "Epoch 4111:\tTraining Loss - 0.0067\n",
      "Epoch 4112:\tTraining Loss - 0.0054\n",
      "Epoch 4113:\tTraining Loss - 0.0057\n",
      "Epoch 4114:\tTraining Loss - 0.0056\n",
      "Epoch 4115:\tTraining Loss - 0.0048\n",
      "Epoch 4116:\tTraining Loss - 0.0036\n",
      "Epoch 4117:\tTraining Loss - 0.0040\n",
      "Epoch 4118:\tTraining Loss - 0.0099\n",
      "Epoch 4119:\tTraining Loss - 0.0076\n",
      "Epoch 4120:\tTraining Loss - 0.0049\n",
      "Epoch 4121:\tTraining Loss - 0.0069\n",
      "Epoch 4122:\tTraining Loss - 0.0084\n",
      "Epoch 4123:\tTraining Loss - 0.0066\n",
      "Epoch 4124:\tTraining Loss - 0.0048\n",
      "Epoch 4125:\tTraining Loss - 0.0100\n",
      "Epoch 4126:\tTraining Loss - 0.0059\n",
      "Epoch 4127:\tTraining Loss - 0.0070\n",
      "Epoch 4128:\tTraining Loss - 0.0039\n",
      "Epoch 4129:\tTraining Loss - 0.0056\n",
      "Epoch 4130:\tTraining Loss - 0.0081\n",
      "Epoch 4131:\tTraining Loss - 0.0057\n",
      "Epoch 4132:\tTraining Loss - 0.0052\n",
      "Epoch 4133:\tTraining Loss - 0.0031\n",
      "Epoch 4134:\tTraining Loss - 0.0029\n",
      "Epoch 4135:\tTraining Loss - 0.0038\n",
      "Epoch 4136:\tTraining Loss - 0.0053\n",
      "Epoch 4137:\tTraining Loss - 0.0034\n",
      "Epoch 4138:\tTraining Loss - 0.0040\n",
      "Epoch 4139:\tTraining Loss - 0.0064\n",
      "Epoch 4140:\tTraining Loss - 0.0041\n",
      "Epoch 4141:\tTraining Loss - 0.0061\n",
      "Epoch 4142:\tTraining Loss - 0.0036\n",
      "Epoch 4143:\tTraining Loss - 0.0026\n",
      "Epoch 4144:\tTraining Loss - 0.0064\n",
      "Epoch 4145:\tTraining Loss - 0.0073\n",
      "Epoch 4146:\tTraining Loss - 0.0113\n",
      "Epoch 4147:\tTraining Loss - 0.0093\n",
      "Epoch 4148:\tTraining Loss - 0.0066\n",
      "Epoch 4149:\tTraining Loss - 0.0040\n",
      "Epoch 4150:\tTraining Loss - 0.0058\n",
      "Epoch 4151:\tTraining Loss - 0.0084\n",
      "Epoch 4152:\tTraining Loss - 0.0078\n",
      "Epoch 4153:\tTraining Loss - 0.0070\n",
      "Epoch 4154:\tTraining Loss - 0.0042\n",
      "Epoch 4155:\tTraining Loss - 0.0042\n",
      "Epoch 4156:\tTraining Loss - 0.0051\n",
      "Epoch 4157:\tTraining Loss - 0.0042\n",
      "Epoch 4158:\tTraining Loss - 0.0069\n",
      "Epoch 4159:\tTraining Loss - 0.0094\n",
      "Epoch 4160:\tTraining Loss - 0.0040\n",
      "Epoch 4161:\tTraining Loss - 0.0072\n",
      "Epoch 4162:\tTraining Loss - 0.0056\n",
      "Epoch 4163:\tTraining Loss - 0.0049\n",
      "Epoch 4164:\tTraining Loss - 0.0058\n",
      "Epoch 4165:\tTraining Loss - 0.0054\n",
      "Epoch 4166:\tTraining Loss - 0.0035\n",
      "Epoch 4167:\tTraining Loss - 0.0053\n",
      "Epoch 4168:\tTraining Loss - 0.0060\n",
      "Epoch 4169:\tTraining Loss - 0.0061\n",
      "Epoch 4170:\tTraining Loss - 0.0054\n",
      "Epoch 4171:\tTraining Loss - 0.0066\n",
      "Epoch 4172:\tTraining Loss - 0.0068\n",
      "Epoch 4173:\tTraining Loss - 0.0073\n",
      "Epoch 4174:\tTraining Loss - 0.0060\n",
      "Epoch 4175:\tTraining Loss - 0.0064\n",
      "Epoch 4176:\tTraining Loss - 0.0036\n",
      "Epoch 4177:\tTraining Loss - 0.0051\n",
      "Epoch 4178:\tTraining Loss - 0.0056\n",
      "Epoch 4179:\tTraining Loss - 0.0066\n",
      "Epoch 4180:\tTraining Loss - 0.0068\n",
      "Epoch 4181:\tTraining Loss - 0.0043\n",
      "Epoch 4182:\tTraining Loss - 0.0051\n",
      "Epoch 4183:\tTraining Loss - 0.0075\n",
      "Epoch 4184:\tTraining Loss - 0.0033\n",
      "Epoch 4185:\tTraining Loss - 0.0046\n",
      "Epoch 4186:\tTraining Loss - 0.0072\n",
      "Epoch 4187:\tTraining Loss - 0.0058\n",
      "Epoch 4188:\tTraining Loss - 0.0039\n",
      "Epoch 4189:\tTraining Loss - 0.0066\n",
      "Epoch 4190:\tTraining Loss - 0.0029\n",
      "Epoch 4191:\tTraining Loss - 0.0030\n",
      "Epoch 4192:\tTraining Loss - 0.0080\n",
      "Epoch 4193:\tTraining Loss - 0.0052\n",
      "Epoch 4194:\tTraining Loss - 0.0030\n",
      "Epoch 4195:\tTraining Loss - 0.0057\n",
      "Epoch 4196:\tTraining Loss - 0.0065\n",
      "Epoch 4197:\tTraining Loss - 0.0041\n",
      "Epoch 4198:\tTraining Loss - 0.0061\n",
      "Epoch 4199:\tTraining Loss - 0.0052\n",
      "Epoch 4200:\tTraining Loss - 0.0043\n",
      "Epoch 4201:\tTraining Loss - 0.0058\n",
      "Epoch 4202:\tTraining Loss - 0.0061\n",
      "Epoch 4203:\tTraining Loss - 0.0059\n",
      "Epoch 4204:\tTraining Loss - 0.0065\n",
      "Epoch 4205:\tTraining Loss - 0.0115\n",
      "Epoch 4206:\tTraining Loss - 0.0070\n",
      "Epoch 4207:\tTraining Loss - 0.0031\n",
      "Epoch 4208:\tTraining Loss - 0.0050\n",
      "Epoch 4209:\tTraining Loss - 0.0055\n",
      "Epoch 4210:\tTraining Loss - 0.0061\n",
      "Epoch 4211:\tTraining Loss - 0.0058\n",
      "Epoch 4212:\tTraining Loss - 0.0045\n",
      "Epoch 4213:\tTraining Loss - 0.0060\n",
      "Epoch 4214:\tTraining Loss - 0.0047\n",
      "Epoch 4215:\tTraining Loss - 0.0035\n",
      "Epoch 4216:\tTraining Loss - 0.0075\n",
      "Epoch 4217:\tTraining Loss - 0.0054\n",
      "Epoch 4218:\tTraining Loss - 0.0075\n",
      "Epoch 4219:\tTraining Loss - 0.0069\n",
      "Epoch 4220:\tTraining Loss - 0.0117\n",
      "Epoch 4221:\tTraining Loss - 0.0062\n",
      "Epoch 4222:\tTraining Loss - 0.0044\n",
      "Epoch 4223:\tTraining Loss - 0.0059\n",
      "Epoch 4224:\tTraining Loss - 0.0055\n",
      "Epoch 4225:\tTraining Loss - 0.0067\n",
      "Epoch 4226:\tTraining Loss - 0.0048\n",
      "Epoch 4227:\tTraining Loss - 0.0043\n",
      "Epoch 4228:\tTraining Loss - 0.0091\n",
      "Epoch 4229:\tTraining Loss - 0.0038\n",
      "Epoch 4230:\tTraining Loss - 0.0065\n",
      "Epoch 4231:\tTraining Loss - 0.0044\n",
      "Epoch 4232:\tTraining Loss - 0.0068\n",
      "Epoch 4233:\tTraining Loss - 0.0044\n",
      "Epoch 4234:\tTraining Loss - 0.0054\n",
      "Epoch 4235:\tTraining Loss - 0.0058\n",
      "Epoch 4236:\tTraining Loss - 0.0041\n",
      "Epoch 4237:\tTraining Loss - 0.0034\n",
      "Epoch 4238:\tTraining Loss - 0.0085\n",
      "Epoch 4239:\tTraining Loss - 0.0029\n",
      "Epoch 4240:\tTraining Loss - 0.0063\n",
      "Epoch 4241:\tTraining Loss - 0.0077\n",
      "Epoch 4242:\tTraining Loss - 0.0048\n",
      "Epoch 4243:\tTraining Loss - 0.0065\n",
      "Epoch 4244:\tTraining Loss - 0.0033\n",
      "Epoch 4245:\tTraining Loss - 0.0057\n",
      "Epoch 4246:\tTraining Loss - 0.0057\n",
      "Epoch 4247:\tTraining Loss - 0.0064\n",
      "Epoch 4248:\tTraining Loss - 0.0057\n",
      "Epoch 4249:\tTraining Loss - 0.0052\n",
      "Epoch 4250:\tTraining Loss - 0.0053\n",
      "Epoch 4251:\tTraining Loss - 0.0066\n",
      "Epoch 4252:\tTraining Loss - 0.0096\n",
      "Epoch 4253:\tTraining Loss - 0.0084\n",
      "Epoch 4254:\tTraining Loss - 0.0059\n",
      "Epoch 4255:\tTraining Loss - 0.0027\n",
      "Epoch 4256:\tTraining Loss - 0.0063\n",
      "Epoch 4257:\tTraining Loss - 0.0045\n",
      "Epoch 4258:\tTraining Loss - 0.0045\n",
      "Epoch 4259:\tTraining Loss - 0.0041\n",
      "Epoch 4260:\tTraining Loss - 0.0051\n",
      "Epoch 4261:\tTraining Loss - 0.0060\n",
      "Epoch 4262:\tTraining Loss - 0.0109\n",
      "Epoch 4263:\tTraining Loss - 0.0056\n",
      "Epoch 4264:\tTraining Loss - 0.0055\n",
      "Epoch 4265:\tTraining Loss - 0.0047\n",
      "Epoch 4266:\tTraining Loss - 0.0032\n",
      "Epoch 4267:\tTraining Loss - 0.0055\n",
      "Epoch 4268:\tTraining Loss - 0.0040\n",
      "Epoch 4269:\tTraining Loss - 0.0054\n",
      "Epoch 4270:\tTraining Loss - 0.0071\n",
      "Epoch 4271:\tTraining Loss - 0.0061\n",
      "Epoch 4272:\tTraining Loss - 0.0063\n",
      "Epoch 4273:\tTraining Loss - 0.0083\n",
      "Epoch 4274:\tTraining Loss - 0.0049\n",
      "Epoch 4275:\tTraining Loss - 0.0040\n",
      "Epoch 4276:\tTraining Loss - 0.0034\n",
      "Epoch 4277:\tTraining Loss - 0.0073\n",
      "Epoch 4278:\tTraining Loss - 0.0045\n",
      "Epoch 4279:\tTraining Loss - 0.0073\n",
      "Epoch 4280:\tTraining Loss - 0.0102\n",
      "Epoch 4281:\tTraining Loss - 0.0058\n",
      "Epoch 4282:\tTraining Loss - 0.0042\n",
      "Epoch 4283:\tTraining Loss - 0.0056\n",
      "Epoch 4284:\tTraining Loss - 0.0023\n",
      "Epoch 4285:\tTraining Loss - 0.0056\n",
      "Epoch 4286:\tTraining Loss - 0.0031\n",
      "Epoch 4287:\tTraining Loss - 0.0050\n",
      "Epoch 4288:\tTraining Loss - 0.0044\n",
      "Epoch 4289:\tTraining Loss - 0.0085\n",
      "Epoch 4290:\tTraining Loss - 0.0058\n",
      "Epoch 4291:\tTraining Loss - 0.0040\n",
      "Epoch 4292:\tTraining Loss - 0.0063\n",
      "Epoch 4293:\tTraining Loss - 0.0082\n",
      "Epoch 4294:\tTraining Loss - 0.0061\n",
      "Epoch 4295:\tTraining Loss - 0.0021\n",
      "Epoch 4296:\tTraining Loss - 0.0040\n",
      "Epoch 4297:\tTraining Loss - 0.0063\n",
      "Epoch 4298:\tTraining Loss - 0.0053\n",
      "Epoch 4299:\tTraining Loss - 0.0061\n",
      "Epoch 4300:\tTraining Loss - 0.0069\n",
      "Epoch 4301:\tTraining Loss - 0.0041\n",
      "Epoch 4302:\tTraining Loss - 0.0067\n",
      "Epoch 4303:\tTraining Loss - 0.0081\n",
      "Epoch 4304:\tTraining Loss - 0.0055\n",
      "Epoch 4305:\tTraining Loss - 0.0069\n",
      "Epoch 4306:\tTraining Loss - 0.0054\n",
      "Epoch 4307:\tTraining Loss - 0.0038\n",
      "Epoch 4308:\tTraining Loss - 0.0042\n",
      "Epoch 4309:\tTraining Loss - 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4310:\tTraining Loss - 0.0051\n",
      "Epoch 4311:\tTraining Loss - 0.0046\n",
      "Epoch 4312:\tTraining Loss - 0.0064\n",
      "Epoch 4313:\tTraining Loss - 0.0060\n",
      "Epoch 4314:\tTraining Loss - 0.0051\n",
      "Epoch 4315:\tTraining Loss - 0.0042\n",
      "Epoch 4316:\tTraining Loss - 0.0036\n",
      "Epoch 4317:\tTraining Loss - 0.0036\n",
      "Epoch 4318:\tTraining Loss - 0.0031\n",
      "Epoch 4319:\tTraining Loss - 0.0032\n",
      "Epoch 4320:\tTraining Loss - 0.0055\n",
      "Epoch 4321:\tTraining Loss - 0.0039\n",
      "Epoch 4322:\tTraining Loss - 0.0066\n",
      "Epoch 4323:\tTraining Loss - 0.0031\n",
      "Epoch 4324:\tTraining Loss - 0.0038\n",
      "Epoch 4325:\tTraining Loss - 0.0034\n",
      "Epoch 4326:\tTraining Loss - 0.0074\n",
      "Epoch 4327:\tTraining Loss - 0.0061\n",
      "Epoch 4328:\tTraining Loss - 0.0104\n",
      "Epoch 4329:\tTraining Loss - 0.0079\n",
      "Epoch 4330:\tTraining Loss - 0.0068\n",
      "Epoch 4331:\tTraining Loss - 0.0036\n",
      "Epoch 4332:\tTraining Loss - 0.0040\n",
      "Epoch 4333:\tTraining Loss - 0.0057\n",
      "Epoch 4334:\tTraining Loss - 0.0065\n",
      "Epoch 4335:\tTraining Loss - 0.0041\n",
      "Epoch 4336:\tTraining Loss - 0.0041\n",
      "Epoch 4337:\tTraining Loss - 0.0031\n",
      "Epoch 4338:\tTraining Loss - 0.0036\n",
      "Epoch 4339:\tTraining Loss - 0.0039\n",
      "Epoch 4340:\tTraining Loss - 0.0058\n",
      "Epoch 4341:\tTraining Loss - 0.0087\n",
      "Epoch 4342:\tTraining Loss - 0.0048\n",
      "Epoch 4343:\tTraining Loss - 0.0045\n",
      "Epoch 4344:\tTraining Loss - 0.0051\n",
      "Epoch 4345:\tTraining Loss - 0.0029\n",
      "Epoch 4346:\tTraining Loss - 0.0028\n",
      "Epoch 4347:\tTraining Loss - 0.0085\n",
      "Epoch 4348:\tTraining Loss - 0.0037\n",
      "Epoch 4349:\tTraining Loss - 0.0032\n",
      "Epoch 4350:\tTraining Loss - 0.0062\n",
      "Epoch 4351:\tTraining Loss - 0.0031\n",
      "Epoch 4352:\tTraining Loss - 0.0057\n",
      "Epoch 4353:\tTraining Loss - 0.0081\n",
      "Epoch 4354:\tTraining Loss - 0.0034\n",
      "Epoch 4355:\tTraining Loss - 0.0028\n",
      "Epoch 4356:\tTraining Loss - 0.0051\n",
      "Epoch 4357:\tTraining Loss - 0.0096\n",
      "Epoch 4358:\tTraining Loss - 0.0047\n",
      "Epoch 4359:\tTraining Loss - 0.0044\n",
      "Epoch 4360:\tTraining Loss - 0.0065\n",
      "Epoch 4361:\tTraining Loss - 0.0066\n",
      "Epoch 4362:\tTraining Loss - 0.0093\n",
      "Epoch 4363:\tTraining Loss - 0.0072\n",
      "Epoch 4364:\tTraining Loss - 0.0095\n",
      "Epoch 4365:\tTraining Loss - 0.0032\n",
      "Epoch 4366:\tTraining Loss - 0.0119\n",
      "Epoch 4367:\tTraining Loss - 0.0041\n",
      "Epoch 4368:\tTraining Loss - 0.0039\n",
      "Epoch 4369:\tTraining Loss - 0.0050\n",
      "Epoch 4370:\tTraining Loss - 0.0058\n",
      "Epoch 4371:\tTraining Loss - 0.0029\n",
      "Epoch 4372:\tTraining Loss - 0.0064\n",
      "Epoch 4373:\tTraining Loss - 0.0046\n",
      "Epoch 4374:\tTraining Loss - 0.0057\n",
      "Epoch 4375:\tTraining Loss - 0.0050\n",
      "Epoch 4376:\tTraining Loss - 0.0069\n",
      "Epoch 4377:\tTraining Loss - 0.0138\n",
      "Epoch 4378:\tTraining Loss - 0.0050\n",
      "Epoch 4379:\tTraining Loss - 0.0034\n",
      "Epoch 4380:\tTraining Loss - 0.0066\n",
      "Epoch 4381:\tTraining Loss - 0.0071\n",
      "Epoch 4382:\tTraining Loss - 0.0054\n",
      "Epoch 4383:\tTraining Loss - 0.0068\n",
      "Epoch 4384:\tTraining Loss - 0.0075\n",
      "Epoch 4385:\tTraining Loss - 0.0059\n",
      "Epoch 4386:\tTraining Loss - 0.0034\n",
      "Epoch 4387:\tTraining Loss - 0.0064\n",
      "Epoch 4388:\tTraining Loss - 0.0093\n",
      "Epoch 4389:\tTraining Loss - 0.0040\n",
      "Epoch 4390:\tTraining Loss - 0.0065\n",
      "Epoch 4391:\tTraining Loss - 0.0041\n",
      "Epoch 4392:\tTraining Loss - 0.0061\n",
      "Epoch 4393:\tTraining Loss - 0.0058\n",
      "Epoch 4394:\tTraining Loss - 0.0066\n",
      "Epoch 4395:\tTraining Loss - 0.0092\n",
      "Epoch 4396:\tTraining Loss - 0.0066\n",
      "Epoch 4397:\tTraining Loss - 0.0062\n",
      "Epoch 4398:\tTraining Loss - 0.0056\n",
      "Epoch 4399:\tTraining Loss - 0.0045\n",
      "Epoch 4400:\tTraining Loss - 0.0071\n",
      "Epoch 4401:\tTraining Loss - 0.0059\n",
      "Epoch 4402:\tTraining Loss - 0.0056\n",
      "Epoch 4403:\tTraining Loss - 0.0035\n",
      "Epoch 4404:\tTraining Loss - 0.0061\n",
      "Epoch 4405:\tTraining Loss - 0.0077\n",
      "Epoch 4406:\tTraining Loss - 0.0056\n",
      "Epoch 4407:\tTraining Loss - 0.0043\n",
      "Epoch 4408:\tTraining Loss - 0.0077\n",
      "Epoch 4409:\tTraining Loss - 0.0065\n",
      "Epoch 4410:\tTraining Loss - 0.0086\n",
      "Epoch 4411:\tTraining Loss - 0.0052\n",
      "Epoch 4412:\tTraining Loss - 0.0052\n",
      "Epoch 4413:\tTraining Loss - 0.0034\n",
      "Epoch 4414:\tTraining Loss - 0.0059\n",
      "Epoch 4415:\tTraining Loss - 0.0066\n",
      "Epoch 4416:\tTraining Loss - 0.0063\n",
      "Epoch 4417:\tTraining Loss - 0.0069\n",
      "Epoch 4418:\tTraining Loss - 0.0057\n",
      "Epoch 4419:\tTraining Loss - 0.0049\n",
      "Epoch 4420:\tTraining Loss - 0.0081\n",
      "Epoch 4421:\tTraining Loss - 0.0049\n",
      "Epoch 4422:\tTraining Loss - 0.0049\n",
      "Epoch 4423:\tTraining Loss - 0.0054\n",
      "Epoch 4424:\tTraining Loss - 0.0036\n",
      "Epoch 4425:\tTraining Loss - 0.0039\n",
      "Epoch 4426:\tTraining Loss - 0.0048\n",
      "Epoch 4427:\tTraining Loss - 0.0070\n",
      "Epoch 4428:\tTraining Loss - 0.0081\n",
      "Epoch 4429:\tTraining Loss - 0.0045\n",
      "Epoch 4430:\tTraining Loss - 0.0086\n",
      "Epoch 4431:\tTraining Loss - 0.0085\n",
      "Epoch 4432:\tTraining Loss - 0.0060\n",
      "Epoch 4433:\tTraining Loss - 0.0064\n",
      "Epoch 4434:\tTraining Loss - 0.0032\n",
      "Epoch 4435:\tTraining Loss - 0.0089\n",
      "Epoch 4436:\tTraining Loss - 0.0078\n",
      "Epoch 4437:\tTraining Loss - 0.0069\n",
      "Epoch 4438:\tTraining Loss - 0.0041\n",
      "Epoch 4439:\tTraining Loss - 0.0042\n",
      "Epoch 4440:\tTraining Loss - 0.0050\n",
      "Epoch 4441:\tTraining Loss - 0.0036\n",
      "Epoch 4442:\tTraining Loss - 0.0047\n",
      "Epoch 4443:\tTraining Loss - 0.0117\n",
      "Epoch 4444:\tTraining Loss - 0.0048\n",
      "Epoch 4445:\tTraining Loss - 0.0041\n",
      "Epoch 4446:\tTraining Loss - 0.0029\n",
      "Epoch 4447:\tTraining Loss - 0.0040\n",
      "Epoch 4448:\tTraining Loss - 0.0052\n",
      "Epoch 4449:\tTraining Loss - 0.0086\n",
      "Epoch 4450:\tTraining Loss - 0.0026\n",
      "Epoch 4451:\tTraining Loss - 0.0069\n",
      "Epoch 4452:\tTraining Loss - 0.0064\n",
      "Epoch 4453:\tTraining Loss - 0.0065\n",
      "Epoch 4454:\tTraining Loss - 0.0054\n",
      "Epoch 4455:\tTraining Loss - 0.0068\n",
      "Epoch 4456:\tTraining Loss - 0.0058\n",
      "Epoch 4457:\tTraining Loss - 0.0033\n",
      "Epoch 4458:\tTraining Loss - 0.0059\n",
      "Epoch 4459:\tTraining Loss - 0.0041\n",
      "Epoch 4460:\tTraining Loss - 0.0061\n",
      "Epoch 4461:\tTraining Loss - 0.0063\n",
      "Epoch 4462:\tTraining Loss - 0.0068\n",
      "Epoch 4463:\tTraining Loss - 0.0044\n",
      "Epoch 4464:\tTraining Loss - 0.0034\n",
      "Epoch 4465:\tTraining Loss - 0.0047\n",
      "Epoch 4466:\tTraining Loss - 0.0042\n",
      "Epoch 4467:\tTraining Loss - 0.0056\n",
      "Epoch 4468:\tTraining Loss - 0.0028\n",
      "Epoch 4469:\tTraining Loss - 0.0026\n",
      "Epoch 4470:\tTraining Loss - 0.0057\n",
      "Epoch 4471:\tTraining Loss - 0.0024\n",
      "Epoch 4472:\tTraining Loss - 0.0057\n",
      "Epoch 4473:\tTraining Loss - 0.0098\n",
      "Epoch 4474:\tTraining Loss - 0.0067\n",
      "Epoch 4475:\tTraining Loss - 0.0100\n",
      "Epoch 4476:\tTraining Loss - 0.0066\n",
      "Epoch 4477:\tTraining Loss - 0.0057\n",
      "Epoch 4478:\tTraining Loss - 0.0040\n",
      "Epoch 4479:\tTraining Loss - 0.0079\n",
      "Epoch 4480:\tTraining Loss - 0.0066\n",
      "Epoch 4481:\tTraining Loss - 0.0061\n",
      "Epoch 4482:\tTraining Loss - 0.0052\n",
      "Epoch 4483:\tTraining Loss - 0.0076\n",
      "Epoch 4484:\tTraining Loss - 0.0046\n",
      "Epoch 4485:\tTraining Loss - 0.0050\n",
      "Epoch 4486:\tTraining Loss - 0.0051\n",
      "Epoch 4487:\tTraining Loss - 0.0042\n",
      "Epoch 4488:\tTraining Loss - 0.0072\n",
      "Epoch 4489:\tTraining Loss - 0.0053\n",
      "Epoch 4490:\tTraining Loss - 0.0061\n",
      "Epoch 4491:\tTraining Loss - 0.0038\n",
      "Epoch 4492:\tTraining Loss - 0.0055\n",
      "Epoch 4493:\tTraining Loss - 0.0129\n",
      "Epoch 4494:\tTraining Loss - 0.0055\n",
      "Epoch 4495:\tTraining Loss - 0.0073\n",
      "Epoch 4496:\tTraining Loss - 0.0046\n",
      "Epoch 4497:\tTraining Loss - 0.0022\n",
      "Epoch 4498:\tTraining Loss - 0.0057\n",
      "Epoch 4499:\tTraining Loss - 0.0066\n",
      "Epoch 4500:\tTraining Loss - 0.0092\n",
      "Epoch 4501:\tTraining Loss - 0.0073\n",
      "Epoch 4502:\tTraining Loss - 0.0036\n",
      "Epoch 4503:\tTraining Loss - 0.0084\n",
      "Epoch 4504:\tTraining Loss - 0.0101\n",
      "Epoch 4505:\tTraining Loss - 0.0066\n",
      "Epoch 4506:\tTraining Loss - 0.0076\n",
      "Epoch 4507:\tTraining Loss - 0.0044\n",
      "Epoch 4508:\tTraining Loss - 0.0056\n",
      "Epoch 4509:\tTraining Loss - 0.0034\n",
      "Epoch 4510:\tTraining Loss - 0.0051\n",
      "Epoch 4511:\tTraining Loss - 0.0037\n",
      "Epoch 4512:\tTraining Loss - 0.0052\n",
      "Epoch 4513:\tTraining Loss - 0.0032\n",
      "Epoch 4514:\tTraining Loss - 0.0095\n",
      "Epoch 4515:\tTraining Loss - 0.0053\n",
      "Epoch 4516:\tTraining Loss - 0.0058\n",
      "Epoch 4517:\tTraining Loss - 0.0041\n",
      "Epoch 4518:\tTraining Loss - 0.0078\n",
      "Epoch 4519:\tTraining Loss - 0.0110\n",
      "Epoch 4520:\tTraining Loss - 0.0039\n",
      "Epoch 4521:\tTraining Loss - 0.0097\n",
      "Epoch 4522:\tTraining Loss - 0.0044\n",
      "Epoch 4523:\tTraining Loss - 0.0075\n",
      "Epoch 4524:\tTraining Loss - 0.0070\n",
      "Epoch 4525:\tTraining Loss - 0.0033\n",
      "Epoch 4526:\tTraining Loss - 0.0125\n",
      "Epoch 4527:\tTraining Loss - 0.0054\n",
      "Epoch 4528:\tTraining Loss - 0.0067\n",
      "Epoch 4529:\tTraining Loss - 0.0065\n",
      "Epoch 4530:\tTraining Loss - 0.0067\n",
      "Epoch 4531:\tTraining Loss - 0.0045\n",
      "Epoch 4532:\tTraining Loss - 0.0051\n",
      "Epoch 4533:\tTraining Loss - 0.0047\n",
      "Epoch 4534:\tTraining Loss - 0.0046\n",
      "Epoch 4535:\tTraining Loss - 0.0072\n",
      "Epoch 4536:\tTraining Loss - 0.0062\n",
      "Epoch 4537:\tTraining Loss - 0.0040\n",
      "Epoch 4538:\tTraining Loss - 0.0030\n",
      "Epoch 4539:\tTraining Loss - 0.0039\n",
      "Epoch 4540:\tTraining Loss - 0.0047\n",
      "Epoch 4541:\tTraining Loss - 0.0072\n",
      "Epoch 4542:\tTraining Loss - 0.0051\n",
      "Epoch 4543:\tTraining Loss - 0.0031\n",
      "Epoch 4544:\tTraining Loss - 0.0075\n",
      "Epoch 4545:\tTraining Loss - 0.0049\n",
      "Epoch 4546:\tTraining Loss - 0.0064\n",
      "Epoch 4547:\tTraining Loss - 0.0077\n",
      "Epoch 4548:\tTraining Loss - 0.0043\n",
      "Epoch 4549:\tTraining Loss - 0.0033\n",
      "Epoch 4550:\tTraining Loss - 0.0049\n",
      "Epoch 4551:\tTraining Loss - 0.0056\n",
      "Epoch 4552:\tTraining Loss - 0.0036\n",
      "Epoch 4553:\tTraining Loss - 0.0066\n",
      "Epoch 4554:\tTraining Loss - 0.0066\n",
      "Epoch 4555:\tTraining Loss - 0.0067\n",
      "Epoch 4556:\tTraining Loss - 0.0021\n",
      "Epoch 4557:\tTraining Loss - 0.0058\n",
      "Epoch 4558:\tTraining Loss - 0.0065\n",
      "Epoch 4559:\tTraining Loss - 0.0095\n",
      "Epoch 4560:\tTraining Loss - 0.0055\n",
      "Epoch 4561:\tTraining Loss - 0.0092\n",
      "Epoch 4562:\tTraining Loss - 0.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4563:\tTraining Loss - 0.0022\n",
      "Epoch 4564:\tTraining Loss - 0.0058\n",
      "Epoch 4565:\tTraining Loss - 0.0046\n",
      "Epoch 4566:\tTraining Loss - 0.0055\n",
      "Epoch 4567:\tTraining Loss - 0.0049\n",
      "Epoch 4568:\tTraining Loss - 0.0057\n",
      "Epoch 4569:\tTraining Loss - 0.0055\n",
      "Epoch 4570:\tTraining Loss - 0.0063\n",
      "Epoch 4571:\tTraining Loss - 0.0027\n",
      "Epoch 4572:\tTraining Loss - 0.0056\n",
      "Epoch 4573:\tTraining Loss - 0.0059\n",
      "Epoch 4574:\tTraining Loss - 0.0043\n",
      "Epoch 4575:\tTraining Loss - 0.0034\n",
      "Epoch 4576:\tTraining Loss - 0.0061\n",
      "Epoch 4577:\tTraining Loss - 0.0028\n",
      "Epoch 4578:\tTraining Loss - 0.0074\n",
      "Epoch 4579:\tTraining Loss - 0.0066\n",
      "Epoch 4580:\tTraining Loss - 0.0039\n",
      "Epoch 4581:\tTraining Loss - 0.0047\n",
      "Epoch 4582:\tTraining Loss - 0.0083\n",
      "Epoch 4583:\tTraining Loss - 0.0065\n",
      "Epoch 4584:\tTraining Loss - 0.0065\n",
      "Epoch 4585:\tTraining Loss - 0.0066\n",
      "Epoch 4586:\tTraining Loss - 0.0042\n",
      "Epoch 4587:\tTraining Loss - 0.0068\n",
      "Epoch 4588:\tTraining Loss - 0.0047\n",
      "Epoch 4589:\tTraining Loss - 0.0053\n",
      "Epoch 4590:\tTraining Loss - 0.0057\n",
      "Epoch 4591:\tTraining Loss - 0.0044\n",
      "Epoch 4592:\tTraining Loss - 0.0053\n",
      "Epoch 4593:\tTraining Loss - 0.0066\n",
      "Epoch 4594:\tTraining Loss - 0.0058\n",
      "Epoch 4595:\tTraining Loss - 0.0034\n",
      "Epoch 4596:\tTraining Loss - 0.0056\n",
      "Epoch 4597:\tTraining Loss - 0.0061\n",
      "Epoch 4598:\tTraining Loss - 0.0088\n",
      "Epoch 4599:\tTraining Loss - 0.0080\n",
      "Epoch 4600:\tTraining Loss - 0.0063\n",
      "Epoch 4601:\tTraining Loss - 0.0060\n",
      "Epoch 4602:\tTraining Loss - 0.0044\n",
      "Epoch 4603:\tTraining Loss - 0.0061\n",
      "Epoch 4604:\tTraining Loss - 0.0059\n",
      "Epoch 4605:\tTraining Loss - 0.0046\n",
      "Epoch 4606:\tTraining Loss - 0.0033\n",
      "Epoch 4607:\tTraining Loss - 0.0033\n",
      "Epoch 4608:\tTraining Loss - 0.0053\n",
      "Epoch 4609:\tTraining Loss - 0.0046\n",
      "Epoch 4610:\tTraining Loss - 0.0054\n",
      "Epoch 4611:\tTraining Loss - 0.0054\n",
      "Epoch 4612:\tTraining Loss - 0.0052\n",
      "Epoch 4613:\tTraining Loss - 0.0068\n",
      "Epoch 4614:\tTraining Loss - 0.0029\n",
      "Epoch 4615:\tTraining Loss - 0.0072\n",
      "Epoch 4616:\tTraining Loss - 0.0061\n",
      "Epoch 4617:\tTraining Loss - 0.0075\n",
      "Epoch 4618:\tTraining Loss - 0.0046\n",
      "Epoch 4619:\tTraining Loss - 0.0038\n",
      "Epoch 4620:\tTraining Loss - 0.0053\n",
      "Epoch 4621:\tTraining Loss - 0.0051\n",
      "Epoch 4622:\tTraining Loss - 0.0078\n",
      "Epoch 4623:\tTraining Loss - 0.0059\n",
      "Epoch 4624:\tTraining Loss - 0.0078\n",
      "Epoch 4625:\tTraining Loss - 0.0025\n",
      "Epoch 4626:\tTraining Loss - 0.0028\n",
      "Epoch 4627:\tTraining Loss - 0.0061\n",
      "Epoch 4628:\tTraining Loss - 0.0029\n",
      "Epoch 4629:\tTraining Loss - 0.0054\n",
      "Epoch 4630:\tTraining Loss - 0.0040\n",
      "Epoch 4631:\tTraining Loss - 0.0071\n",
      "Epoch 4632:\tTraining Loss - 0.0035\n",
      "Epoch 4633:\tTraining Loss - 0.0057\n",
      "Epoch 4634:\tTraining Loss - 0.0096\n",
      "Epoch 4635:\tTraining Loss - 0.0031\n",
      "Epoch 4636:\tTraining Loss - 0.0090\n",
      "Epoch 4637:\tTraining Loss - 0.0043\n",
      "Epoch 4638:\tTraining Loss - 0.0037\n",
      "Epoch 4639:\tTraining Loss - 0.0064\n",
      "Epoch 4640:\tTraining Loss - 0.0059\n",
      "Epoch 4641:\tTraining Loss - 0.0047\n",
      "Epoch 4642:\tTraining Loss - 0.0059\n",
      "Epoch 4643:\tTraining Loss - 0.0067\n",
      "Epoch 4644:\tTraining Loss - 0.0072\n",
      "Epoch 4645:\tTraining Loss - 0.0033\n",
      "Epoch 4646:\tTraining Loss - 0.0086\n",
      "Epoch 4647:\tTraining Loss - 0.0057\n",
      "Epoch 4648:\tTraining Loss - 0.0068\n",
      "Epoch 4649:\tTraining Loss - 0.0061\n",
      "Epoch 4650:\tTraining Loss - 0.0031\n",
      "Epoch 4651:\tTraining Loss - 0.0054\n",
      "Epoch 4652:\tTraining Loss - 0.0053\n",
      "Epoch 4653:\tTraining Loss - 0.0027\n",
      "Epoch 4654:\tTraining Loss - 0.0059\n",
      "Epoch 4655:\tTraining Loss - 0.0057\n",
      "Epoch 4656:\tTraining Loss - 0.0051\n",
      "Epoch 4657:\tTraining Loss - 0.0050\n",
      "Epoch 4658:\tTraining Loss - 0.0059\n",
      "Epoch 4659:\tTraining Loss - 0.0039\n",
      "Epoch 4660:\tTraining Loss - 0.0064\n",
      "Epoch 4661:\tTraining Loss - 0.0041\n",
      "Epoch 4662:\tTraining Loss - 0.0058\n",
      "Epoch 4663:\tTraining Loss - 0.0066\n",
      "Epoch 4664:\tTraining Loss - 0.0047\n",
      "Epoch 4665:\tTraining Loss - 0.0031\n",
      "Epoch 4666:\tTraining Loss - 0.0035\n",
      "Epoch 4667:\tTraining Loss - 0.0043\n",
      "Epoch 4668:\tTraining Loss - 0.0087\n",
      "Epoch 4669:\tTraining Loss - 0.0062\n",
      "Epoch 4670:\tTraining Loss - 0.0053\n",
      "Epoch 4671:\tTraining Loss - 0.0056\n",
      "Epoch 4672:\tTraining Loss - 0.0052\n",
      "Epoch 4673:\tTraining Loss - 0.0052\n",
      "Epoch 4674:\tTraining Loss - 0.0037\n",
      "Epoch 4675:\tTraining Loss - 0.0073\n",
      "Epoch 4676:\tTraining Loss - 0.0072\n",
      "Epoch 4677:\tTraining Loss - 0.0048\n",
      "Epoch 4678:\tTraining Loss - 0.0048\n",
      "Epoch 4679:\tTraining Loss - 0.0039\n",
      "Epoch 4680:\tTraining Loss - 0.0039\n",
      "Epoch 4681:\tTraining Loss - 0.0045\n",
      "Epoch 4682:\tTraining Loss - 0.0036\n",
      "Epoch 4683:\tTraining Loss - 0.0036\n",
      "Epoch 4684:\tTraining Loss - 0.0043\n",
      "Epoch 4685:\tTraining Loss - 0.0044\n",
      "Epoch 4686:\tTraining Loss - 0.0031\n",
      "Epoch 4687:\tTraining Loss - 0.0044\n",
      "Epoch 4688:\tTraining Loss - 0.0079\n",
      "Epoch 4689:\tTraining Loss - 0.0066\n",
      "Epoch 4690:\tTraining Loss - 0.0091\n",
      "Epoch 4691:\tTraining Loss - 0.0024\n",
      "Epoch 4692:\tTraining Loss - 0.0055\n",
      "Epoch 4693:\tTraining Loss - 0.0036\n",
      "Epoch 4694:\tTraining Loss - 0.0043\n",
      "Epoch 4695:\tTraining Loss - 0.0062\n",
      "Epoch 4696:\tTraining Loss - 0.0080\n",
      "Epoch 4697:\tTraining Loss - 0.0047\n",
      "Epoch 4698:\tTraining Loss - 0.0026\n",
      "Epoch 4699:\tTraining Loss - 0.0050\n",
      "Epoch 4700:\tTraining Loss - 0.0064\n",
      "Epoch 4701:\tTraining Loss - 0.0054\n",
      "Epoch 4702:\tTraining Loss - 0.0115\n",
      "Epoch 4703:\tTraining Loss - 0.0064\n",
      "Epoch 4704:\tTraining Loss - 0.0073\n",
      "Epoch 4705:\tTraining Loss - 0.0064\n",
      "Epoch 4706:\tTraining Loss - 0.0096\n",
      "Epoch 4707:\tTraining Loss - 0.0046\n",
      "Epoch 4708:\tTraining Loss - 0.0035\n",
      "Epoch 4709:\tTraining Loss - 0.0062\n",
      "Epoch 4710:\tTraining Loss - 0.0057\n",
      "Epoch 4711:\tTraining Loss - 0.0031\n",
      "Epoch 4712:\tTraining Loss - 0.0088\n",
      "Epoch 4713:\tTraining Loss - 0.0027\n",
      "Epoch 4714:\tTraining Loss - 0.0057\n",
      "Epoch 4715:\tTraining Loss - 0.0072\n",
      "Epoch 4716:\tTraining Loss - 0.0063\n",
      "Epoch 4717:\tTraining Loss - 0.0055\n",
      "Epoch 4718:\tTraining Loss - 0.0038\n",
      "Epoch 4719:\tTraining Loss - 0.0070\n",
      "Epoch 4720:\tTraining Loss - 0.0061\n",
      "Epoch 4721:\tTraining Loss - 0.0071\n",
      "Epoch 4722:\tTraining Loss - 0.0053\n",
      "Epoch 4723:\tTraining Loss - 0.0069\n",
      "Epoch 4724:\tTraining Loss - 0.0032\n",
      "Epoch 4725:\tTraining Loss - 0.0043\n",
      "Epoch 4726:\tTraining Loss - 0.0050\n",
      "Epoch 4727:\tTraining Loss - 0.0046\n",
      "Epoch 4728:\tTraining Loss - 0.0062\n",
      "Epoch 4729:\tTraining Loss - 0.0047\n",
      "Epoch 4730:\tTraining Loss - 0.0045\n",
      "Epoch 4731:\tTraining Loss - 0.0036\n",
      "Epoch 4732:\tTraining Loss - 0.0083\n",
      "Epoch 4733:\tTraining Loss - 0.0087\n",
      "Epoch 4734:\tTraining Loss - 0.0050\n",
      "Epoch 4735:\tTraining Loss - 0.0058\n",
      "Epoch 4736:\tTraining Loss - 0.0031\n",
      "Epoch 4737:\tTraining Loss - 0.0054\n",
      "Epoch 4738:\tTraining Loss - 0.0105\n",
      "Epoch 4739:\tTraining Loss - 0.0051\n",
      "Epoch 4740:\tTraining Loss - 0.0039\n",
      "Epoch 4741:\tTraining Loss - 0.0091\n",
      "Epoch 4742:\tTraining Loss - 0.0066\n",
      "Epoch 4743:\tTraining Loss - 0.0087\n",
      "Epoch 4744:\tTraining Loss - 0.0067\n",
      "Epoch 4745:\tTraining Loss - 0.0047\n",
      "Epoch 4746:\tTraining Loss - 0.0056\n",
      "Epoch 4747:\tTraining Loss - 0.0077\n",
      "Epoch 4748:\tTraining Loss - 0.0043\n",
      "Epoch 4749:\tTraining Loss - 0.0059\n",
      "Epoch 4750:\tTraining Loss - 0.0042\n",
      "Epoch 4751:\tTraining Loss - 0.0031\n",
      "Epoch 4752:\tTraining Loss - 0.0047\n",
      "Epoch 4753:\tTraining Loss - 0.0056\n",
      "Epoch 4754:\tTraining Loss - 0.0058\n",
      "Epoch 4755:\tTraining Loss - 0.0057\n",
      "Epoch 4756:\tTraining Loss - 0.0059\n",
      "Epoch 4757:\tTraining Loss - 0.0049\n",
      "Epoch 4758:\tTraining Loss - 0.0023\n",
      "Epoch 4759:\tTraining Loss - 0.0059\n",
      "Epoch 4760:\tTraining Loss - 0.0041\n",
      "Epoch 4761:\tTraining Loss - 0.0071\n",
      "Epoch 4762:\tTraining Loss - 0.0054\n",
      "Epoch 4763:\tTraining Loss - 0.0035\n",
      "Epoch 4764:\tTraining Loss - 0.0083\n",
      "Epoch 4765:\tTraining Loss - 0.0050\n",
      "Epoch 4766:\tTraining Loss - 0.0064\n",
      "Epoch 4767:\tTraining Loss - 0.0051\n",
      "Epoch 4768:\tTraining Loss - 0.0029\n",
      "Epoch 4769:\tTraining Loss - 0.0040\n",
      "Epoch 4770:\tTraining Loss - 0.0038\n",
      "Epoch 4771:\tTraining Loss - 0.0054\n",
      "Epoch 4772:\tTraining Loss - 0.0057\n",
      "Epoch 4773:\tTraining Loss - 0.0072\n",
      "Epoch 4774:\tTraining Loss - 0.0037\n",
      "Epoch 4775:\tTraining Loss - 0.0034\n",
      "Epoch 4776:\tTraining Loss - 0.0051\n",
      "Epoch 4777:\tTraining Loss - 0.0050\n",
      "Epoch 4778:\tTraining Loss - 0.0058\n",
      "Epoch 4779:\tTraining Loss - 0.0040\n",
      "Epoch 4780:\tTraining Loss - 0.0062\n",
      "Epoch 4781:\tTraining Loss - 0.0036\n",
      "Epoch 4782:\tTraining Loss - 0.0031\n",
      "Epoch 4783:\tTraining Loss - 0.0065\n",
      "Epoch 4784:\tTraining Loss - 0.0049\n",
      "Epoch 4785:\tTraining Loss - 0.0071\n",
      "Epoch 4786:\tTraining Loss - 0.0039\n",
      "Epoch 4787:\tTraining Loss - 0.0051\n",
      "Epoch 4788:\tTraining Loss - 0.0059\n",
      "Epoch 4789:\tTraining Loss - 0.0054\n",
      "Epoch 4790:\tTraining Loss - 0.0037\n",
      "Epoch 4791:\tTraining Loss - 0.0040\n",
      "Epoch 4792:\tTraining Loss - 0.0042\n",
      "Epoch 4793:\tTraining Loss - 0.0065\n",
      "Epoch 4794:\tTraining Loss - 0.0041\n",
      "Epoch 4795:\tTraining Loss - 0.0031\n",
      "Epoch 4796:\tTraining Loss - 0.0056\n",
      "Epoch 4797:\tTraining Loss - 0.0046\n",
      "Epoch 4798:\tTraining Loss - 0.0046\n",
      "Epoch 4799:\tTraining Loss - 0.0036\n",
      "Epoch 4800:\tTraining Loss - 0.0054\n",
      "Epoch 4801:\tTraining Loss - 0.0039\n",
      "Epoch 4802:\tTraining Loss - 0.0035\n",
      "Epoch 4803:\tTraining Loss - 0.0054\n",
      "Epoch 4804:\tTraining Loss - 0.0104\n",
      "Epoch 4805:\tTraining Loss - 0.0090\n",
      "Epoch 4806:\tTraining Loss - 0.0035\n",
      "Epoch 4807:\tTraining Loss - 0.0056\n",
      "Epoch 4808:\tTraining Loss - 0.0058\n",
      "Epoch 4809:\tTraining Loss - 0.0103\n",
      "Epoch 4810:\tTraining Loss - 0.0051\n",
      "Epoch 4811:\tTraining Loss - 0.0042\n",
      "Epoch 4812:\tTraining Loss - 0.0054\n",
      "Epoch 4813:\tTraining Loss - 0.0043\n",
      "Epoch 4814:\tTraining Loss - 0.0085\n",
      "Epoch 4815:\tTraining Loss - 0.0045\n",
      "Epoch 4816:\tTraining Loss - 0.0048\n",
      "Epoch 4817:\tTraining Loss - 0.0053\n",
      "Epoch 4818:\tTraining Loss - 0.0040\n",
      "Epoch 4819:\tTraining Loss - 0.0059\n",
      "Epoch 4820:\tTraining Loss - 0.0038\n",
      "Epoch 4821:\tTraining Loss - 0.0067\n",
      "Epoch 4822:\tTraining Loss - 0.0057\n",
      "Epoch 4823:\tTraining Loss - 0.0045\n",
      "Epoch 4824:\tTraining Loss - 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4825:\tTraining Loss - 0.0069\n",
      "Epoch 4826:\tTraining Loss - 0.0063\n",
      "Epoch 4827:\tTraining Loss - 0.0033\n",
      "Epoch 4828:\tTraining Loss - 0.0038\n",
      "Epoch 4829:\tTraining Loss - 0.0054\n",
      "Epoch 4830:\tTraining Loss - 0.0043\n",
      "Epoch 4831:\tTraining Loss - 0.0037\n",
      "Epoch 4832:\tTraining Loss - 0.0036\n",
      "Epoch 4833:\tTraining Loss - 0.0099\n",
      "Epoch 4834:\tTraining Loss - 0.0043\n",
      "Epoch 4835:\tTraining Loss - 0.0052\n",
      "Epoch 4836:\tTraining Loss - 0.0092\n",
      "Epoch 4837:\tTraining Loss - 0.0032\n",
      "Epoch 4838:\tTraining Loss - 0.0062\n",
      "Epoch 4839:\tTraining Loss - 0.0036\n",
      "Epoch 4840:\tTraining Loss - 0.0040\n",
      "Epoch 4841:\tTraining Loss - 0.0048\n",
      "Epoch 4842:\tTraining Loss - 0.0037\n",
      "Epoch 4843:\tTraining Loss - 0.0041\n",
      "Epoch 4844:\tTraining Loss - 0.0054\n",
      "Epoch 4845:\tTraining Loss - 0.0062\n",
      "Epoch 4846:\tTraining Loss - 0.0048\n",
      "Epoch 4847:\tTraining Loss - 0.0084\n",
      "Epoch 4848:\tTraining Loss - 0.0068\n",
      "Epoch 4849:\tTraining Loss - 0.0039\n",
      "Epoch 4850:\tTraining Loss - 0.0037\n",
      "Epoch 4851:\tTraining Loss - 0.0044\n",
      "Epoch 4852:\tTraining Loss - 0.0050\n",
      "Epoch 4853:\tTraining Loss - 0.0077\n",
      "Epoch 4854:\tTraining Loss - 0.0077\n",
      "Epoch 4855:\tTraining Loss - 0.0034\n",
      "Epoch 4856:\tTraining Loss - 0.0053\n",
      "Epoch 4857:\tTraining Loss - 0.0033\n",
      "Epoch 4858:\tTraining Loss - 0.0043\n",
      "Epoch 4859:\tTraining Loss - 0.0029\n",
      "Epoch 4860:\tTraining Loss - 0.0036\n",
      "Epoch 4861:\tTraining Loss - 0.0042\n",
      "Epoch 4862:\tTraining Loss - 0.0044\n",
      "Epoch 4863:\tTraining Loss - 0.0073\n",
      "Epoch 4864:\tTraining Loss - 0.0049\n",
      "Epoch 4865:\tTraining Loss - 0.0036\n",
      "Epoch 4866:\tTraining Loss - 0.0051\n",
      "Epoch 4867:\tTraining Loss - 0.0064\n",
      "Epoch 4868:\tTraining Loss - 0.0024\n",
      "Epoch 4869:\tTraining Loss - 0.0068\n",
      "Epoch 4870:\tTraining Loss - 0.0040\n",
      "Epoch 4871:\tTraining Loss - 0.0065\n",
      "Epoch 4872:\tTraining Loss - 0.0039\n",
      "Epoch 4873:\tTraining Loss - 0.0018\n",
      "Epoch 4874:\tTraining Loss - 0.0053\n",
      "Epoch 4875:\tTraining Loss - 0.0099\n",
      "Epoch 4876:\tTraining Loss - 0.0070\n",
      "Epoch 4877:\tTraining Loss - 0.0020\n",
      "Epoch 4878:\tTraining Loss - 0.0088\n",
      "Epoch 4879:\tTraining Loss - 0.0036\n",
      "Epoch 4880:\tTraining Loss - 0.0077\n",
      "Epoch 4881:\tTraining Loss - 0.0038\n",
      "Epoch 4882:\tTraining Loss - 0.0072\n",
      "Epoch 4883:\tTraining Loss - 0.0038\n",
      "Epoch 4884:\tTraining Loss - 0.0022\n",
      "Epoch 4885:\tTraining Loss - 0.0040\n",
      "Epoch 4886:\tTraining Loss - 0.0090\n",
      "Epoch 4887:\tTraining Loss - 0.0037\n",
      "Epoch 4888:\tTraining Loss - 0.0055\n",
      "Epoch 4889:\tTraining Loss - 0.0046\n",
      "Epoch 4890:\tTraining Loss - 0.0084\n",
      "Epoch 4891:\tTraining Loss - 0.0095\n",
      "Epoch 4892:\tTraining Loss - 0.0074\n",
      "Epoch 4893:\tTraining Loss - 0.0064\n",
      "Epoch 4894:\tTraining Loss - 0.0057\n",
      "Epoch 4895:\tTraining Loss - 0.0061\n",
      "Epoch 4896:\tTraining Loss - 0.0047\n",
      "Epoch 4897:\tTraining Loss - 0.0022\n",
      "Epoch 4898:\tTraining Loss - 0.0065\n",
      "Epoch 4899:\tTraining Loss - 0.0053\n",
      "Epoch 4900:\tTraining Loss - 0.0037\n",
      "Epoch 4901:\tTraining Loss - 0.0021\n",
      "Epoch 4902:\tTraining Loss - 0.0066\n",
      "Epoch 4903:\tTraining Loss - 0.0045\n",
      "Epoch 4904:\tTraining Loss - 0.0101\n",
      "Epoch 4905:\tTraining Loss - 0.0042\n",
      "Epoch 4906:\tTraining Loss - 0.0052\n",
      "Epoch 4907:\tTraining Loss - 0.0033\n",
      "Epoch 4908:\tTraining Loss - 0.0030\n",
      "Epoch 4909:\tTraining Loss - 0.0039\n",
      "Epoch 4910:\tTraining Loss - 0.0023\n",
      "Epoch 4911:\tTraining Loss - 0.0105\n",
      "Epoch 4912:\tTraining Loss - 0.0061\n",
      "Epoch 4913:\tTraining Loss - 0.0030\n",
      "Epoch 4914:\tTraining Loss - 0.0056\n",
      "Epoch 4915:\tTraining Loss - 0.0048\n",
      "Epoch 4916:\tTraining Loss - 0.0106\n",
      "Epoch 4917:\tTraining Loss - 0.0037\n",
      "Epoch 4918:\tTraining Loss - 0.0064\n",
      "Epoch 4919:\tTraining Loss - 0.0042\n",
      "Epoch 4920:\tTraining Loss - 0.0041\n",
      "Epoch 4921:\tTraining Loss - 0.0047\n",
      "Epoch 4922:\tTraining Loss - 0.0049\n",
      "Epoch 4923:\tTraining Loss - 0.0060\n",
      "Epoch 4924:\tTraining Loss - 0.0050\n",
      "Epoch 4925:\tTraining Loss - 0.0038\n",
      "Epoch 4926:\tTraining Loss - 0.0049\n",
      "Epoch 4927:\tTraining Loss - 0.0030\n",
      "Epoch 4928:\tTraining Loss - 0.0068\n",
      "Epoch 4929:\tTraining Loss - 0.0057\n",
      "Epoch 4930:\tTraining Loss - 0.0051\n",
      "Epoch 4931:\tTraining Loss - 0.0084\n",
      "Epoch 4932:\tTraining Loss - 0.0023\n",
      "Epoch 4933:\tTraining Loss - 0.0021\n",
      "Epoch 4934:\tTraining Loss - 0.0088\n",
      "Epoch 4935:\tTraining Loss - 0.0021\n",
      "Epoch 4936:\tTraining Loss - 0.0049\n",
      "Epoch 4937:\tTraining Loss - 0.0070\n",
      "Epoch 4938:\tTraining Loss - 0.0073\n",
      "Epoch 4939:\tTraining Loss - 0.0028\n",
      "Epoch 4940:\tTraining Loss - 0.0055\n",
      "Epoch 4941:\tTraining Loss - 0.0040\n",
      "Epoch 4942:\tTraining Loss - 0.0062\n",
      "Epoch 4943:\tTraining Loss - 0.0037\n",
      "Epoch 4944:\tTraining Loss - 0.0058\n",
      "Epoch 4945:\tTraining Loss - 0.0044\n",
      "Epoch 4946:\tTraining Loss - 0.0056\n",
      "Epoch 4947:\tTraining Loss - 0.0030\n",
      "Epoch 4948:\tTraining Loss - 0.0054\n",
      "Epoch 4949:\tTraining Loss - 0.0059\n",
      "Epoch 4950:\tTraining Loss - 0.0047\n",
      "Epoch 4951:\tTraining Loss - 0.0032\n",
      "Epoch 4952:\tTraining Loss - 0.0051\n",
      "Epoch 4953:\tTraining Loss - 0.0082\n",
      "Epoch 4954:\tTraining Loss - 0.0080\n",
      "Epoch 4955:\tTraining Loss - 0.0082\n",
      "Epoch 4956:\tTraining Loss - 0.0056\n",
      "Epoch 4957:\tTraining Loss - 0.0083\n",
      "Epoch 4958:\tTraining Loss - 0.0081\n",
      "Epoch 4959:\tTraining Loss - 0.0074\n",
      "Epoch 4960:\tTraining Loss - 0.0052\n",
      "Epoch 4961:\tTraining Loss - 0.0039\n",
      "Epoch 4962:\tTraining Loss - 0.0041\n",
      "Epoch 4963:\tTraining Loss - 0.0043\n",
      "Epoch 4964:\tTraining Loss - 0.0074\n",
      "Epoch 4965:\tTraining Loss - 0.0037\n",
      "Epoch 4966:\tTraining Loss - 0.0031\n",
      "Epoch 4967:\tTraining Loss - 0.0037\n",
      "Epoch 4968:\tTraining Loss - 0.0038\n",
      "Epoch 4969:\tTraining Loss - 0.0048\n",
      "Epoch 4970:\tTraining Loss - 0.0045\n",
      "Epoch 4971:\tTraining Loss - 0.0027\n",
      "Epoch 4972:\tTraining Loss - 0.0065\n",
      "Epoch 4973:\tTraining Loss - 0.0036\n",
      "Epoch 4974:\tTraining Loss - 0.0058\n",
      "Epoch 4975:\tTraining Loss - 0.0067\n",
      "Epoch 4976:\tTraining Loss - 0.0090\n",
      "Epoch 4977:\tTraining Loss - 0.0055\n",
      "Epoch 4978:\tTraining Loss - 0.0066\n",
      "Epoch 4979:\tTraining Loss - 0.0056\n",
      "Epoch 4980:\tTraining Loss - 0.0035\n",
      "Epoch 4981:\tTraining Loss - 0.0035\n",
      "Epoch 4982:\tTraining Loss - 0.0059\n",
      "Epoch 4983:\tTraining Loss - 0.0053\n",
      "Epoch 4984:\tTraining Loss - 0.0035\n",
      "Epoch 4985:\tTraining Loss - 0.0074\n",
      "Epoch 4986:\tTraining Loss - 0.0058\n",
      "Epoch 4987:\tTraining Loss - 0.0066\n",
      "Epoch 4988:\tTraining Loss - 0.0074\n",
      "Epoch 4989:\tTraining Loss - 0.0062\n",
      "Epoch 4990:\tTraining Loss - 0.0100\n",
      "Epoch 4991:\tTraining Loss - 0.0053\n",
      "Epoch 4992:\tTraining Loss - 0.0071\n",
      "Epoch 4993:\tTraining Loss - 0.0038\n",
      "Epoch 4994:\tTraining Loss - 0.0027\n",
      "Epoch 4995:\tTraining Loss - 0.0046\n",
      "Epoch 4996:\tTraining Loss - 0.0064\n",
      "Epoch 4997:\tTraining Loss - 0.0042\n",
      "Epoch 4998:\tTraining Loss - 0.0050\n",
      "Epoch 4999:\tTraining Loss - 0.0030\n",
      "Epoch 5000:\tTraining Loss - 0.0093\n",
      "Epoch 5001:\tTraining Loss - 0.0136\n",
      "Epoch 5002:\tTraining Loss - 0.0024\n",
      "Epoch 5003:\tTraining Loss - 0.0052\n",
      "Epoch 5004:\tTraining Loss - 0.0058\n",
      "Epoch 5005:\tTraining Loss - 0.0065\n",
      "Epoch 5006:\tTraining Loss - 0.0054\n",
      "Epoch 5007:\tTraining Loss - 0.0048\n",
      "Epoch 5008:\tTraining Loss - 0.0040\n",
      "Epoch 5009:\tTraining Loss - 0.0050\n",
      "Epoch 5010:\tTraining Loss - 0.0047\n",
      "Epoch 5011:\tTraining Loss - 0.0059\n",
      "Epoch 5012:\tTraining Loss - 0.0073\n",
      "Epoch 5013:\tTraining Loss - 0.0024\n",
      "Epoch 5014:\tTraining Loss - 0.0046\n",
      "Epoch 5015:\tTraining Loss - 0.0033\n",
      "Epoch 5016:\tTraining Loss - 0.0049\n",
      "Epoch 5017:\tTraining Loss - 0.0065\n",
      "Epoch 5018:\tTraining Loss - 0.0041\n",
      "Epoch 5019:\tTraining Loss - 0.0037\n",
      "Epoch 5020:\tTraining Loss - 0.0030\n",
      "Epoch 5021:\tTraining Loss - 0.0031\n",
      "Epoch 5022:\tTraining Loss - 0.0052\n",
      "Epoch 5023:\tTraining Loss - 0.0096\n",
      "Epoch 5024:\tTraining Loss - 0.0066\n",
      "Epoch 5025:\tTraining Loss - 0.0043\n",
      "Epoch 5026:\tTraining Loss - 0.0038\n",
      "Epoch 5027:\tTraining Loss - 0.0075\n",
      "Epoch 5028:\tTraining Loss - 0.0108\n",
      "Epoch 5029:\tTraining Loss - 0.0053\n",
      "Epoch 5030:\tTraining Loss - 0.0050\n",
      "Epoch 5031:\tTraining Loss - 0.0075\n",
      "Epoch 5032:\tTraining Loss - 0.0030\n",
      "Epoch 5033:\tTraining Loss - 0.0088\n",
      "Epoch 5034:\tTraining Loss - 0.0034\n",
      "Epoch 5035:\tTraining Loss - 0.0029\n",
      "Epoch 5036:\tTraining Loss - 0.0113\n",
      "Epoch 5037:\tTraining Loss - 0.0051\n",
      "Epoch 5038:\tTraining Loss - 0.0039\n",
      "Epoch 5039:\tTraining Loss - 0.0038\n",
      "Epoch 5040:\tTraining Loss - 0.0072\n",
      "Epoch 5041:\tTraining Loss - 0.0051\n",
      "Epoch 5042:\tTraining Loss - 0.0046\n",
      "Epoch 5043:\tTraining Loss - 0.0040\n",
      "Epoch 5044:\tTraining Loss - 0.0057\n",
      "Epoch 5045:\tTraining Loss - 0.0065\n",
      "Epoch 5046:\tTraining Loss - 0.0049\n",
      "Epoch 5047:\tTraining Loss - 0.0065\n",
      "Epoch 5048:\tTraining Loss - 0.0098\n",
      "Epoch 5049:\tTraining Loss - 0.0054\n",
      "Epoch 5050:\tTraining Loss - 0.0077\n",
      "Epoch 5051:\tTraining Loss - 0.0054\n",
      "Epoch 5052:\tTraining Loss - 0.0024\n",
      "Epoch 5053:\tTraining Loss - 0.0052\n",
      "Epoch 5054:\tTraining Loss - 0.0089\n",
      "Epoch 5055:\tTraining Loss - 0.0060\n",
      "Epoch 5056:\tTraining Loss - 0.0075\n",
      "Epoch 5057:\tTraining Loss - 0.0069\n",
      "Epoch 5058:\tTraining Loss - 0.0021\n",
      "Epoch 5059:\tTraining Loss - 0.0027\n",
      "Epoch 5060:\tTraining Loss - 0.0037\n",
      "Epoch 5061:\tTraining Loss - 0.0028\n",
      "Epoch 5062:\tTraining Loss - 0.0041\n",
      "Epoch 5063:\tTraining Loss - 0.0033\n",
      "Epoch 5064:\tTraining Loss - 0.0021\n",
      "Epoch 5065:\tTraining Loss - 0.0041\n",
      "Epoch 5066:\tTraining Loss - 0.0025\n",
      "Epoch 5067:\tTraining Loss - 0.0044\n",
      "Epoch 5068:\tTraining Loss - 0.0056\n",
      "Epoch 5069:\tTraining Loss - 0.0081\n",
      "Epoch 5070:\tTraining Loss - 0.0076\n",
      "Epoch 5071:\tTraining Loss - 0.0075\n",
      "Epoch 5072:\tTraining Loss - 0.0063\n",
      "Epoch 5073:\tTraining Loss - 0.0047\n",
      "Epoch 5074:\tTraining Loss - 0.0034\n",
      "Epoch 5075:\tTraining Loss - 0.0035\n",
      "Epoch 5076:\tTraining Loss - 0.0042\n",
      "Epoch 5077:\tTraining Loss - 0.0075\n",
      "Epoch 5078:\tTraining Loss - 0.0046\n",
      "Epoch 5079:\tTraining Loss - 0.0046\n",
      "Epoch 5080:\tTraining Loss - 0.0065\n",
      "Epoch 5081:\tTraining Loss - 0.0066\n",
      "Epoch 5082:\tTraining Loss - 0.0055\n",
      "Epoch 5083:\tTraining Loss - 0.0046\n",
      "Epoch 5084:\tTraining Loss - 0.0070\n",
      "Epoch 5085:\tTraining Loss - 0.0087\n",
      "Epoch 5086:\tTraining Loss - 0.0083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5087:\tTraining Loss - 0.0061\n",
      "Epoch 5088:\tTraining Loss - 0.0034\n",
      "Epoch 5089:\tTraining Loss - 0.0055\n",
      "Epoch 5090:\tTraining Loss - 0.0057\n",
      "Epoch 5091:\tTraining Loss - 0.0063\n",
      "Epoch 5092:\tTraining Loss - 0.0061\n",
      "Epoch 5093:\tTraining Loss - 0.0024\n",
      "Epoch 5094:\tTraining Loss - 0.0047\n",
      "Epoch 5095:\tTraining Loss - 0.0079\n",
      "Epoch 5096:\tTraining Loss - 0.0028\n",
      "Epoch 5097:\tTraining Loss - 0.0054\n",
      "Epoch 5098:\tTraining Loss - 0.0070\n",
      "Epoch 5099:\tTraining Loss - 0.0044\n",
      "Epoch 5100:\tTraining Loss - 0.0068\n",
      "Epoch 5101:\tTraining Loss - 0.0052\n",
      "Epoch 5102:\tTraining Loss - 0.0032\n",
      "Epoch 5103:\tTraining Loss - 0.0068\n",
      "Epoch 5104:\tTraining Loss - 0.0057\n",
      "Epoch 5105:\tTraining Loss - 0.0086\n",
      "Epoch 5106:\tTraining Loss - 0.0027\n",
      "Epoch 5107:\tTraining Loss - 0.0056\n",
      "Epoch 5108:\tTraining Loss - 0.0060\n",
      "Epoch 5109:\tTraining Loss - 0.0067\n",
      "Epoch 5110:\tTraining Loss - 0.0083\n",
      "Epoch 5111:\tTraining Loss - 0.0031\n",
      "Epoch 5112:\tTraining Loss - 0.0051\n",
      "Epoch 5113:\tTraining Loss - 0.0066\n",
      "Epoch 5114:\tTraining Loss - 0.0072\n",
      "Epoch 5115:\tTraining Loss - 0.0059\n",
      "Epoch 5116:\tTraining Loss - 0.0076\n",
      "Epoch 5117:\tTraining Loss - 0.0052\n",
      "Epoch 5118:\tTraining Loss - 0.0038\n",
      "Epoch 5119:\tTraining Loss - 0.0090\n",
      "Epoch 5120:\tTraining Loss - 0.0032\n",
      "Epoch 5121:\tTraining Loss - 0.0069\n",
      "Epoch 5122:\tTraining Loss - 0.0058\n",
      "Epoch 5123:\tTraining Loss - 0.0103\n",
      "Epoch 5124:\tTraining Loss - 0.0072\n",
      "Epoch 5125:\tTraining Loss - 0.0071\n",
      "Epoch 5126:\tTraining Loss - 0.0043\n",
      "Epoch 5127:\tTraining Loss - 0.0063\n",
      "Epoch 5128:\tTraining Loss - 0.0069\n",
      "Epoch 5129:\tTraining Loss - 0.0030\n",
      "Epoch 5130:\tTraining Loss - 0.0042\n",
      "Epoch 5131:\tTraining Loss - 0.0056\n",
      "Epoch 5132:\tTraining Loss - 0.0033\n",
      "Epoch 5133:\tTraining Loss - 0.0059\n",
      "Epoch 5134:\tTraining Loss - 0.0043\n",
      "Epoch 5135:\tTraining Loss - 0.0044\n",
      "Epoch 5136:\tTraining Loss - 0.0052\n",
      "Epoch 5137:\tTraining Loss - 0.0082\n",
      "Epoch 5138:\tTraining Loss - 0.0058\n",
      "Epoch 5139:\tTraining Loss - 0.0039\n",
      "Epoch 5140:\tTraining Loss - 0.0042\n",
      "Epoch 5141:\tTraining Loss - 0.0079\n",
      "Epoch 5142:\tTraining Loss - 0.0083\n",
      "Epoch 5143:\tTraining Loss - 0.0030\n",
      "Epoch 5144:\tTraining Loss - 0.0082\n",
      "Epoch 5145:\tTraining Loss - 0.0035\n",
      "Epoch 5146:\tTraining Loss - 0.0035\n",
      "Epoch 5147:\tTraining Loss - 0.0051\n",
      "Epoch 5148:\tTraining Loss - 0.0054\n",
      "Epoch 5149:\tTraining Loss - 0.0062\n",
      "Epoch 5150:\tTraining Loss - 0.0090\n",
      "Epoch 5151:\tTraining Loss - 0.0040\n",
      "Epoch 5152:\tTraining Loss - 0.0130\n",
      "Epoch 5153:\tTraining Loss - 0.0130\n",
      "Epoch 5154:\tTraining Loss - 0.0035\n",
      "Epoch 5155:\tTraining Loss - 0.0027\n",
      "Epoch 5156:\tTraining Loss - 0.0044\n",
      "Epoch 5157:\tTraining Loss - 0.0052\n",
      "Epoch 5158:\tTraining Loss - 0.0066\n",
      "Epoch 5159:\tTraining Loss - 0.0071\n",
      "Epoch 5160:\tTraining Loss - 0.0073\n",
      "Epoch 5161:\tTraining Loss - 0.0040\n",
      "Epoch 5162:\tTraining Loss - 0.0047\n",
      "Epoch 5163:\tTraining Loss - 0.0031\n",
      "Epoch 5164:\tTraining Loss - 0.0063\n",
      "Epoch 5165:\tTraining Loss - 0.0032\n",
      "Epoch 5166:\tTraining Loss - 0.0063\n",
      "Epoch 5167:\tTraining Loss - 0.0034\n",
      "Epoch 5168:\tTraining Loss - 0.0061\n",
      "Epoch 5169:\tTraining Loss - 0.0040\n",
      "Epoch 5170:\tTraining Loss - 0.0040\n",
      "Epoch 5171:\tTraining Loss - 0.0030\n",
      "Epoch 5172:\tTraining Loss - 0.0083\n",
      "Epoch 5173:\tTraining Loss - 0.0043\n",
      "Epoch 5174:\tTraining Loss - 0.0044\n",
      "Epoch 5175:\tTraining Loss - 0.0026\n",
      "Epoch 5176:\tTraining Loss - 0.0025\n",
      "Epoch 5177:\tTraining Loss - 0.0049\n",
      "Epoch 5178:\tTraining Loss - 0.0062\n",
      "Epoch 5179:\tTraining Loss - 0.0027\n",
      "Epoch 5180:\tTraining Loss - 0.0061\n",
      "Epoch 5181:\tTraining Loss - 0.0029\n",
      "Epoch 5182:\tTraining Loss - 0.0041\n",
      "Epoch 5183:\tTraining Loss - 0.0030\n",
      "Epoch 5184:\tTraining Loss - 0.0070\n",
      "Epoch 5185:\tTraining Loss - 0.0065\n",
      "Epoch 5186:\tTraining Loss - 0.0053\n",
      "Epoch 5187:\tTraining Loss - 0.0039\n",
      "Epoch 5188:\tTraining Loss - 0.0052\n",
      "Epoch 5189:\tTraining Loss - 0.0055\n",
      "Epoch 5190:\tTraining Loss - 0.0025\n",
      "Epoch 5191:\tTraining Loss - 0.0047\n",
      "Epoch 5192:\tTraining Loss - 0.0065\n",
      "Epoch 5193:\tTraining Loss - 0.0072\n",
      "Epoch 5194:\tTraining Loss - 0.0076\n",
      "Epoch 5195:\tTraining Loss - 0.0045\n",
      "Epoch 5196:\tTraining Loss - 0.0029\n",
      "Epoch 5197:\tTraining Loss - 0.0028\n",
      "Epoch 5198:\tTraining Loss - 0.0075\n",
      "Epoch 5199:\tTraining Loss - 0.0043\n",
      "Epoch 5200:\tTraining Loss - 0.0081\n",
      "Epoch 5201:\tTraining Loss - 0.0057\n",
      "Epoch 5202:\tTraining Loss - 0.0067\n",
      "Epoch 5203:\tTraining Loss - 0.0074\n",
      "Epoch 5204:\tTraining Loss - 0.0041\n",
      "Epoch 5205:\tTraining Loss - 0.0065\n",
      "Epoch 5206:\tTraining Loss - 0.0029\n",
      "Epoch 5207:\tTraining Loss - 0.0024\n",
      "Epoch 5208:\tTraining Loss - 0.0053\n",
      "Epoch 5209:\tTraining Loss - 0.0055\n",
      "Epoch 5210:\tTraining Loss - 0.0065\n",
      "Epoch 5211:\tTraining Loss - 0.0048\n",
      "Epoch 5212:\tTraining Loss - 0.0073\n",
      "Epoch 5213:\tTraining Loss - 0.0069\n",
      "Epoch 5214:\tTraining Loss - 0.0034\n",
      "Epoch 5215:\tTraining Loss - 0.0107\n",
      "Epoch 5216:\tTraining Loss - 0.0031\n",
      "Epoch 5217:\tTraining Loss - 0.0029\n",
      "Epoch 5218:\tTraining Loss - 0.0026\n",
      "Epoch 5219:\tTraining Loss - 0.0030\n",
      "Epoch 5220:\tTraining Loss - 0.0041\n",
      "Epoch 5221:\tTraining Loss - 0.0023\n",
      "Epoch 5222:\tTraining Loss - 0.0047\n",
      "Epoch 5223:\tTraining Loss - 0.0031\n",
      "Epoch 5224:\tTraining Loss - 0.0057\n",
      "Epoch 5225:\tTraining Loss - 0.0029\n",
      "Epoch 5226:\tTraining Loss - 0.0032\n",
      "Epoch 5227:\tTraining Loss - 0.0036\n",
      "Epoch 5228:\tTraining Loss - 0.0064\n",
      "Epoch 5229:\tTraining Loss - 0.0059\n",
      "Epoch 5230:\tTraining Loss - 0.0058\n",
      "Epoch 5231:\tTraining Loss - 0.0046\n",
      "Epoch 5232:\tTraining Loss - 0.0019\n",
      "Epoch 5233:\tTraining Loss - 0.0061\n",
      "Epoch 5234:\tTraining Loss - 0.0055\n",
      "Epoch 5235:\tTraining Loss - 0.0057\n",
      "Epoch 5236:\tTraining Loss - 0.0054\n",
      "Epoch 5237:\tTraining Loss - 0.0045\n",
      "Epoch 5238:\tTraining Loss - 0.0023\n",
      "Epoch 5239:\tTraining Loss - 0.0040\n",
      "Epoch 5240:\tTraining Loss - 0.0058\n",
      "Epoch 5241:\tTraining Loss - 0.0055\n",
      "Epoch 5242:\tTraining Loss - 0.0035\n",
      "Epoch 5243:\tTraining Loss - 0.0064\n",
      "Epoch 5244:\tTraining Loss - 0.0089\n",
      "Epoch 5245:\tTraining Loss - 0.0041\n",
      "Epoch 5246:\tTraining Loss - 0.0018\n",
      "Epoch 5247:\tTraining Loss - 0.0050\n",
      "Epoch 5248:\tTraining Loss - 0.0034\n",
      "Epoch 5249:\tTraining Loss - 0.0064\n",
      "Epoch 5250:\tTraining Loss - 0.0065\n",
      "Epoch 5251:\tTraining Loss - 0.0060\n",
      "Epoch 5252:\tTraining Loss - 0.0063\n",
      "Epoch 5253:\tTraining Loss - 0.0032\n",
      "Epoch 5254:\tTraining Loss - 0.0038\n",
      "Epoch 5255:\tTraining Loss - 0.0089\n",
      "Epoch 5256:\tTraining Loss - 0.0079\n",
      "Epoch 5257:\tTraining Loss - 0.0055\n",
      "Epoch 5258:\tTraining Loss - 0.0061\n",
      "Epoch 5259:\tTraining Loss - 0.0043\n",
      "Epoch 5260:\tTraining Loss - 0.0037\n",
      "Epoch 5261:\tTraining Loss - 0.0063\n",
      "Epoch 5262:\tTraining Loss - 0.0057\n",
      "Epoch 5263:\tTraining Loss - 0.0081\n",
      "Epoch 5264:\tTraining Loss - 0.0093\n",
      "Epoch 5265:\tTraining Loss - 0.0073\n",
      "Epoch 5266:\tTraining Loss - 0.0029\n",
      "Epoch 5267:\tTraining Loss - 0.0026\n",
      "Epoch 5268:\tTraining Loss - 0.0035\n",
      "Epoch 5269:\tTraining Loss - 0.0019\n",
      "Epoch 5270:\tTraining Loss - 0.0094\n",
      "Epoch 5271:\tTraining Loss - 0.0033\n",
      "Epoch 5272:\tTraining Loss - 0.0023\n",
      "Epoch 5273:\tTraining Loss - 0.0124\n",
      "Epoch 5274:\tTraining Loss - 0.0046\n",
      "Epoch 5275:\tTraining Loss - 0.0052\n",
      "Epoch 5276:\tTraining Loss - 0.0062\n",
      "Epoch 5277:\tTraining Loss - 0.0068\n",
      "Epoch 5278:\tTraining Loss - 0.0041\n",
      "Epoch 5279:\tTraining Loss - 0.0037\n",
      "Epoch 5280:\tTraining Loss - 0.0034\n",
      "Epoch 5281:\tTraining Loss - 0.0055\n",
      "Epoch 5282:\tTraining Loss - 0.0044\n",
      "Epoch 5283:\tTraining Loss - 0.0097\n",
      "Epoch 5284:\tTraining Loss - 0.0081\n",
      "Epoch 5285:\tTraining Loss - 0.0055\n",
      "Epoch 5286:\tTraining Loss - 0.0052\n",
      "Epoch 5287:\tTraining Loss - 0.0028\n",
      "Epoch 5288:\tTraining Loss - 0.0025\n",
      "Epoch 5289:\tTraining Loss - 0.0057\n",
      "Epoch 5290:\tTraining Loss - 0.0078\n",
      "Epoch 5291:\tTraining Loss - 0.0023\n",
      "Epoch 5292:\tTraining Loss - 0.0048\n",
      "Epoch 5293:\tTraining Loss - 0.0051\n",
      "Epoch 5294:\tTraining Loss - 0.0037\n",
      "Epoch 5295:\tTraining Loss - 0.0045\n",
      "Epoch 5296:\tTraining Loss - 0.0037\n",
      "Epoch 5297:\tTraining Loss - 0.0090\n",
      "Epoch 5298:\tTraining Loss - 0.0062\n",
      "Epoch 5299:\tTraining Loss - 0.0033\n",
      "Epoch 5300:\tTraining Loss - 0.0034\n",
      "Epoch 5301:\tTraining Loss - 0.0051\n",
      "Epoch 5302:\tTraining Loss - 0.0054\n",
      "Epoch 5303:\tTraining Loss - 0.0031\n",
      "Epoch 5304:\tTraining Loss - 0.0053\n",
      "Epoch 5305:\tTraining Loss - 0.0030\n",
      "Epoch 5306:\tTraining Loss - 0.0040\n",
      "Epoch 5307:\tTraining Loss - 0.0046\n",
      "Epoch 5308:\tTraining Loss - 0.0051\n",
      "Epoch 5309:\tTraining Loss - 0.0020\n",
      "Epoch 5310:\tTraining Loss - 0.0056\n",
      "Epoch 5311:\tTraining Loss - 0.0046\n",
      "Epoch 5312:\tTraining Loss - 0.0039\n",
      "Epoch 5313:\tTraining Loss - 0.0037\n",
      "Epoch 5314:\tTraining Loss - 0.0041\n",
      "Epoch 5315:\tTraining Loss - 0.0042\n",
      "Epoch 5316:\tTraining Loss - 0.0079\n",
      "Epoch 5317:\tTraining Loss - 0.0043\n",
      "Epoch 5318:\tTraining Loss - 0.0093\n",
      "Epoch 5319:\tTraining Loss - 0.0022\n",
      "Epoch 5320:\tTraining Loss - 0.0035\n",
      "Epoch 5321:\tTraining Loss - 0.0061\n",
      "Epoch 5322:\tTraining Loss - 0.0075\n",
      "Epoch 5323:\tTraining Loss - 0.0052\n",
      "Epoch 5324:\tTraining Loss - 0.0045\n",
      "Epoch 5325:\tTraining Loss - 0.0027\n",
      "Epoch 5326:\tTraining Loss - 0.0030\n",
      "Epoch 5327:\tTraining Loss - 0.0079\n",
      "Epoch 5328:\tTraining Loss - 0.0044\n",
      "Epoch 5329:\tTraining Loss - 0.0029\n",
      "Epoch 5330:\tTraining Loss - 0.0046\n",
      "Epoch 5331:\tTraining Loss - 0.0032\n",
      "Epoch 5332:\tTraining Loss - 0.0053\n",
      "Epoch 5333:\tTraining Loss - 0.0047\n",
      "Epoch 5334:\tTraining Loss - 0.0066\n",
      "Epoch 5335:\tTraining Loss - 0.0026\n",
      "Epoch 5336:\tTraining Loss - 0.0059\n",
      "Epoch 5337:\tTraining Loss - 0.0068\n",
      "Epoch 5338:\tTraining Loss - 0.0073\n",
      "Epoch 5339:\tTraining Loss - 0.0049\n",
      "Epoch 5340:\tTraining Loss - 0.0059\n",
      "Epoch 5341:\tTraining Loss - 0.0022\n",
      "Epoch 5342:\tTraining Loss - 0.0055\n",
      "Epoch 5343:\tTraining Loss - 0.0035\n",
      "Epoch 5344:\tTraining Loss - 0.0109\n",
      "Epoch 5345:\tTraining Loss - 0.0031\n",
      "Epoch 5346:\tTraining Loss - 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5347:\tTraining Loss - 0.0059\n",
      "Epoch 5348:\tTraining Loss - 0.0034\n",
      "Epoch 5349:\tTraining Loss - 0.0072\n",
      "Epoch 5350:\tTraining Loss - 0.0068\n",
      "Epoch 5351:\tTraining Loss - 0.0024\n",
      "Epoch 5352:\tTraining Loss - 0.0053\n",
      "Epoch 5353:\tTraining Loss - 0.0071\n",
      "Epoch 5354:\tTraining Loss - 0.0019\n",
      "Epoch 5355:\tTraining Loss - 0.0032\n",
      "Epoch 5356:\tTraining Loss - 0.0027\n",
      "Epoch 5357:\tTraining Loss - 0.0040\n",
      "Epoch 5358:\tTraining Loss - 0.0024\n",
      "Epoch 5359:\tTraining Loss - 0.0042\n",
      "Epoch 5360:\tTraining Loss - 0.0061\n",
      "Epoch 5361:\tTraining Loss - 0.0069\n",
      "Epoch 5362:\tTraining Loss - 0.0039\n",
      "Epoch 5363:\tTraining Loss - 0.0068\n",
      "Epoch 5364:\tTraining Loss - 0.0066\n",
      "Epoch 5365:\tTraining Loss - 0.0022\n",
      "Epoch 5366:\tTraining Loss - 0.0032\n",
      "Epoch 5367:\tTraining Loss - 0.0061\n",
      "Epoch 5368:\tTraining Loss - 0.0068\n",
      "Epoch 5369:\tTraining Loss - 0.0083\n",
      "Epoch 5370:\tTraining Loss - 0.0117\n",
      "Epoch 5371:\tTraining Loss - 0.0034\n",
      "Epoch 5372:\tTraining Loss - 0.0094\n",
      "Epoch 5373:\tTraining Loss - 0.0040\n",
      "Epoch 5374:\tTraining Loss - 0.0048\n",
      "Epoch 5375:\tTraining Loss - 0.0033\n",
      "Epoch 5376:\tTraining Loss - 0.0096\n",
      "Epoch 5377:\tTraining Loss - 0.0051\n",
      "Epoch 5378:\tTraining Loss - 0.0064\n",
      "Epoch 5379:\tTraining Loss - 0.0037\n",
      "Epoch 5380:\tTraining Loss - 0.0034\n",
      "Epoch 5381:\tTraining Loss - 0.0102\n",
      "Epoch 5382:\tTraining Loss - 0.0036\n",
      "Epoch 5383:\tTraining Loss - 0.0060\n",
      "Epoch 5384:\tTraining Loss - 0.0083\n",
      "Epoch 5385:\tTraining Loss - 0.0097\n",
      "Epoch 5386:\tTraining Loss - 0.0083\n",
      "Epoch 5387:\tTraining Loss - 0.0071\n",
      "Epoch 5388:\tTraining Loss - 0.0030\n",
      "Epoch 5389:\tTraining Loss - 0.0052\n",
      "Epoch 5390:\tTraining Loss - 0.0070\n",
      "Epoch 5391:\tTraining Loss - 0.0039\n",
      "Epoch 5392:\tTraining Loss - 0.0030\n",
      "Epoch 5393:\tTraining Loss - 0.0032\n",
      "Epoch 5394:\tTraining Loss - 0.0053\n",
      "Epoch 5395:\tTraining Loss - 0.0092\n",
      "Epoch 5396:\tTraining Loss - 0.0028\n",
      "Epoch 5397:\tTraining Loss - 0.0026\n",
      "Epoch 5398:\tTraining Loss - 0.0060\n",
      "Epoch 5399:\tTraining Loss - 0.0039\n",
      "Epoch 5400:\tTraining Loss - 0.0041\n",
      "Epoch 5401:\tTraining Loss - 0.0034\n",
      "Epoch 5402:\tTraining Loss - 0.0033\n",
      "Epoch 5403:\tTraining Loss - 0.0073\n",
      "Epoch 5404:\tTraining Loss - 0.0043\n",
      "Epoch 5405:\tTraining Loss - 0.0067\n",
      "Epoch 5406:\tTraining Loss - 0.0057\n",
      "Epoch 5407:\tTraining Loss - 0.0040\n",
      "Epoch 5408:\tTraining Loss - 0.0062\n",
      "Epoch 5409:\tTraining Loss - 0.0056\n",
      "Epoch 5410:\tTraining Loss - 0.0041\n",
      "Epoch 5411:\tTraining Loss - 0.0026\n",
      "Epoch 5412:\tTraining Loss - 0.0112\n",
      "Epoch 5413:\tTraining Loss - 0.0057\n",
      "Epoch 5414:\tTraining Loss - 0.0063\n",
      "Epoch 5415:\tTraining Loss - 0.0040\n",
      "Epoch 5416:\tTraining Loss - 0.0038\n",
      "Epoch 5417:\tTraining Loss - 0.0060\n",
      "Epoch 5418:\tTraining Loss - 0.0038\n",
      "Epoch 5419:\tTraining Loss - 0.0068\n",
      "Epoch 5420:\tTraining Loss - 0.0046\n",
      "Epoch 5421:\tTraining Loss - 0.0093\n",
      "Epoch 5422:\tTraining Loss - 0.0029\n",
      "Epoch 5423:\tTraining Loss - 0.0091\n",
      "Epoch 5424:\tTraining Loss - 0.0035\n",
      "Epoch 5425:\tTraining Loss - 0.0044\n",
      "Epoch 5426:\tTraining Loss - 0.0057\n",
      "Epoch 5427:\tTraining Loss - 0.0055\n",
      "Epoch 5428:\tTraining Loss - 0.0055\n",
      "Epoch 5429:\tTraining Loss - 0.0066\n",
      "Epoch 5430:\tTraining Loss - 0.0053\n",
      "Epoch 5431:\tTraining Loss - 0.0052\n",
      "Epoch 5432:\tTraining Loss - 0.0058\n",
      "Epoch 5433:\tTraining Loss - 0.0073\n",
      "Epoch 5434:\tTraining Loss - 0.0053\n",
      "Epoch 5435:\tTraining Loss - 0.0057\n",
      "Epoch 5436:\tTraining Loss - 0.0040\n",
      "Epoch 5437:\tTraining Loss - 0.0031\n",
      "Epoch 5438:\tTraining Loss - 0.0044\n",
      "Epoch 5439:\tTraining Loss - 0.0090\n",
      "Epoch 5440:\tTraining Loss - 0.0102\n",
      "Epoch 5441:\tTraining Loss - 0.0029\n",
      "Epoch 5442:\tTraining Loss - 0.0053\n",
      "Epoch 5443:\tTraining Loss - 0.0022\n",
      "Epoch 5444:\tTraining Loss - 0.0027\n",
      "Epoch 5445:\tTraining Loss - 0.0043\n",
      "Epoch 5446:\tTraining Loss - 0.0097\n",
      "Epoch 5447:\tTraining Loss - 0.0056\n",
      "Epoch 5448:\tTraining Loss - 0.0054\n",
      "Epoch 5449:\tTraining Loss - 0.0060\n",
      "Epoch 5450:\tTraining Loss - 0.0077\n",
      "Epoch 5451:\tTraining Loss - 0.0068\n",
      "Epoch 5452:\tTraining Loss - 0.0036\n",
      "Epoch 5453:\tTraining Loss - 0.0072\n",
      "Epoch 5454:\tTraining Loss - 0.0029\n",
      "Epoch 5455:\tTraining Loss - 0.0029\n",
      "Epoch 5456:\tTraining Loss - 0.0054\n",
      "Epoch 5457:\tTraining Loss - 0.0032\n",
      "Epoch 5458:\tTraining Loss - 0.0043\n",
      "Epoch 5459:\tTraining Loss - 0.0054\n",
      "Epoch 5460:\tTraining Loss - 0.0106\n",
      "Epoch 5461:\tTraining Loss - 0.0039\n",
      "Epoch 5462:\tTraining Loss - 0.0044\n",
      "Epoch 5463:\tTraining Loss - 0.0056\n",
      "Epoch 5464:\tTraining Loss - 0.0051\n",
      "Epoch 5465:\tTraining Loss - 0.0054\n",
      "Epoch 5466:\tTraining Loss - 0.0093\n",
      "Epoch 5467:\tTraining Loss - 0.0055\n",
      "Epoch 5468:\tTraining Loss - 0.0033\n",
      "Epoch 5469:\tTraining Loss - 0.0053\n",
      "Epoch 5470:\tTraining Loss - 0.0028\n",
      "Epoch 5471:\tTraining Loss - 0.0026\n",
      "Epoch 5472:\tTraining Loss - 0.0043\n",
      "Epoch 5473:\tTraining Loss - 0.0056\n",
      "Epoch 5474:\tTraining Loss - 0.0047\n",
      "Epoch 5475:\tTraining Loss - 0.0046\n",
      "Epoch 5476:\tTraining Loss - 0.0054\n",
      "Epoch 5477:\tTraining Loss - 0.0034\n",
      "Epoch 5478:\tTraining Loss - 0.0056\n",
      "Epoch 5479:\tTraining Loss - 0.0037\n",
      "Epoch 5480:\tTraining Loss - 0.0067\n",
      "Epoch 5481:\tTraining Loss - 0.0063\n",
      "Epoch 5482:\tTraining Loss - 0.0044\n",
      "Epoch 5483:\tTraining Loss - 0.0045\n",
      "Epoch 5484:\tTraining Loss - 0.0051\n",
      "Epoch 5485:\tTraining Loss - 0.0032\n",
      "Epoch 5486:\tTraining Loss - 0.0043\n",
      "Epoch 5487:\tTraining Loss - 0.0067\n",
      "Epoch 5488:\tTraining Loss - 0.0024\n",
      "Epoch 5489:\tTraining Loss - 0.0016\n",
      "Epoch 5490:\tTraining Loss - 0.0036\n",
      "Epoch 5491:\tTraining Loss - 0.0059\n",
      "Epoch 5492:\tTraining Loss - 0.0055\n",
      "Epoch 5493:\tTraining Loss - 0.0028\n",
      "Epoch 5494:\tTraining Loss - 0.0089\n",
      "Epoch 5495:\tTraining Loss - 0.0034\n",
      "Epoch 5496:\tTraining Loss - 0.0021\n",
      "Epoch 5497:\tTraining Loss - 0.0070\n",
      "Epoch 5498:\tTraining Loss - 0.0026\n",
      "Epoch 5499:\tTraining Loss - 0.0025\n",
      "Epoch 5500:\tTraining Loss - 0.0054\n",
      "Epoch 5501:\tTraining Loss - 0.0029\n",
      "Epoch 5502:\tTraining Loss - 0.0038\n",
      "Epoch 5503:\tTraining Loss - 0.0034\n",
      "Epoch 5504:\tTraining Loss - 0.0061\n",
      "Epoch 5505:\tTraining Loss - 0.0067\n",
      "Epoch 5506:\tTraining Loss - 0.0025\n",
      "Epoch 5507:\tTraining Loss - 0.0048\n",
      "Epoch 5508:\tTraining Loss - 0.0050\n",
      "Epoch 5509:\tTraining Loss - 0.0028\n",
      "Epoch 5510:\tTraining Loss - 0.0041\n",
      "Epoch 5511:\tTraining Loss - 0.0050\n",
      "Epoch 5512:\tTraining Loss - 0.0051\n",
      "Epoch 5513:\tTraining Loss - 0.0044\n",
      "Epoch 5514:\tTraining Loss - 0.0057\n",
      "Epoch 5515:\tTraining Loss - 0.0050\n",
      "Epoch 5516:\tTraining Loss - 0.0021\n",
      "Epoch 5517:\tTraining Loss - 0.0105\n",
      "Epoch 5518:\tTraining Loss - 0.0060\n",
      "Epoch 5519:\tTraining Loss - 0.0049\n",
      "Epoch 5520:\tTraining Loss - 0.0058\n",
      "Epoch 5521:\tTraining Loss - 0.0048\n",
      "Epoch 5522:\tTraining Loss - 0.0076\n",
      "Epoch 5523:\tTraining Loss - 0.0071\n",
      "Epoch 5524:\tTraining Loss - 0.0054\n",
      "Epoch 5525:\tTraining Loss - 0.0077\n",
      "Epoch 5526:\tTraining Loss - 0.0061\n",
      "Epoch 5527:\tTraining Loss - 0.0043\n",
      "Epoch 5528:\tTraining Loss - 0.0056\n",
      "Epoch 5529:\tTraining Loss - 0.0040\n",
      "Epoch 5530:\tTraining Loss - 0.0032\n",
      "Epoch 5531:\tTraining Loss - 0.0051\n",
      "Epoch 5532:\tTraining Loss - 0.0028\n",
      "Epoch 5533:\tTraining Loss - 0.0040\n",
      "Epoch 5534:\tTraining Loss - 0.0046\n",
      "Epoch 5535:\tTraining Loss - 0.0029\n",
      "Epoch 5536:\tTraining Loss - 0.0019\n",
      "Epoch 5537:\tTraining Loss - 0.0041\n",
      "Epoch 5538:\tTraining Loss - 0.0030\n",
      "Epoch 5539:\tTraining Loss - 0.0045\n",
      "Epoch 5540:\tTraining Loss - 0.0053\n",
      "Epoch 5541:\tTraining Loss - 0.0052\n",
      "Epoch 5542:\tTraining Loss - 0.0068\n",
      "Epoch 5543:\tTraining Loss - 0.0031\n",
      "Epoch 5544:\tTraining Loss - 0.0044\n",
      "Epoch 5545:\tTraining Loss - 0.0049\n",
      "Epoch 5546:\tTraining Loss - 0.0026\n",
      "Epoch 5547:\tTraining Loss - 0.0076\n",
      "Epoch 5548:\tTraining Loss - 0.0032\n",
      "Epoch 5549:\tTraining Loss - 0.0102\n",
      "Epoch 5550:\tTraining Loss - 0.0075\n",
      "Epoch 5551:\tTraining Loss - 0.0084\n",
      "Epoch 5552:\tTraining Loss - 0.0065\n",
      "Epoch 5553:\tTraining Loss - 0.0043\n",
      "Epoch 5554:\tTraining Loss - 0.0052\n",
      "Epoch 5555:\tTraining Loss - 0.0060\n",
      "Epoch 5556:\tTraining Loss - 0.0046\n",
      "Epoch 5557:\tTraining Loss - 0.0091\n",
      "Epoch 5558:\tTraining Loss - 0.0027\n",
      "Epoch 5559:\tTraining Loss - 0.0068\n",
      "Epoch 5560:\tTraining Loss - 0.0070\n",
      "Epoch 5561:\tTraining Loss - 0.0092\n",
      "Epoch 5562:\tTraining Loss - 0.0060\n",
      "Epoch 5563:\tTraining Loss - 0.0059\n",
      "Epoch 5564:\tTraining Loss - 0.0036\n",
      "Epoch 5565:\tTraining Loss - 0.0086\n",
      "Epoch 5566:\tTraining Loss - 0.0051\n",
      "Epoch 5567:\tTraining Loss - 0.0048\n",
      "Epoch 5568:\tTraining Loss - 0.0063\n",
      "Epoch 5569:\tTraining Loss - 0.0038\n",
      "Epoch 5570:\tTraining Loss - 0.0031\n",
      "Epoch 5571:\tTraining Loss - 0.0058\n",
      "Epoch 5572:\tTraining Loss - 0.0041\n",
      "Epoch 5573:\tTraining Loss - 0.0047\n",
      "Epoch 5574:\tTraining Loss - 0.0055\n",
      "Epoch 5575:\tTraining Loss - 0.0041\n",
      "Epoch 5576:\tTraining Loss - 0.0065\n",
      "Epoch 5577:\tTraining Loss - 0.0044\n",
      "Epoch 5578:\tTraining Loss - 0.0037\n",
      "Epoch 5579:\tTraining Loss - 0.0050\n",
      "Epoch 5580:\tTraining Loss - 0.0026\n",
      "Epoch 5581:\tTraining Loss - 0.0018\n",
      "Epoch 5582:\tTraining Loss - 0.0054\n",
      "Epoch 5583:\tTraining Loss - 0.0091\n",
      "Epoch 5584:\tTraining Loss - 0.0052\n",
      "Epoch 5585:\tTraining Loss - 0.0034\n",
      "Epoch 5586:\tTraining Loss - 0.0057\n",
      "Epoch 5587:\tTraining Loss - 0.0058\n",
      "Epoch 5588:\tTraining Loss - 0.0051\n",
      "Epoch 5589:\tTraining Loss - 0.0049\n",
      "Epoch 5590:\tTraining Loss - 0.0071\n",
      "Epoch 5591:\tTraining Loss - 0.0063\n",
      "Epoch 5592:\tTraining Loss - 0.0043\n",
      "Epoch 5593:\tTraining Loss - 0.0059\n",
      "Epoch 5594:\tTraining Loss - 0.0035\n",
      "Epoch 5595:\tTraining Loss - 0.0050\n",
      "Epoch 5596:\tTraining Loss - 0.0090\n",
      "Epoch 5597:\tTraining Loss - 0.0050\n",
      "Epoch 5598:\tTraining Loss - 0.0031\n",
      "Epoch 5599:\tTraining Loss - 0.0065\n",
      "Epoch 5600:\tTraining Loss - 0.0038\n",
      "Epoch 5601:\tTraining Loss - 0.0025\n",
      "Epoch 5602:\tTraining Loss - 0.0089\n",
      "Epoch 5603:\tTraining Loss - 0.0073\n",
      "Epoch 5604:\tTraining Loss - 0.0051\n",
      "Epoch 5605:\tTraining Loss - 0.0039\n",
      "Epoch 5606:\tTraining Loss - 0.0033\n",
      "Epoch 5607:\tTraining Loss - 0.0055\n",
      "Epoch 5608:\tTraining Loss - 0.0023\n",
      "Epoch 5609:\tTraining Loss - 0.0022\n",
      "Epoch 5610:\tTraining Loss - 0.0046\n",
      "Epoch 5611:\tTraining Loss - 0.0101\n",
      "Epoch 5612:\tTraining Loss - 0.0053\n",
      "Epoch 5613:\tTraining Loss - 0.0094\n",
      "Epoch 5614:\tTraining Loss - 0.0035\n",
      "Epoch 5615:\tTraining Loss - 0.0034\n",
      "Epoch 5616:\tTraining Loss - 0.0067\n",
      "Epoch 5617:\tTraining Loss - 0.0075\n",
      "Epoch 5618:\tTraining Loss - 0.0044\n",
      "Epoch 5619:\tTraining Loss - 0.0033\n",
      "Epoch 5620:\tTraining Loss - 0.0047\n",
      "Epoch 5621:\tTraining Loss - 0.0069\n",
      "Epoch 5622:\tTraining Loss - 0.0049\n",
      "Epoch 5623:\tTraining Loss - 0.0059\n",
      "Epoch 5624:\tTraining Loss - 0.0061\n",
      "Epoch 5625:\tTraining Loss - 0.0021\n",
      "Epoch 5626:\tTraining Loss - 0.0048\n",
      "Epoch 5627:\tTraining Loss - 0.0032\n",
      "Epoch 5628:\tTraining Loss - 0.0035\n",
      "Epoch 5629:\tTraining Loss - 0.0022\n",
      "Epoch 5630:\tTraining Loss - 0.0026\n",
      "Epoch 5631:\tTraining Loss - 0.0078\n",
      "Epoch 5632:\tTraining Loss - 0.0043\n",
      "Epoch 5633:\tTraining Loss - 0.0026\n",
      "Epoch 5634:\tTraining Loss - 0.0094\n",
      "Epoch 5635:\tTraining Loss - 0.0023\n",
      "Epoch 5636:\tTraining Loss - 0.0037\n",
      "Epoch 5637:\tTraining Loss - 0.0085\n",
      "Epoch 5638:\tTraining Loss - 0.0074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5639:\tTraining Loss - 0.0055\n",
      "Epoch 5640:\tTraining Loss - 0.0061\n",
      "Epoch 5641:\tTraining Loss - 0.0040\n",
      "Epoch 5642:\tTraining Loss - 0.0069\n",
      "Epoch 5643:\tTraining Loss - 0.0106\n",
      "Epoch 5644:\tTraining Loss - 0.0051\n",
      "Epoch 5645:\tTraining Loss - 0.0083\n",
      "Epoch 5646:\tTraining Loss - 0.0033\n",
      "Epoch 5647:\tTraining Loss - 0.0029\n",
      "Epoch 5648:\tTraining Loss - 0.0017\n",
      "Epoch 5649:\tTraining Loss - 0.0055\n",
      "Epoch 5650:\tTraining Loss - 0.0063\n",
      "Epoch 5651:\tTraining Loss - 0.0044\n",
      "Epoch 5652:\tTraining Loss - 0.0042\n",
      "Epoch 5653:\tTraining Loss - 0.0057\n",
      "Epoch 5654:\tTraining Loss - 0.0049\n",
      "Epoch 5655:\tTraining Loss - 0.0075\n",
      "Epoch 5656:\tTraining Loss - 0.0087\n",
      "Epoch 5657:\tTraining Loss - 0.0048\n",
      "Epoch 5658:\tTraining Loss - 0.0032\n",
      "Epoch 5659:\tTraining Loss - 0.0031\n",
      "Epoch 5660:\tTraining Loss - 0.0054\n",
      "Epoch 5661:\tTraining Loss - 0.0042\n",
      "Epoch 5662:\tTraining Loss - 0.0038\n",
      "Epoch 5663:\tTraining Loss - 0.0028\n",
      "Epoch 5664:\tTraining Loss - 0.0041\n",
      "Epoch 5665:\tTraining Loss - 0.0073\n",
      "Epoch 5666:\tTraining Loss - 0.0084\n",
      "Epoch 5667:\tTraining Loss - 0.0050\n",
      "Epoch 5668:\tTraining Loss - 0.0038\n",
      "Epoch 5669:\tTraining Loss - 0.0021\n",
      "Epoch 5670:\tTraining Loss - 0.0039\n",
      "Epoch 5671:\tTraining Loss - 0.0067\n",
      "Epoch 5672:\tTraining Loss - 0.0056\n",
      "Epoch 5673:\tTraining Loss - 0.0025\n",
      "Epoch 5674:\tTraining Loss - 0.0079\n",
      "Epoch 5675:\tTraining Loss - 0.0036\n",
      "Epoch 5676:\tTraining Loss - 0.0044\n",
      "Epoch 5677:\tTraining Loss - 0.0075\n",
      "Epoch 5678:\tTraining Loss - 0.0105\n",
      "Epoch 5679:\tTraining Loss - 0.0018\n",
      "Epoch 5680:\tTraining Loss - 0.0087\n",
      "Epoch 5681:\tTraining Loss - 0.0047\n",
      "Epoch 5682:\tTraining Loss - 0.0061\n",
      "Epoch 5683:\tTraining Loss - 0.0035\n",
      "Epoch 5684:\tTraining Loss - 0.0060\n",
      "Epoch 5685:\tTraining Loss - 0.0065\n",
      "Epoch 5686:\tTraining Loss - 0.0036\n",
      "Epoch 5687:\tTraining Loss - 0.0056\n",
      "Epoch 5688:\tTraining Loss - 0.0033\n",
      "Epoch 5689:\tTraining Loss - 0.0019\n",
      "Epoch 5690:\tTraining Loss - 0.0041\n",
      "Epoch 5691:\tTraining Loss - 0.0032\n",
      "Epoch 5692:\tTraining Loss - 0.0055\n",
      "Epoch 5693:\tTraining Loss - 0.0058\n",
      "Epoch 5694:\tTraining Loss - 0.0032\n",
      "Epoch 5695:\tTraining Loss - 0.0070\n",
      "Epoch 5696:\tTraining Loss - 0.0065\n",
      "Epoch 5697:\tTraining Loss - 0.0058\n",
      "Epoch 5698:\tTraining Loss - 0.0048\n",
      "Epoch 5699:\tTraining Loss - 0.0056\n",
      "Epoch 5700:\tTraining Loss - 0.0067\n",
      "Epoch 5701:\tTraining Loss - 0.0080\n",
      "Epoch 5702:\tTraining Loss - 0.0061\n",
      "Epoch 5703:\tTraining Loss - 0.0029\n",
      "Epoch 5704:\tTraining Loss - 0.0081\n",
      "Epoch 5705:\tTraining Loss - 0.0075\n",
      "Epoch 5706:\tTraining Loss - 0.0037\n",
      "Epoch 5707:\tTraining Loss - 0.0020\n",
      "Epoch 5708:\tTraining Loss - 0.0035\n",
      "Epoch 5709:\tTraining Loss - 0.0076\n",
      "Epoch 5710:\tTraining Loss - 0.0053\n",
      "Epoch 5711:\tTraining Loss - 0.0053\n",
      "Epoch 5712:\tTraining Loss - 0.0052\n",
      "Epoch 5713:\tTraining Loss - 0.0050\n",
      "Epoch 5714:\tTraining Loss - 0.0086\n",
      "Epoch 5715:\tTraining Loss - 0.0054\n",
      "Epoch 5716:\tTraining Loss - 0.0063\n",
      "Epoch 5717:\tTraining Loss - 0.0045\n",
      "Epoch 5718:\tTraining Loss - 0.0038\n",
      "Epoch 5719:\tTraining Loss - 0.0067\n",
      "Epoch 5720:\tTraining Loss - 0.0049\n",
      "Epoch 5721:\tTraining Loss - 0.0076\n",
      "Epoch 5722:\tTraining Loss - 0.0046\n",
      "Epoch 5723:\tTraining Loss - 0.0065\n",
      "Epoch 5724:\tTraining Loss - 0.0045\n",
      "Epoch 5725:\tTraining Loss - 0.0040\n",
      "Epoch 5726:\tTraining Loss - 0.0040\n",
      "Epoch 5727:\tTraining Loss - 0.0024\n",
      "Epoch 5728:\tTraining Loss - 0.0042\n",
      "Epoch 5729:\tTraining Loss - 0.0029\n",
      "Epoch 5730:\tTraining Loss - 0.0067\n",
      "Epoch 5731:\tTraining Loss - 0.0038\n",
      "Epoch 5732:\tTraining Loss - 0.0021\n",
      "Epoch 5733:\tTraining Loss - 0.0068\n",
      "Epoch 5734:\tTraining Loss - 0.0044\n",
      "Epoch 5735:\tTraining Loss - 0.0056\n",
      "Epoch 5736:\tTraining Loss - 0.0048\n",
      "Epoch 5737:\tTraining Loss - 0.0038\n",
      "Epoch 5738:\tTraining Loss - 0.0049\n",
      "Epoch 5739:\tTraining Loss - 0.0034\n",
      "Epoch 5740:\tTraining Loss - 0.0044\n",
      "Epoch 5741:\tTraining Loss - 0.0039\n",
      "Epoch 5742:\tTraining Loss - 0.0030\n",
      "Epoch 5743:\tTraining Loss - 0.0027\n",
      "Epoch 5744:\tTraining Loss - 0.0049\n",
      "Epoch 5745:\tTraining Loss - 0.0061\n",
      "Epoch 5746:\tTraining Loss - 0.0053\n",
      "Epoch 5747:\tTraining Loss - 0.0058\n",
      "Epoch 5748:\tTraining Loss - 0.0057\n",
      "Epoch 5749:\tTraining Loss - 0.0020\n",
      "Epoch 5750:\tTraining Loss - 0.0045\n",
      "Epoch 5751:\tTraining Loss - 0.0096\n",
      "Epoch 5752:\tTraining Loss - 0.0092\n",
      "Epoch 5753:\tTraining Loss - 0.0044\n",
      "Epoch 5754:\tTraining Loss - 0.0047\n",
      "Epoch 5755:\tTraining Loss - 0.0049\n",
      "Epoch 5756:\tTraining Loss - 0.0037\n",
      "Epoch 5757:\tTraining Loss - 0.0060\n",
      "Epoch 5758:\tTraining Loss - 0.0041\n",
      "Epoch 5759:\tTraining Loss - 0.0021\n",
      "Epoch 5760:\tTraining Loss - 0.0032\n",
      "Epoch 5761:\tTraining Loss - 0.0082\n",
      "Epoch 5762:\tTraining Loss - 0.0025\n",
      "Epoch 5763:\tTraining Loss - 0.0068\n",
      "Epoch 5764:\tTraining Loss - 0.0067\n",
      "Epoch 5765:\tTraining Loss - 0.0052\n",
      "Epoch 5766:\tTraining Loss - 0.0066\n",
      "Epoch 5767:\tTraining Loss - 0.0045\n",
      "Epoch 5768:\tTraining Loss - 0.0063\n",
      "Epoch 5769:\tTraining Loss - 0.0061\n",
      "Epoch 5770:\tTraining Loss - 0.0035\n",
      "Epoch 5771:\tTraining Loss - 0.0037\n",
      "Epoch 5772:\tTraining Loss - 0.0041\n",
      "Epoch 5773:\tTraining Loss - 0.0027\n",
      "Epoch 5774:\tTraining Loss - 0.0046\n",
      "Epoch 5775:\tTraining Loss - 0.0071\n",
      "Epoch 5776:\tTraining Loss - 0.0049\n",
      "Epoch 5777:\tTraining Loss - 0.0031\n",
      "Epoch 5778:\tTraining Loss - 0.0053\n",
      "Epoch 5779:\tTraining Loss - 0.0051\n",
      "Epoch 5780:\tTraining Loss - 0.0026\n",
      "Epoch 5781:\tTraining Loss - 0.0100\n",
      "Epoch 5782:\tTraining Loss - 0.0033\n",
      "Epoch 5783:\tTraining Loss - 0.0045\n",
      "Epoch 5784:\tTraining Loss - 0.0053\n",
      "Epoch 5785:\tTraining Loss - 0.0046\n",
      "Epoch 5786:\tTraining Loss - 0.0041\n",
      "Epoch 5787:\tTraining Loss - 0.0030\n",
      "Epoch 5788:\tTraining Loss - 0.0054\n",
      "Epoch 5789:\tTraining Loss - 0.0048\n",
      "Epoch 5790:\tTraining Loss - 0.0080\n",
      "Epoch 5791:\tTraining Loss - 0.0073\n",
      "Epoch 5792:\tTraining Loss - 0.0088\n",
      "Epoch 5793:\tTraining Loss - 0.0019\n",
      "Epoch 5794:\tTraining Loss - 0.0042\n",
      "Epoch 5795:\tTraining Loss - 0.0044\n",
      "Epoch 5796:\tTraining Loss - 0.0080\n",
      "Epoch 5797:\tTraining Loss - 0.0066\n",
      "Epoch 5798:\tTraining Loss - 0.0089\n",
      "Epoch 5799:\tTraining Loss - 0.0054\n",
      "Epoch 5800:\tTraining Loss - 0.0032\n",
      "Epoch 5801:\tTraining Loss - 0.0050\n",
      "Epoch 5802:\tTraining Loss - 0.0058\n",
      "Epoch 5803:\tTraining Loss - 0.0018\n",
      "Epoch 5804:\tTraining Loss - 0.0038\n",
      "Epoch 5805:\tTraining Loss - 0.0069\n",
      "Epoch 5806:\tTraining Loss - 0.0084\n",
      "Epoch 5807:\tTraining Loss - 0.0051\n",
      "Epoch 5808:\tTraining Loss - 0.0052\n",
      "Epoch 5809:\tTraining Loss - 0.0064\n",
      "Epoch 5810:\tTraining Loss - 0.0054\n",
      "Epoch 5811:\tTraining Loss - 0.0061\n",
      "Epoch 5812:\tTraining Loss - 0.0044\n",
      "Epoch 5813:\tTraining Loss - 0.0061\n",
      "Epoch 5814:\tTraining Loss - 0.0056\n",
      "Epoch 5815:\tTraining Loss - 0.0041\n",
      "Epoch 5816:\tTraining Loss - 0.0061\n",
      "Epoch 5817:\tTraining Loss - 0.0046\n",
      "Epoch 5818:\tTraining Loss - 0.0062\n",
      "Epoch 5819:\tTraining Loss - 0.0023\n",
      "Epoch 5820:\tTraining Loss - 0.0056\n",
      "Epoch 5821:\tTraining Loss - 0.0097\n",
      "Epoch 5822:\tTraining Loss - 0.0063\n",
      "Epoch 5823:\tTraining Loss - 0.0041\n",
      "Epoch 5824:\tTraining Loss - 0.0053\n",
      "Epoch 5825:\tTraining Loss - 0.0038\n",
      "Epoch 5826:\tTraining Loss - 0.0047\n",
      "Epoch 5827:\tTraining Loss - 0.0056\n",
      "Epoch 5828:\tTraining Loss - 0.0050\n",
      "Epoch 5829:\tTraining Loss - 0.0079\n",
      "Epoch 5830:\tTraining Loss - 0.0022\n",
      "Epoch 5831:\tTraining Loss - 0.0037\n",
      "Epoch 5832:\tTraining Loss - 0.0057\n",
      "Epoch 5833:\tTraining Loss - 0.0038\n",
      "Epoch 5834:\tTraining Loss - 0.0077\n",
      "Epoch 5835:\tTraining Loss - 0.0042\n",
      "Epoch 5836:\tTraining Loss - 0.0046\n",
      "Epoch 5837:\tTraining Loss - 0.0055\n",
      "Epoch 5838:\tTraining Loss - 0.0050\n",
      "Epoch 5839:\tTraining Loss - 0.0053\n",
      "Epoch 5840:\tTraining Loss - 0.0071\n",
      "Epoch 5841:\tTraining Loss - 0.0071\n",
      "Epoch 5842:\tTraining Loss - 0.0068\n",
      "Epoch 5843:\tTraining Loss - 0.0070\n",
      "Epoch 5844:\tTraining Loss - 0.0058\n",
      "Epoch 5845:\tTraining Loss - 0.0019\n",
      "Epoch 5846:\tTraining Loss - 0.0029\n",
      "Epoch 5847:\tTraining Loss - 0.0037\n",
      "Epoch 5848:\tTraining Loss - 0.0037\n",
      "Epoch 5849:\tTraining Loss - 0.0055\n",
      "Epoch 5850:\tTraining Loss - 0.0028\n",
      "Epoch 5851:\tTraining Loss - 0.0036\n",
      "Epoch 5852:\tTraining Loss - 0.0055\n",
      "Epoch 5853:\tTraining Loss - 0.0081\n",
      "Epoch 5854:\tTraining Loss - 0.0040\n",
      "Epoch 5855:\tTraining Loss - 0.0057\n",
      "Epoch 5856:\tTraining Loss - 0.0030\n",
      "Epoch 5857:\tTraining Loss - 0.0037\n",
      "Epoch 5858:\tTraining Loss - 0.0025\n",
      "Epoch 5859:\tTraining Loss - 0.0038\n",
      "Epoch 5860:\tTraining Loss - 0.0026\n",
      "Epoch 5861:\tTraining Loss - 0.0087\n",
      "Epoch 5862:\tTraining Loss - 0.0058\n",
      "Epoch 5863:\tTraining Loss - 0.0044\n",
      "Epoch 5864:\tTraining Loss - 0.0064\n",
      "Epoch 5865:\tTraining Loss - 0.0057\n",
      "Epoch 5866:\tTraining Loss - 0.0043\n",
      "Epoch 5867:\tTraining Loss - 0.0028\n",
      "Epoch 5868:\tTraining Loss - 0.0045\n",
      "Epoch 5869:\tTraining Loss - 0.0064\n",
      "Epoch 5870:\tTraining Loss - 0.0030\n",
      "Epoch 5871:\tTraining Loss - 0.0064\n",
      "Epoch 5872:\tTraining Loss - 0.0050\n",
      "Epoch 5873:\tTraining Loss - 0.0072\n",
      "Epoch 5874:\tTraining Loss - 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5875:\tTraining Loss - 0.0054\n",
      "Epoch 5876:\tTraining Loss - 0.0032\n",
      "Epoch 5877:\tTraining Loss - 0.0055\n",
      "Epoch 5878:\tTraining Loss - 0.0047\n",
      "Epoch 5879:\tTraining Loss - 0.0052\n",
      "Epoch 5880:\tTraining Loss - 0.0087\n",
      "Epoch 5881:\tTraining Loss - 0.0035\n",
      "Epoch 5882:\tTraining Loss - 0.0079\n",
      "Epoch 5883:\tTraining Loss - 0.0054\n",
      "Epoch 5884:\tTraining Loss - 0.0055\n",
      "Epoch 5885:\tTraining Loss - 0.0036\n",
      "Epoch 5886:\tTraining Loss - 0.0090\n",
      "Epoch 5887:\tTraining Loss - 0.0042\n",
      "Epoch 5888:\tTraining Loss - 0.0063\n",
      "Epoch 5889:\tTraining Loss - 0.0056\n",
      "Epoch 5890:\tTraining Loss - 0.0076\n",
      "Epoch 5891:\tTraining Loss - 0.0041\n",
      "Epoch 5892:\tTraining Loss - 0.0021\n",
      "Epoch 5893:\tTraining Loss - 0.0052\n",
      "Epoch 5894:\tTraining Loss - 0.0056\n",
      "Epoch 5895:\tTraining Loss - 0.0027\n",
      "Epoch 5896:\tTraining Loss - 0.0057\n",
      "Epoch 5897:\tTraining Loss - 0.0042\n",
      "Epoch 5898:\tTraining Loss - 0.0039\n",
      "Epoch 5899:\tTraining Loss - 0.0026\n",
      "Epoch 5900:\tTraining Loss - 0.0061\n",
      "Epoch 5901:\tTraining Loss - 0.0027\n",
      "Epoch 5902:\tTraining Loss - 0.0094\n",
      "Epoch 5903:\tTraining Loss - 0.0038\n",
      "Epoch 5904:\tTraining Loss - 0.0024\n",
      "Epoch 5905:\tTraining Loss - 0.0033\n",
      "Epoch 5906:\tTraining Loss - 0.0031\n",
      "Epoch 5907:\tTraining Loss - 0.0049\n",
      "Epoch 5908:\tTraining Loss - 0.0019\n",
      "Epoch 5909:\tTraining Loss - 0.0057\n",
      "Epoch 5910:\tTraining Loss - 0.0074\n",
      "Epoch 5911:\tTraining Loss - 0.0023\n",
      "Epoch 5912:\tTraining Loss - 0.0054\n",
      "Epoch 5913:\tTraining Loss - 0.0096\n",
      "Epoch 5914:\tTraining Loss - 0.0092\n",
      "Epoch 5915:\tTraining Loss - 0.0045\n",
      "Epoch 5916:\tTraining Loss - 0.0062\n",
      "Epoch 5917:\tTraining Loss - 0.0032\n",
      "Epoch 5918:\tTraining Loss - 0.0046\n",
      "Epoch 5919:\tTraining Loss - 0.0044\n",
      "Epoch 5920:\tTraining Loss - 0.0052\n",
      "Epoch 5921:\tTraining Loss - 0.0076\n",
      "Epoch 5922:\tTraining Loss - 0.0035\n",
      "Epoch 5923:\tTraining Loss - 0.0068\n",
      "Epoch 5924:\tTraining Loss - 0.0058\n",
      "Epoch 5925:\tTraining Loss - 0.0062\n",
      "Epoch 5926:\tTraining Loss - 0.0042\n",
      "Epoch 5927:\tTraining Loss - 0.0044\n",
      "Epoch 5928:\tTraining Loss - 0.0028\n",
      "Epoch 5929:\tTraining Loss - 0.0054\n",
      "Epoch 5930:\tTraining Loss - 0.0056\n",
      "Epoch 5931:\tTraining Loss - 0.0045\n",
      "Epoch 5932:\tTraining Loss - 0.0063\n",
      "Epoch 5933:\tTraining Loss - 0.0074\n",
      "Epoch 5934:\tTraining Loss - 0.0092\n",
      "Epoch 5935:\tTraining Loss - 0.0030\n",
      "Epoch 5936:\tTraining Loss - 0.0061\n",
      "Epoch 5937:\tTraining Loss - 0.0029\n",
      "Epoch 5938:\tTraining Loss - 0.0028\n",
      "Epoch 5939:\tTraining Loss - 0.0066\n",
      "Epoch 5940:\tTraining Loss - 0.0046\n",
      "Epoch 5941:\tTraining Loss - 0.0041\n",
      "Epoch 5942:\tTraining Loss - 0.0033\n",
      "Epoch 5943:\tTraining Loss - 0.0109\n",
      "Epoch 5944:\tTraining Loss - 0.0041\n",
      "Epoch 5945:\tTraining Loss - 0.0054\n",
      "Epoch 5946:\tTraining Loss - 0.0058\n",
      "Epoch 5947:\tTraining Loss - 0.0046\n",
      "Epoch 5948:\tTraining Loss - 0.0038\n",
      "Epoch 5949:\tTraining Loss - 0.0050\n",
      "Epoch 5950:\tTraining Loss - 0.0093\n",
      "Epoch 5951:\tTraining Loss - 0.0054\n",
      "Epoch 5952:\tTraining Loss - 0.0025\n",
      "Epoch 5953:\tTraining Loss - 0.0034\n",
      "Epoch 5954:\tTraining Loss - 0.0039\n",
      "Epoch 5955:\tTraining Loss - 0.0042\n",
      "Epoch 5956:\tTraining Loss - 0.0097\n",
      "Epoch 5957:\tTraining Loss - 0.0027\n",
      "Epoch 5958:\tTraining Loss - 0.0063\n",
      "Epoch 5959:\tTraining Loss - 0.0034\n",
      "Epoch 5960:\tTraining Loss - 0.0059\n",
      "Epoch 5961:\tTraining Loss - 0.0064\n",
      "Epoch 5962:\tTraining Loss - 0.0052\n",
      "Epoch 5963:\tTraining Loss - 0.0031\n",
      "Epoch 5964:\tTraining Loss - 0.0028\n",
      "Epoch 5965:\tTraining Loss - 0.0040\n",
      "Epoch 5966:\tTraining Loss - 0.0052\n",
      "Epoch 5967:\tTraining Loss - 0.0062\n",
      "Epoch 5968:\tTraining Loss - 0.0034\n",
      "Epoch 5969:\tTraining Loss - 0.0043\n",
      "Epoch 5970:\tTraining Loss - 0.0083\n",
      "Epoch 5971:\tTraining Loss - 0.0022\n",
      "Epoch 5972:\tTraining Loss - 0.0050\n",
      "Epoch 5973:\tTraining Loss - 0.0071\n",
      "Epoch 5974:\tTraining Loss - 0.0030\n",
      "Epoch 5975:\tTraining Loss - 0.0023\n",
      "Epoch 5976:\tTraining Loss - 0.0057\n",
      "Epoch 5977:\tTraining Loss - 0.0038\n",
      "Epoch 5978:\tTraining Loss - 0.0035\n",
      "Epoch 5979:\tTraining Loss - 0.0058\n",
      "Epoch 5980:\tTraining Loss - 0.0056\n",
      "Epoch 5981:\tTraining Loss - 0.0021\n",
      "Epoch 5982:\tTraining Loss - 0.0053\n",
      "Epoch 5983:\tTraining Loss - 0.0070\n",
      "Epoch 5984:\tTraining Loss - 0.0045\n",
      "Epoch 5985:\tTraining Loss - 0.0059\n",
      "Epoch 5986:\tTraining Loss - 0.0058\n",
      "Epoch 5987:\tTraining Loss - 0.0021\n",
      "Epoch 5988:\tTraining Loss - 0.0037\n",
      "Epoch 5989:\tTraining Loss - 0.0054\n",
      "Epoch 5990:\tTraining Loss - 0.0062\n",
      "Epoch 5991:\tTraining Loss - 0.0042\n",
      "Epoch 5992:\tTraining Loss - 0.0095\n",
      "Epoch 5993:\tTraining Loss - 0.0034\n",
      "Epoch 5994:\tTraining Loss - 0.0068\n",
      "Epoch 5995:\tTraining Loss - 0.0047\n",
      "Epoch 5996:\tTraining Loss - 0.0082\n",
      "Epoch 5997:\tTraining Loss - 0.0066\n",
      "Epoch 5998:\tTraining Loss - 0.0026\n",
      "Epoch 5999:\tTraining Loss - 0.0059\n",
      "Epoch 6000:\tTraining Loss - 0.0030\n",
      "Epoch 6001:\tTraining Loss - 0.0042\n",
      "Epoch 6002:\tTraining Loss - 0.0050\n",
      "Epoch 6003:\tTraining Loss - 0.0044\n",
      "Epoch 6004:\tTraining Loss - 0.0036\n",
      "Epoch 6005:\tTraining Loss - 0.0042\n",
      "Epoch 6006:\tTraining Loss - 0.0019\n",
      "Epoch 6007:\tTraining Loss - 0.0037\n",
      "Epoch 6008:\tTraining Loss - 0.0102\n",
      "Epoch 6009:\tTraining Loss - 0.0021\n",
      "Epoch 6010:\tTraining Loss - 0.0033\n",
      "Epoch 6011:\tTraining Loss - 0.0058\n",
      "Epoch 6012:\tTraining Loss - 0.0046\n",
      "Epoch 6013:\tTraining Loss - 0.0051\n",
      "Epoch 6014:\tTraining Loss - 0.0082\n",
      "Epoch 6015:\tTraining Loss - 0.0058\n",
      "Epoch 6016:\tTraining Loss - 0.0065\n",
      "Epoch 6017:\tTraining Loss - 0.0055\n",
      "Epoch 6018:\tTraining Loss - 0.0052\n",
      "Epoch 6019:\tTraining Loss - 0.0046\n",
      "Epoch 6020:\tTraining Loss - 0.0035\n",
      "Epoch 6021:\tTraining Loss - 0.0052\n",
      "Epoch 6022:\tTraining Loss - 0.0021\n",
      "Epoch 6023:\tTraining Loss - 0.0016\n",
      "Epoch 6024:\tTraining Loss - 0.0056\n",
      "Epoch 6025:\tTraining Loss - 0.0056\n",
      "Epoch 6026:\tTraining Loss - 0.0088\n",
      "Epoch 6027:\tTraining Loss - 0.0020\n",
      "Epoch 6028:\tTraining Loss - 0.0027\n",
      "Epoch 6029:\tTraining Loss - 0.0027\n",
      "Epoch 6030:\tTraining Loss - 0.0039\n",
      "Epoch 6031:\tTraining Loss - 0.0042\n",
      "Epoch 6032:\tTraining Loss - 0.0050\n",
      "Epoch 6033:\tTraining Loss - 0.0053\n",
      "Epoch 6034:\tTraining Loss - 0.0034\n",
      "Epoch 6035:\tTraining Loss - 0.0022\n",
      "Epoch 6036:\tTraining Loss - 0.0042\n",
      "Epoch 6037:\tTraining Loss - 0.0066\n",
      "Epoch 6038:\tTraining Loss - 0.0027\n",
      "Epoch 6039:\tTraining Loss - 0.0054\n",
      "Epoch 6040:\tTraining Loss - 0.0029\n",
      "Epoch 6041:\tTraining Loss - 0.0050\n",
      "Epoch 6042:\tTraining Loss - 0.0031\n",
      "Epoch 6043:\tTraining Loss - 0.0027\n",
      "Epoch 6044:\tTraining Loss - 0.0055\n",
      "Epoch 6045:\tTraining Loss - 0.0114\n",
      "Epoch 6046:\tTraining Loss - 0.0100\n",
      "Epoch 6047:\tTraining Loss - 0.0054\n",
      "Epoch 6048:\tTraining Loss - 0.0054\n",
      "Epoch 6049:\tTraining Loss - 0.0058\n",
      "Epoch 6050:\tTraining Loss - 0.0057\n",
      "Epoch 6051:\tTraining Loss - 0.0045\n",
      "Epoch 6052:\tTraining Loss - 0.0036\n",
      "Epoch 6053:\tTraining Loss - 0.0044\n",
      "Epoch 6054:\tTraining Loss - 0.0057\n",
      "Epoch 6055:\tTraining Loss - 0.0075\n",
      "Epoch 6056:\tTraining Loss - 0.0047\n",
      "Epoch 6057:\tTraining Loss - 0.0067\n",
      "Epoch 6058:\tTraining Loss - 0.0067\n",
      "Epoch 6059:\tTraining Loss - 0.0067\n",
      "Epoch 6060:\tTraining Loss - 0.0027\n",
      "Epoch 6061:\tTraining Loss - 0.0053\n",
      "Epoch 6062:\tTraining Loss - 0.0057\n",
      "Epoch 6063:\tTraining Loss - 0.0062\n",
      "Epoch 6064:\tTraining Loss - 0.0031\n",
      "Epoch 6065:\tTraining Loss - 0.0028\n",
      "Epoch 6066:\tTraining Loss - 0.0035\n",
      "Epoch 6067:\tTraining Loss - 0.0077\n",
      "Epoch 6068:\tTraining Loss - 0.0029\n",
      "Epoch 6069:\tTraining Loss - 0.0067\n",
      "Epoch 6070:\tTraining Loss - 0.0057\n",
      "Epoch 6071:\tTraining Loss - 0.0093\n",
      "Epoch 6072:\tTraining Loss - 0.0095\n",
      "Epoch 6073:\tTraining Loss - 0.0070\n",
      "Epoch 6074:\tTraining Loss - 0.0056\n",
      "Epoch 6075:\tTraining Loss - 0.0047\n",
      "Epoch 6076:\tTraining Loss - 0.0071\n",
      "Epoch 6077:\tTraining Loss - 0.0053\n",
      "Epoch 6078:\tTraining Loss - 0.0048\n",
      "Epoch 6079:\tTraining Loss - 0.0049\n",
      "Epoch 6080:\tTraining Loss - 0.0021\n",
      "Epoch 6081:\tTraining Loss - 0.0040\n",
      "Epoch 6082:\tTraining Loss - 0.0025\n",
      "Epoch 6083:\tTraining Loss - 0.0063\n",
      "Epoch 6084:\tTraining Loss - 0.0068\n",
      "Epoch 6085:\tTraining Loss - 0.0030\n",
      "Epoch 6086:\tTraining Loss - 0.0070\n",
      "Epoch 6087:\tTraining Loss - 0.0038\n",
      "Epoch 6088:\tTraining Loss - 0.0049\n",
      "Epoch 6089:\tTraining Loss - 0.0067\n",
      "Epoch 6090:\tTraining Loss - 0.0034\n",
      "Epoch 6091:\tTraining Loss - 0.0067\n",
      "Epoch 6092:\tTraining Loss - 0.0047\n",
      "Epoch 6093:\tTraining Loss - 0.0030\n",
      "Epoch 6094:\tTraining Loss - 0.0111\n",
      "Epoch 6095:\tTraining Loss - 0.0055\n",
      "Epoch 6096:\tTraining Loss - 0.0099\n",
      "Epoch 6097:\tTraining Loss - 0.0043\n",
      "Epoch 6098:\tTraining Loss - 0.0034\n",
      "Epoch 6099:\tTraining Loss - 0.0043\n",
      "Epoch 6100:\tTraining Loss - 0.0050\n",
      "Epoch 6101:\tTraining Loss - 0.0040\n",
      "Epoch 6102:\tTraining Loss - 0.0018\n",
      "Epoch 6103:\tTraining Loss - 0.0068\n",
      "Epoch 6104:\tTraining Loss - 0.0030\n",
      "Epoch 6105:\tTraining Loss - 0.0062\n",
      "Epoch 6106:\tTraining Loss - 0.0073\n",
      "Epoch 6107:\tTraining Loss - 0.0026\n",
      "Epoch 6108:\tTraining Loss - 0.0067\n",
      "Epoch 6109:\tTraining Loss - 0.0043\n",
      "Epoch 6110:\tTraining Loss - 0.0062\n",
      "Epoch 6111:\tTraining Loss - 0.0042\n",
      "Epoch 6112:\tTraining Loss - 0.0023\n",
      "Epoch 6113:\tTraining Loss - 0.0032\n",
      "Epoch 6114:\tTraining Loss - 0.0091\n",
      "Epoch 6115:\tTraining Loss - 0.0042\n",
      "Epoch 6116:\tTraining Loss - 0.0070\n",
      "Epoch 6117:\tTraining Loss - 0.0029\n",
      "Epoch 6118:\tTraining Loss - 0.0036\n",
      "Epoch 6119:\tTraining Loss - 0.0025\n",
      "Epoch 6120:\tTraining Loss - 0.0041\n",
      "Epoch 6121:\tTraining Loss - 0.0042\n",
      "Epoch 6122:\tTraining Loss - 0.0067\n",
      "Epoch 6123:\tTraining Loss - 0.0065\n",
      "Epoch 6124:\tTraining Loss - 0.0052\n",
      "Epoch 6125:\tTraining Loss - 0.0035\n",
      "Epoch 6126:\tTraining Loss - 0.0018\n",
      "Epoch 6127:\tTraining Loss - 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6128:\tTraining Loss - 0.0053\n",
      "Epoch 6129:\tTraining Loss - 0.0078\n",
      "Epoch 6130:\tTraining Loss - 0.0026\n",
      "Epoch 6131:\tTraining Loss - 0.0028\n",
      "Epoch 6132:\tTraining Loss - 0.0042\n",
      "Epoch 6133:\tTraining Loss - 0.0039\n",
      "Epoch 6134:\tTraining Loss - 0.0032\n",
      "Epoch 6135:\tTraining Loss - 0.0086\n",
      "Epoch 6136:\tTraining Loss - 0.0035\n",
      "Epoch 6137:\tTraining Loss - 0.0044\n",
      "Epoch 6138:\tTraining Loss - 0.0064\n",
      "Epoch 6139:\tTraining Loss - 0.0025\n",
      "Epoch 6140:\tTraining Loss - 0.0061\n",
      "Epoch 6141:\tTraining Loss - 0.0039\n",
      "Epoch 6142:\tTraining Loss - 0.0025\n",
      "Epoch 6143:\tTraining Loss - 0.0048\n",
      "Epoch 6144:\tTraining Loss - 0.0021\n",
      "Epoch 6145:\tTraining Loss - 0.0052\n",
      "Epoch 6146:\tTraining Loss - 0.0107\n",
      "Epoch 6147:\tTraining Loss - 0.0053\n",
      "Epoch 6148:\tTraining Loss - 0.0038\n",
      "Epoch 6149:\tTraining Loss - 0.0013\n",
      "Epoch 6150:\tTraining Loss - 0.0063\n",
      "Epoch 6151:\tTraining Loss - 0.0029\n",
      "Epoch 6152:\tTraining Loss - 0.0104\n",
      "Epoch 6153:\tTraining Loss - 0.0103\n",
      "Epoch 6154:\tTraining Loss - 0.0051\n",
      "Epoch 6155:\tTraining Loss - 0.0067\n",
      "Epoch 6156:\tTraining Loss - 0.0028\n",
      "Epoch 6157:\tTraining Loss - 0.0030\n",
      "Epoch 6158:\tTraining Loss - 0.0045\n",
      "Epoch 6159:\tTraining Loss - 0.0069\n",
      "Epoch 6160:\tTraining Loss - 0.0041\n",
      "Epoch 6161:\tTraining Loss - 0.0064\n",
      "Epoch 6162:\tTraining Loss - 0.0055\n",
      "Epoch 6163:\tTraining Loss - 0.0033\n",
      "Epoch 6164:\tTraining Loss - 0.0048\n",
      "Epoch 6165:\tTraining Loss - 0.0063\n",
      "Epoch 6166:\tTraining Loss - 0.0042\n",
      "Epoch 6167:\tTraining Loss - 0.0029\n",
      "Epoch 6168:\tTraining Loss - 0.0076\n",
      "Epoch 6169:\tTraining Loss - 0.0077\n",
      "Epoch 6170:\tTraining Loss - 0.0048\n",
      "Epoch 6171:\tTraining Loss - 0.0069\n",
      "Epoch 6172:\tTraining Loss - 0.0029\n",
      "Epoch 6173:\tTraining Loss - 0.0032\n",
      "Epoch 6174:\tTraining Loss - 0.0041\n",
      "Epoch 6175:\tTraining Loss - 0.0046\n",
      "Epoch 6176:\tTraining Loss - 0.0062\n",
      "Epoch 6177:\tTraining Loss - 0.0058\n",
      "Epoch 6178:\tTraining Loss - 0.0052\n",
      "Epoch 6179:\tTraining Loss - 0.0072\n",
      "Epoch 6180:\tTraining Loss - 0.0048\n",
      "Epoch 6181:\tTraining Loss - 0.0035\n",
      "Epoch 6182:\tTraining Loss - 0.0069\n",
      "Epoch 6183:\tTraining Loss - 0.0030\n",
      "Epoch 6184:\tTraining Loss - 0.0060\n",
      "Epoch 6185:\tTraining Loss - 0.0096\n",
      "Epoch 6186:\tTraining Loss - 0.0042\n",
      "Epoch 6187:\tTraining Loss - 0.0058\n",
      "Epoch 6188:\tTraining Loss - 0.0054\n",
      "Epoch 6189:\tTraining Loss - 0.0023\n",
      "Epoch 6190:\tTraining Loss - 0.0025\n",
      "Epoch 6191:\tTraining Loss - 0.0036\n",
      "Epoch 6192:\tTraining Loss - 0.0059\n",
      "Epoch 6193:\tTraining Loss - 0.0065\n",
      "Epoch 6194:\tTraining Loss - 0.0081\n",
      "Epoch 6195:\tTraining Loss - 0.0046\n",
      "Epoch 6196:\tTraining Loss - 0.0046\n",
      "Epoch 6197:\tTraining Loss - 0.0036\n",
      "Epoch 6198:\tTraining Loss - 0.0036\n",
      "Epoch 6199:\tTraining Loss - 0.0066\n",
      "Epoch 6200:\tTraining Loss - 0.0042\n",
      "Epoch 6201:\tTraining Loss - 0.0042\n",
      "Epoch 6202:\tTraining Loss - 0.0105\n",
      "Epoch 6203:\tTraining Loss - 0.0024\n",
      "Epoch 6204:\tTraining Loss - 0.0064\n",
      "Epoch 6205:\tTraining Loss - 0.0021\n",
      "Epoch 6206:\tTraining Loss - 0.0021\n",
      "Epoch 6207:\tTraining Loss - 0.0062\n",
      "Epoch 6208:\tTraining Loss - 0.0108\n",
      "Epoch 6209:\tTraining Loss - 0.0037\n",
      "Epoch 6210:\tTraining Loss - 0.0050\n",
      "Epoch 6211:\tTraining Loss - 0.0060\n",
      "Epoch 6212:\tTraining Loss - 0.0053\n",
      "Epoch 6213:\tTraining Loss - 0.0020\n",
      "Epoch 6214:\tTraining Loss - 0.0024\n",
      "Epoch 6215:\tTraining Loss - 0.0039\n",
      "Epoch 6216:\tTraining Loss - 0.0063\n",
      "Epoch 6217:\tTraining Loss - 0.0028\n",
      "Epoch 6218:\tTraining Loss - 0.0061\n",
      "Epoch 6219:\tTraining Loss - 0.0046\n",
      "Epoch 6220:\tTraining Loss - 0.0064\n",
      "Epoch 6221:\tTraining Loss - 0.0048\n",
      "Epoch 6222:\tTraining Loss - 0.0028\n",
      "Epoch 6223:\tTraining Loss - 0.0064\n",
      "Epoch 6224:\tTraining Loss - 0.0040\n",
      "Epoch 6225:\tTraining Loss - 0.0036\n",
      "Epoch 6226:\tTraining Loss - 0.0055\n",
      "Epoch 6227:\tTraining Loss - 0.0045\n",
      "Epoch 6228:\tTraining Loss - 0.0040\n",
      "Epoch 6229:\tTraining Loss - 0.0055\n",
      "Epoch 6230:\tTraining Loss - 0.0078\n",
      "Epoch 6231:\tTraining Loss - 0.0043\n",
      "Epoch 6232:\tTraining Loss - 0.0032\n",
      "Epoch 6233:\tTraining Loss - 0.0050\n",
      "Epoch 6234:\tTraining Loss - 0.0045\n",
      "Epoch 6235:\tTraining Loss - 0.0016\n",
      "Epoch 6236:\tTraining Loss - 0.0023\n",
      "Epoch 6237:\tTraining Loss - 0.0045\n",
      "Epoch 6238:\tTraining Loss - 0.0085\n",
      "Epoch 6239:\tTraining Loss - 0.0061\n",
      "Epoch 6240:\tTraining Loss - 0.0028\n",
      "Epoch 6241:\tTraining Loss - 0.0085\n",
      "Epoch 6242:\tTraining Loss - 0.0034\n",
      "Epoch 6243:\tTraining Loss - 0.0053\n",
      "Epoch 6244:\tTraining Loss - 0.0093\n",
      "Epoch 6245:\tTraining Loss - 0.0042\n",
      "Epoch 6246:\tTraining Loss - 0.0062\n",
      "Epoch 6247:\tTraining Loss - 0.0020\n",
      "Epoch 6248:\tTraining Loss - 0.0043\n",
      "Epoch 6249:\tTraining Loss - 0.0081\n",
      "Epoch 6250:\tTraining Loss - 0.0025\n",
      "Epoch 6251:\tTraining Loss - 0.0061\n",
      "Epoch 6252:\tTraining Loss - 0.0072\n",
      "Epoch 6253:\tTraining Loss - 0.0050\n",
      "Epoch 6254:\tTraining Loss - 0.0026\n",
      "Epoch 6255:\tTraining Loss - 0.0060\n",
      "Epoch 6256:\tTraining Loss - 0.0050\n",
      "Epoch 6257:\tTraining Loss - 0.0035\n",
      "Epoch 6258:\tTraining Loss - 0.0049\n",
      "Epoch 6259:\tTraining Loss - 0.0017\n",
      "Epoch 6260:\tTraining Loss - 0.0049\n",
      "Epoch 6261:\tTraining Loss - 0.0039\n",
      "Epoch 6262:\tTraining Loss - 0.0076\n",
      "Epoch 6263:\tTraining Loss - 0.0059\n",
      "Epoch 6264:\tTraining Loss - 0.0028\n",
      "Epoch 6265:\tTraining Loss - 0.0067\n",
      "Epoch 6266:\tTraining Loss - 0.0055\n",
      "Epoch 6267:\tTraining Loss - 0.0025\n",
      "Epoch 6268:\tTraining Loss - 0.0079\n",
      "Epoch 6269:\tTraining Loss - 0.0023\n",
      "Epoch 6270:\tTraining Loss - 0.0044\n",
      "Epoch 6271:\tTraining Loss - 0.0061\n",
      "Epoch 6272:\tTraining Loss - 0.0052\n",
      "Epoch 6273:\tTraining Loss - 0.0068\n",
      "Epoch 6274:\tTraining Loss - 0.0046\n",
      "Epoch 6275:\tTraining Loss - 0.0045\n",
      "Epoch 6276:\tTraining Loss - 0.0049\n",
      "Epoch 6277:\tTraining Loss - 0.0023\n",
      "Epoch 6278:\tTraining Loss - 0.0051\n",
      "Epoch 6279:\tTraining Loss - 0.0067\n",
      "Epoch 6280:\tTraining Loss - 0.0025\n",
      "Epoch 6281:\tTraining Loss - 0.0046\n",
      "Epoch 6282:\tTraining Loss - 0.0031\n",
      "Epoch 6283:\tTraining Loss - 0.0086\n",
      "Epoch 6284:\tTraining Loss - 0.0041\n",
      "Epoch 6285:\tTraining Loss - 0.0049\n",
      "Epoch 6286:\tTraining Loss - 0.0031\n",
      "Epoch 6287:\tTraining Loss - 0.0034\n",
      "Epoch 6288:\tTraining Loss - 0.0068\n",
      "Epoch 6289:\tTraining Loss - 0.0105\n",
      "Epoch 6290:\tTraining Loss - 0.0047\n",
      "Epoch 6291:\tTraining Loss - 0.0079\n",
      "Epoch 6292:\tTraining Loss - 0.0037\n",
      "Epoch 6293:\tTraining Loss - 0.0072\n",
      "Epoch 6294:\tTraining Loss - 0.0056\n",
      "Epoch 6295:\tTraining Loss - 0.0072\n",
      "Epoch 6296:\tTraining Loss - 0.0081\n",
      "Epoch 6297:\tTraining Loss - 0.0036\n",
      "Epoch 6298:\tTraining Loss - 0.0055\n",
      "Epoch 6299:\tTraining Loss - 0.0023\n",
      "Epoch 6300:\tTraining Loss - 0.0048\n",
      "Epoch 6301:\tTraining Loss - 0.0065\n",
      "Epoch 6302:\tTraining Loss - 0.0041\n",
      "Epoch 6303:\tTraining Loss - 0.0040\n",
      "Epoch 6304:\tTraining Loss - 0.0018\n",
      "Epoch 6305:\tTraining Loss - 0.0066\n",
      "Epoch 6306:\tTraining Loss - 0.0105\n",
      "Epoch 6307:\tTraining Loss - 0.0028\n",
      "Epoch 6308:\tTraining Loss - 0.0034\n",
      "Epoch 6309:\tTraining Loss - 0.0025\n",
      "Epoch 6310:\tTraining Loss - 0.0057\n",
      "Epoch 6311:\tTraining Loss - 0.0045\n",
      "Epoch 6312:\tTraining Loss - 0.0049\n",
      "Epoch 6313:\tTraining Loss - 0.0043\n",
      "Epoch 6314:\tTraining Loss - 0.0035\n",
      "Epoch 6315:\tTraining Loss - 0.0032\n",
      "Epoch 6316:\tTraining Loss - 0.0048\n",
      "Epoch 6317:\tTraining Loss - 0.0043\n",
      "Epoch 6318:\tTraining Loss - 0.0035\n",
      "Epoch 6319:\tTraining Loss - 0.0051\n",
      "Epoch 6320:\tTraining Loss - 0.0039\n",
      "Epoch 6321:\tTraining Loss - 0.0075\n",
      "Epoch 6322:\tTraining Loss - 0.0035\n",
      "Epoch 6323:\tTraining Loss - 0.0055\n",
      "Epoch 6324:\tTraining Loss - 0.0031\n",
      "Epoch 6325:\tTraining Loss - 0.0080\n",
      "Epoch 6326:\tTraining Loss - 0.0034\n",
      "Epoch 6327:\tTraining Loss - 0.0022\n",
      "Epoch 6328:\tTraining Loss - 0.0025\n",
      "Epoch 6329:\tTraining Loss - 0.0039\n",
      "Epoch 6330:\tTraining Loss - 0.0029\n",
      "Epoch 6331:\tTraining Loss - 0.0037\n",
      "Epoch 6332:\tTraining Loss - 0.0039\n",
      "Epoch 6333:\tTraining Loss - 0.0051\n",
      "Epoch 6334:\tTraining Loss - 0.0033\n",
      "Epoch 6335:\tTraining Loss - 0.0046\n",
      "Epoch 6336:\tTraining Loss - 0.0065\n",
      "Epoch 6337:\tTraining Loss - 0.0055\n",
      "Epoch 6338:\tTraining Loss - 0.0056\n",
      "Epoch 6339:\tTraining Loss - 0.0064\n",
      "Epoch 6340:\tTraining Loss - 0.0030\n",
      "Epoch 6341:\tTraining Loss - 0.0094\n",
      "Epoch 6342:\tTraining Loss - 0.0029\n",
      "Epoch 6343:\tTraining Loss - 0.0095\n",
      "Epoch 6344:\tTraining Loss - 0.0040\n",
      "Epoch 6345:\tTraining Loss - 0.0024\n",
      "Epoch 6346:\tTraining Loss - 0.0070\n",
      "Epoch 6347:\tTraining Loss - 0.0077\n",
      "Epoch 6348:\tTraining Loss - 0.0026\n",
      "Epoch 6349:\tTraining Loss - 0.0025\n",
      "Epoch 6350:\tTraining Loss - 0.0058\n",
      "Epoch 6351:\tTraining Loss - 0.0066\n",
      "Epoch 6352:\tTraining Loss - 0.0032\n",
      "Epoch 6353:\tTraining Loss - 0.0027\n",
      "Epoch 6354:\tTraining Loss - 0.0053\n",
      "Epoch 6355:\tTraining Loss - 0.0043\n",
      "Epoch 6356:\tTraining Loss - 0.0037\n",
      "Epoch 6357:\tTraining Loss - 0.0090\n",
      "Epoch 6358:\tTraining Loss - 0.0037\n",
      "Epoch 6359:\tTraining Loss - 0.0075\n",
      "Epoch 6360:\tTraining Loss - 0.0022\n",
      "Epoch 6361:\tTraining Loss - 0.0038\n",
      "Epoch 6362:\tTraining Loss - 0.0041\n",
      "Epoch 6363:\tTraining Loss - 0.0035\n",
      "Epoch 6364:\tTraining Loss - 0.0033\n",
      "Epoch 6365:\tTraining Loss - 0.0047\n",
      "Epoch 6366:\tTraining Loss - 0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6367:\tTraining Loss - 0.0036\n",
      "Epoch 6368:\tTraining Loss - 0.0065\n",
      "Epoch 6369:\tTraining Loss - 0.0036\n",
      "Epoch 6370:\tTraining Loss - 0.0056\n",
      "Epoch 6371:\tTraining Loss - 0.0023\n",
      "Epoch 6372:\tTraining Loss - 0.0026\n",
      "Epoch 6373:\tTraining Loss - 0.0042\n",
      "Epoch 6374:\tTraining Loss - 0.0032\n",
      "Epoch 6375:\tTraining Loss - 0.0046\n",
      "Epoch 6376:\tTraining Loss - 0.0017\n",
      "Epoch 6377:\tTraining Loss - 0.0023\n",
      "Epoch 6378:\tTraining Loss - 0.0045\n",
      "Epoch 6379:\tTraining Loss - 0.0056\n",
      "Epoch 6380:\tTraining Loss - 0.0026\n",
      "Epoch 6381:\tTraining Loss - 0.0053\n",
      "Epoch 6382:\tTraining Loss - 0.0039\n",
      "Epoch 6383:\tTraining Loss - 0.0038\n",
      "Epoch 6384:\tTraining Loss - 0.0037\n",
      "Epoch 6385:\tTraining Loss - 0.0034\n",
      "Epoch 6386:\tTraining Loss - 0.0046\n",
      "Epoch 6387:\tTraining Loss - 0.0039\n",
      "Epoch 6388:\tTraining Loss - 0.0050\n",
      "Epoch 6389:\tTraining Loss - 0.0097\n",
      "Epoch 6390:\tTraining Loss - 0.0031\n",
      "Epoch 6391:\tTraining Loss - 0.0030\n",
      "Epoch 6392:\tTraining Loss - 0.0061\n",
      "Epoch 6393:\tTraining Loss - 0.0025\n",
      "Epoch 6394:\tTraining Loss - 0.0023\n",
      "Epoch 6395:\tTraining Loss - 0.0040\n",
      "Epoch 6396:\tTraining Loss - 0.0072\n",
      "Epoch 6397:\tTraining Loss - 0.0022\n",
      "Epoch 6398:\tTraining Loss - 0.0040\n",
      "Epoch 6399:\tTraining Loss - 0.0037\n",
      "Epoch 6400:\tTraining Loss - 0.0056\n",
      "Epoch 6401:\tTraining Loss - 0.0040\n",
      "Epoch 6402:\tTraining Loss - 0.0033\n",
      "Epoch 6403:\tTraining Loss - 0.0050\n",
      "Epoch 6404:\tTraining Loss - 0.0043\n",
      "Epoch 6405:\tTraining Loss - 0.0050\n",
      "Epoch 6406:\tTraining Loss - 0.0111\n",
      "Epoch 6407:\tTraining Loss - 0.0118\n",
      "Epoch 6408:\tTraining Loss - 0.0069\n",
      "Epoch 6409:\tTraining Loss - 0.0029\n",
      "Epoch 6410:\tTraining Loss - 0.0079\n",
      "Epoch 6411:\tTraining Loss - 0.0039\n",
      "Epoch 6412:\tTraining Loss - 0.0047\n",
      "Epoch 6413:\tTraining Loss - 0.0049\n",
      "Epoch 6414:\tTraining Loss - 0.0043\n",
      "Epoch 6415:\tTraining Loss - 0.0024\n",
      "Epoch 6416:\tTraining Loss - 0.0101\n",
      "Epoch 6417:\tTraining Loss - 0.0058\n",
      "Epoch 6418:\tTraining Loss - 0.0051\n",
      "Epoch 6419:\tTraining Loss - 0.0057\n",
      "Epoch 6420:\tTraining Loss - 0.0043\n",
      "Epoch 6421:\tTraining Loss - 0.0081\n",
      "Epoch 6422:\tTraining Loss - 0.0032\n",
      "Epoch 6423:\tTraining Loss - 0.0042\n",
      "Epoch 6424:\tTraining Loss - 0.0077\n",
      "Epoch 6425:\tTraining Loss - 0.0056\n",
      "Epoch 6426:\tTraining Loss - 0.0075\n",
      "Epoch 6427:\tTraining Loss - 0.0065\n",
      "Epoch 6428:\tTraining Loss - 0.0055\n",
      "Epoch 6429:\tTraining Loss - 0.0029\n",
      "Epoch 6430:\tTraining Loss - 0.0086\n",
      "Epoch 6431:\tTraining Loss - 0.0028\n",
      "Epoch 6432:\tTraining Loss - 0.0027\n",
      "Epoch 6433:\tTraining Loss - 0.0036\n",
      "Epoch 6434:\tTraining Loss - 0.0061\n",
      "Epoch 6435:\tTraining Loss - 0.0100\n",
      "Epoch 6436:\tTraining Loss - 0.0084\n",
      "Epoch 6437:\tTraining Loss - 0.0035\n",
      "Epoch 6438:\tTraining Loss - 0.0040\n",
      "Epoch 6439:\tTraining Loss - 0.0091\n",
      "Epoch 6440:\tTraining Loss - 0.0041\n",
      "Epoch 6441:\tTraining Loss - 0.0060\n",
      "Epoch 6442:\tTraining Loss - 0.0057\n",
      "Epoch 6443:\tTraining Loss - 0.0042\n",
      "Epoch 6444:\tTraining Loss - 0.0049\n",
      "Epoch 6445:\tTraining Loss - 0.0059\n",
      "Epoch 6446:\tTraining Loss - 0.0063\n",
      "Epoch 6447:\tTraining Loss - 0.0029\n",
      "Epoch 6448:\tTraining Loss - 0.0037\n",
      "Epoch 6449:\tTraining Loss - 0.0049\n",
      "Epoch 6450:\tTraining Loss - 0.0047\n",
      "Epoch 6451:\tTraining Loss - 0.0067\n",
      "Epoch 6452:\tTraining Loss - 0.0091\n",
      "Epoch 6453:\tTraining Loss - 0.0064\n",
      "Epoch 6454:\tTraining Loss - 0.0081\n",
      "Epoch 6455:\tTraining Loss - 0.0050\n",
      "Epoch 6456:\tTraining Loss - 0.0044\n",
      "Epoch 6457:\tTraining Loss - 0.0038\n",
      "Epoch 6458:\tTraining Loss - 0.0070\n",
      "Epoch 6459:\tTraining Loss - 0.0072\n",
      "Epoch 6460:\tTraining Loss - 0.0035\n",
      "Epoch 6461:\tTraining Loss - 0.0054\n",
      "Epoch 6462:\tTraining Loss - 0.0059\n",
      "Epoch 6463:\tTraining Loss - 0.0072\n",
      "Epoch 6464:\tTraining Loss - 0.0064\n",
      "Epoch 6465:\tTraining Loss - 0.0038\n",
      "Epoch 6466:\tTraining Loss - 0.0066\n",
      "Epoch 6467:\tTraining Loss - 0.0026\n",
      "Epoch 6468:\tTraining Loss - 0.0039\n",
      "Epoch 6469:\tTraining Loss - 0.0028\n",
      "Epoch 6470:\tTraining Loss - 0.0110\n",
      "Epoch 6471:\tTraining Loss - 0.0064\n",
      "Epoch 6472:\tTraining Loss - 0.0045\n",
      "Epoch 6473:\tTraining Loss - 0.0065\n",
      "Epoch 6474:\tTraining Loss - 0.0046\n",
      "Epoch 6475:\tTraining Loss - 0.0043\n",
      "Epoch 6476:\tTraining Loss - 0.0038\n",
      "Epoch 6477:\tTraining Loss - 0.0058\n",
      "Epoch 6478:\tTraining Loss - 0.0058\n",
      "Epoch 6479:\tTraining Loss - 0.0034\n",
      "Epoch 6480:\tTraining Loss - 0.0034\n",
      "Epoch 6481:\tTraining Loss - 0.0029\n",
      "Epoch 6482:\tTraining Loss - 0.0075\n",
      "Epoch 6483:\tTraining Loss - 0.0091\n",
      "Epoch 6484:\tTraining Loss - 0.0029\n",
      "Epoch 6485:\tTraining Loss - 0.0054\n",
      "Epoch 6486:\tTraining Loss - 0.0072\n",
      "Epoch 6487:\tTraining Loss - 0.0076\n",
      "Epoch 6488:\tTraining Loss - 0.0047\n",
      "Epoch 6489:\tTraining Loss - 0.0051\n",
      "Epoch 6490:\tTraining Loss - 0.0105\n",
      "Epoch 6491:\tTraining Loss - 0.0065\n",
      "Epoch 6492:\tTraining Loss - 0.0048\n",
      "Epoch 6493:\tTraining Loss - 0.0056\n",
      "Epoch 6494:\tTraining Loss - 0.0088\n",
      "Epoch 6495:\tTraining Loss - 0.0035\n",
      "Epoch 6496:\tTraining Loss - 0.0032\n",
      "Epoch 6497:\tTraining Loss - 0.0041\n",
      "Epoch 6498:\tTraining Loss - 0.0067\n",
      "Epoch 6499:\tTraining Loss - 0.0057\n",
      "Epoch 6500:\tTraining Loss - 0.0051\n",
      "Epoch 6501:\tTraining Loss - 0.0057\n",
      "Epoch 6502:\tTraining Loss - 0.0057\n",
      "Epoch 6503:\tTraining Loss - 0.0049\n",
      "Epoch 6504:\tTraining Loss - 0.0025\n",
      "Epoch 6505:\tTraining Loss - 0.0068\n",
      "Epoch 6506:\tTraining Loss - 0.0038\n",
      "Epoch 6507:\tTraining Loss - 0.0032\n",
      "Epoch 6508:\tTraining Loss - 0.0032\n",
      "Epoch 6509:\tTraining Loss - 0.0042\n",
      "Epoch 6510:\tTraining Loss - 0.0055\n",
      "Epoch 6511:\tTraining Loss - 0.0064\n",
      "Epoch 6512:\tTraining Loss - 0.0040\n",
      "Epoch 6513:\tTraining Loss - 0.0024\n",
      "Epoch 6514:\tTraining Loss - 0.0046\n",
      "Epoch 6515:\tTraining Loss - 0.0057\n",
      "Epoch 6516:\tTraining Loss - 0.0035\n",
      "Epoch 6517:\tTraining Loss - 0.0069\n",
      "Epoch 6518:\tTraining Loss - 0.0056\n",
      "Epoch 6519:\tTraining Loss - 0.0030\n",
      "Epoch 6520:\tTraining Loss - 0.0075\n",
      "Epoch 6521:\tTraining Loss - 0.0057\n",
      "Epoch 6522:\tTraining Loss - 0.0043\n",
      "Epoch 6523:\tTraining Loss - 0.0083\n",
      "Epoch 6524:\tTraining Loss - 0.0051\n",
      "Epoch 6525:\tTraining Loss - 0.0038\n",
      "Epoch 6526:\tTraining Loss - 0.0079\n",
      "Epoch 6527:\tTraining Loss - 0.0045\n",
      "Epoch 6528:\tTraining Loss - 0.0043\n",
      "Epoch 6529:\tTraining Loss - 0.0038\n",
      "Epoch 6530:\tTraining Loss - 0.0039\n",
      "Epoch 6531:\tTraining Loss - 0.0083\n",
      "Epoch 6532:\tTraining Loss - 0.0031\n",
      "Epoch 6533:\tTraining Loss - 0.0033\n",
      "Epoch 6534:\tTraining Loss - 0.0028\n",
      "Epoch 6535:\tTraining Loss - 0.0026\n",
      "Epoch 6536:\tTraining Loss - 0.0058\n",
      "Epoch 6537:\tTraining Loss - 0.0037\n",
      "Epoch 6538:\tTraining Loss - 0.0044\n",
      "Epoch 6539:\tTraining Loss - 0.0083\n",
      "Epoch 6540:\tTraining Loss - 0.0045\n",
      "Epoch 6541:\tTraining Loss - 0.0033\n",
      "Epoch 6542:\tTraining Loss - 0.0032\n",
      "Epoch 6543:\tTraining Loss - 0.0046\n",
      "Epoch 6544:\tTraining Loss - 0.0045\n",
      "Epoch 6545:\tTraining Loss - 0.0052\n",
      "Epoch 6546:\tTraining Loss - 0.0058\n",
      "Epoch 6547:\tTraining Loss - 0.0054\n",
      "Epoch 6548:\tTraining Loss - 0.0068\n",
      "Epoch 6549:\tTraining Loss - 0.0053\n",
      "Epoch 6550:\tTraining Loss - 0.0091\n",
      "Epoch 6551:\tTraining Loss - 0.0056\n",
      "Epoch 6552:\tTraining Loss - 0.0040\n",
      "Epoch 6553:\tTraining Loss - 0.0070\n",
      "Epoch 6554:\tTraining Loss - 0.0035\n",
      "Epoch 6555:\tTraining Loss - 0.0028\n",
      "Epoch 6556:\tTraining Loss - 0.0032\n",
      "Epoch 6557:\tTraining Loss - 0.0021\n",
      "Epoch 6558:\tTraining Loss - 0.0044\n",
      "Epoch 6559:\tTraining Loss - 0.0070\n",
      "Epoch 6560:\tTraining Loss - 0.0039\n",
      "Epoch 6561:\tTraining Loss - 0.0040\n",
      "Epoch 6562:\tTraining Loss - 0.0063\n",
      "Epoch 6563:\tTraining Loss - 0.0036\n",
      "Epoch 6564:\tTraining Loss - 0.0024\n",
      "Epoch 6565:\tTraining Loss - 0.0027\n",
      "Epoch 6566:\tTraining Loss - 0.0030\n",
      "Epoch 6567:\tTraining Loss - 0.0037\n",
      "Epoch 6568:\tTraining Loss - 0.0060\n",
      "Epoch 6569:\tTraining Loss - 0.0032\n",
      "Epoch 6570:\tTraining Loss - 0.0050\n",
      "Epoch 6571:\tTraining Loss - 0.0054\n",
      "Epoch 6572:\tTraining Loss - 0.0024\n",
      "Epoch 6573:\tTraining Loss - 0.0020\n",
      "Epoch 6574:\tTraining Loss - 0.0029\n",
      "Epoch 6575:\tTraining Loss - 0.0064\n",
      "Epoch 6576:\tTraining Loss - 0.0036\n",
      "Epoch 6577:\tTraining Loss - 0.0065\n",
      "Epoch 6578:\tTraining Loss - 0.0061\n",
      "Epoch 6579:\tTraining Loss - 0.0052\n",
      "Epoch 6580:\tTraining Loss - 0.0047\n",
      "Epoch 6581:\tTraining Loss - 0.0034\n",
      "Epoch 6582:\tTraining Loss - 0.0035\n",
      "Epoch 6583:\tTraining Loss - 0.0045\n",
      "Epoch 6584:\tTraining Loss - 0.0083\n",
      "Epoch 6585:\tTraining Loss - 0.0084\n",
      "Epoch 6586:\tTraining Loss - 0.0032\n",
      "Epoch 6587:\tTraining Loss - 0.0046\n",
      "Epoch 6588:\tTraining Loss - 0.0019\n",
      "Epoch 6589:\tTraining Loss - 0.0032\n",
      "Epoch 6590:\tTraining Loss - 0.0035\n",
      "Epoch 6591:\tTraining Loss - 0.0053\n",
      "Epoch 6592:\tTraining Loss - 0.0031\n",
      "Epoch 6593:\tTraining Loss - 0.0059\n",
      "Epoch 6594:\tTraining Loss - 0.0037\n",
      "Epoch 6595:\tTraining Loss - 0.0075\n",
      "Epoch 6596:\tTraining Loss - 0.0059\n",
      "Epoch 6597:\tTraining Loss - 0.0025\n",
      "Epoch 6598:\tTraining Loss - 0.0026\n",
      "Epoch 6599:\tTraining Loss - 0.0039\n",
      "Epoch 6600:\tTraining Loss - 0.0031\n",
      "Epoch 6601:\tTraining Loss - 0.0054\n",
      "Epoch 6602:\tTraining Loss - 0.0030\n",
      "Epoch 6603:\tTraining Loss - 0.0030\n",
      "Epoch 6604:\tTraining Loss - 0.0030\n",
      "Epoch 6605:\tTraining Loss - 0.0043\n",
      "Epoch 6606:\tTraining Loss - 0.0041\n",
      "Epoch 6607:\tTraining Loss - 0.0072\n",
      "Epoch 6608:\tTraining Loss - 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6609:\tTraining Loss - 0.0041\n",
      "Epoch 6610:\tTraining Loss - 0.0057\n",
      "Epoch 6611:\tTraining Loss - 0.0064\n",
      "Epoch 6612:\tTraining Loss - 0.0053\n",
      "Epoch 6613:\tTraining Loss - 0.0037\n",
      "Epoch 6614:\tTraining Loss - 0.0120\n",
      "Epoch 6615:\tTraining Loss - 0.0048\n",
      "Epoch 6616:\tTraining Loss - 0.0062\n",
      "Epoch 6617:\tTraining Loss - 0.0016\n",
      "Epoch 6618:\tTraining Loss - 0.0036\n",
      "Epoch 6619:\tTraining Loss - 0.0050\n",
      "Epoch 6620:\tTraining Loss - 0.0085\n",
      "Epoch 6621:\tTraining Loss - 0.0045\n",
      "Epoch 6622:\tTraining Loss - 0.0046\n",
      "Epoch 6623:\tTraining Loss - 0.0043\n",
      "Epoch 6624:\tTraining Loss - 0.0050\n",
      "Epoch 6625:\tTraining Loss - 0.0046\n",
      "Epoch 6626:\tTraining Loss - 0.0033\n",
      "Epoch 6627:\tTraining Loss - 0.0034\n",
      "Epoch 6628:\tTraining Loss - 0.0045\n",
      "Epoch 6629:\tTraining Loss - 0.0050\n",
      "Epoch 6630:\tTraining Loss - 0.0027\n",
      "Epoch 6631:\tTraining Loss - 0.0108\n",
      "Epoch 6632:\tTraining Loss - 0.0024\n",
      "Epoch 6633:\tTraining Loss - 0.0034\n",
      "Epoch 6634:\tTraining Loss - 0.0028\n",
      "Epoch 6635:\tTraining Loss - 0.0055\n",
      "Epoch 6636:\tTraining Loss - 0.0050\n",
      "Epoch 6637:\tTraining Loss - 0.0070\n",
      "Epoch 6638:\tTraining Loss - 0.0028\n",
      "Epoch 6639:\tTraining Loss - 0.0039\n",
      "Epoch 6640:\tTraining Loss - 0.0059\n",
      "Epoch 6641:\tTraining Loss - 0.0040\n",
      "Epoch 6642:\tTraining Loss - 0.0040\n",
      "Epoch 6643:\tTraining Loss - 0.0027\n",
      "Epoch 6644:\tTraining Loss - 0.0043\n",
      "Epoch 6645:\tTraining Loss - 0.0028\n",
      "Epoch 6646:\tTraining Loss - 0.0044\n",
      "Epoch 6647:\tTraining Loss - 0.0055\n",
      "Epoch 6648:\tTraining Loss - 0.0036\n",
      "Epoch 6649:\tTraining Loss - 0.0059\n",
      "Epoch 6650:\tTraining Loss - 0.0050\n",
      "Epoch 6651:\tTraining Loss - 0.0041\n",
      "Epoch 6652:\tTraining Loss - 0.0020\n",
      "Epoch 6653:\tTraining Loss - 0.0019\n",
      "Epoch 6654:\tTraining Loss - 0.0046\n",
      "Epoch 6655:\tTraining Loss - 0.0029\n",
      "Epoch 6656:\tTraining Loss - 0.0032\n",
      "Epoch 6657:\tTraining Loss - 0.0015\n",
      "Epoch 6658:\tTraining Loss - 0.0033\n",
      "Epoch 6659:\tTraining Loss - 0.0061\n",
      "Epoch 6660:\tTraining Loss - 0.0052\n",
      "Epoch 6661:\tTraining Loss - 0.0051\n",
      "Epoch 6662:\tTraining Loss - 0.0034\n",
      "Epoch 6663:\tTraining Loss - 0.0048\n",
      "Epoch 6664:\tTraining Loss - 0.0023\n",
      "Epoch 6665:\tTraining Loss - 0.0019\n",
      "Epoch 6666:\tTraining Loss - 0.0050\n",
      "Epoch 6667:\tTraining Loss - 0.0067\n",
      "Epoch 6668:\tTraining Loss - 0.0030\n",
      "Epoch 6669:\tTraining Loss - 0.0013\n",
      "Epoch 6670:\tTraining Loss - 0.0028\n",
      "Epoch 6671:\tTraining Loss - 0.0113\n",
      "Epoch 6672:\tTraining Loss - 0.0037\n",
      "Epoch 6673:\tTraining Loss - 0.0039\n",
      "Epoch 6674:\tTraining Loss - 0.0039\n",
      "Epoch 6675:\tTraining Loss - 0.0032\n",
      "Epoch 6676:\tTraining Loss - 0.0054\n",
      "Epoch 6677:\tTraining Loss - 0.0031\n",
      "Epoch 6678:\tTraining Loss - 0.0061\n",
      "Epoch 6679:\tTraining Loss - 0.0033\n",
      "Epoch 6680:\tTraining Loss - 0.0016\n",
      "Epoch 6681:\tTraining Loss - 0.0073\n",
      "Epoch 6682:\tTraining Loss - 0.0029\n",
      "Epoch 6683:\tTraining Loss - 0.0027\n",
      "Epoch 6684:\tTraining Loss - 0.0064\n",
      "Epoch 6685:\tTraining Loss - 0.0029\n",
      "Epoch 6686:\tTraining Loss - 0.0020\n",
      "Epoch 6687:\tTraining Loss - 0.0047\n",
      "Epoch 6688:\tTraining Loss - 0.0070\n",
      "Epoch 6689:\tTraining Loss - 0.0019\n",
      "Epoch 6690:\tTraining Loss - 0.0066\n",
      "Epoch 6691:\tTraining Loss - 0.0059\n",
      "Epoch 6692:\tTraining Loss - 0.0017\n",
      "Epoch 6693:\tTraining Loss - 0.0061\n",
      "Epoch 6694:\tTraining Loss - 0.0029\n",
      "Epoch 6695:\tTraining Loss - 0.0025\n",
      "Epoch 6696:\tTraining Loss - 0.0068\n",
      "Epoch 6697:\tTraining Loss - 0.0038\n",
      "Epoch 6698:\tTraining Loss - 0.0052\n",
      "Epoch 6699:\tTraining Loss - 0.0026\n",
      "Epoch 6700:\tTraining Loss - 0.0077\n",
      "Epoch 6701:\tTraining Loss - 0.0049\n",
      "Epoch 6702:\tTraining Loss - 0.0075\n",
      "Epoch 6703:\tTraining Loss - 0.0082\n",
      "Epoch 6704:\tTraining Loss - 0.0070\n",
      "Epoch 6705:\tTraining Loss - 0.0099\n",
      "Epoch 6706:\tTraining Loss - 0.0029\n",
      "Epoch 6707:\tTraining Loss - 0.0042\n",
      "Epoch 6708:\tTraining Loss - 0.0056\n",
      "Epoch 6709:\tTraining Loss - 0.0051\n",
      "Epoch 6710:\tTraining Loss - 0.0024\n",
      "Epoch 6711:\tTraining Loss - 0.0040\n",
      "Epoch 6712:\tTraining Loss - 0.0042\n",
      "Epoch 6713:\tTraining Loss - 0.0032\n",
      "Epoch 6714:\tTraining Loss - 0.0086\n",
      "Epoch 6715:\tTraining Loss - 0.0052\n",
      "Epoch 6716:\tTraining Loss - 0.0039\n",
      "Epoch 6717:\tTraining Loss - 0.0099\n",
      "Epoch 6718:\tTraining Loss - 0.0048\n",
      "Epoch 6719:\tTraining Loss - 0.0090\n",
      "Epoch 6720:\tTraining Loss - 0.0023\n",
      "Epoch 6721:\tTraining Loss - 0.0073\n",
      "Epoch 6722:\tTraining Loss - 0.0054\n",
      "Epoch 6723:\tTraining Loss - 0.0031\n",
      "Epoch 6724:\tTraining Loss - 0.0020\n",
      "Epoch 6725:\tTraining Loss - 0.0054\n",
      "Epoch 6726:\tTraining Loss - 0.0058\n",
      "Epoch 6727:\tTraining Loss - 0.0048\n",
      "Epoch 6728:\tTraining Loss - 0.0033\n",
      "Epoch 6729:\tTraining Loss - 0.0065\n",
      "Epoch 6730:\tTraining Loss - 0.0058\n",
      "Epoch 6731:\tTraining Loss - 0.0094\n",
      "Epoch 6732:\tTraining Loss - 0.0044\n",
      "Epoch 6733:\tTraining Loss - 0.0041\n",
      "Epoch 6734:\tTraining Loss - 0.0034\n",
      "Epoch 6735:\tTraining Loss - 0.0043\n",
      "Epoch 6736:\tTraining Loss - 0.0041\n",
      "Epoch 6737:\tTraining Loss - 0.0058\n",
      "Epoch 6738:\tTraining Loss - 0.0047\n",
      "Epoch 6739:\tTraining Loss - 0.0061\n",
      "Epoch 6740:\tTraining Loss - 0.0066\n",
      "Epoch 6741:\tTraining Loss - 0.0052\n",
      "Epoch 6742:\tTraining Loss - 0.0031\n",
      "Epoch 6743:\tTraining Loss - 0.0054\n",
      "Epoch 6744:\tTraining Loss - 0.0031\n",
      "Epoch 6745:\tTraining Loss - 0.0028\n",
      "Epoch 6746:\tTraining Loss - 0.0049\n",
      "Epoch 6747:\tTraining Loss - 0.0037\n",
      "Epoch 6748:\tTraining Loss - 0.0044\n",
      "Epoch 6749:\tTraining Loss - 0.0035\n",
      "Epoch 6750:\tTraining Loss - 0.0039\n",
      "Epoch 6751:\tTraining Loss - 0.0054\n",
      "Epoch 6752:\tTraining Loss - 0.0052\n",
      "Epoch 6753:\tTraining Loss - 0.0045\n",
      "Epoch 6754:\tTraining Loss - 0.0072\n",
      "Epoch 6755:\tTraining Loss - 0.0075\n",
      "Epoch 6756:\tTraining Loss - 0.0034\n",
      "Epoch 6757:\tTraining Loss - 0.0031\n",
      "Epoch 6758:\tTraining Loss - 0.0046\n",
      "Epoch 6759:\tTraining Loss - 0.0043\n",
      "Epoch 6760:\tTraining Loss - 0.0024\n",
      "Epoch 6761:\tTraining Loss - 0.0067\n",
      "Epoch 6762:\tTraining Loss - 0.0030\n",
      "Epoch 6763:\tTraining Loss - 0.0059\n",
      "Epoch 6764:\tTraining Loss - 0.0040\n",
      "Epoch 6765:\tTraining Loss - 0.0049\n",
      "Epoch 6766:\tTraining Loss - 0.0052\n",
      "Epoch 6767:\tTraining Loss - 0.0051\n",
      "Epoch 6768:\tTraining Loss - 0.0046\n",
      "Epoch 6769:\tTraining Loss - 0.0034\n",
      "Epoch 6770:\tTraining Loss - 0.0028\n",
      "Epoch 6771:\tTraining Loss - 0.0049\n",
      "Epoch 6772:\tTraining Loss - 0.0033\n",
      "Epoch 6773:\tTraining Loss - 0.0029\n",
      "Epoch 6774:\tTraining Loss - 0.0085\n",
      "Epoch 6775:\tTraining Loss - 0.0074\n",
      "Epoch 6776:\tTraining Loss - 0.0059\n",
      "Epoch 6777:\tTraining Loss - 0.0033\n",
      "Epoch 6778:\tTraining Loss - 0.0042\n",
      "Epoch 6779:\tTraining Loss - 0.0015\n",
      "Epoch 6780:\tTraining Loss - 0.0022\n",
      "Epoch 6781:\tTraining Loss - 0.0035\n",
      "Epoch 6782:\tTraining Loss - 0.0055\n",
      "Epoch 6783:\tTraining Loss - 0.0049\n",
      "Epoch 6784:\tTraining Loss - 0.0023\n",
      "Epoch 6785:\tTraining Loss - 0.0021\n",
      "Epoch 6786:\tTraining Loss - 0.0059\n",
      "Epoch 6787:\tTraining Loss - 0.0041\n",
      "Epoch 6788:\tTraining Loss - 0.0050\n",
      "Epoch 6789:\tTraining Loss - 0.0045\n",
      "Epoch 6790:\tTraining Loss - 0.0066\n",
      "Epoch 6791:\tTraining Loss - 0.0048\n",
      "Epoch 6792:\tTraining Loss - 0.0064\n",
      "Epoch 6793:\tTraining Loss - 0.0042\n",
      "Epoch 6794:\tTraining Loss - 0.0031\n",
      "Epoch 6795:\tTraining Loss - 0.0048\n",
      "Epoch 6796:\tTraining Loss - 0.0041\n",
      "Epoch 6797:\tTraining Loss - 0.0101\n",
      "Epoch 6798:\tTraining Loss - 0.0041\n",
      "Epoch 6799:\tTraining Loss - 0.0037\n",
      "Epoch 6800:\tTraining Loss - 0.0048\n",
      "Epoch 6801:\tTraining Loss - 0.0033\n",
      "Epoch 6802:\tTraining Loss - 0.0077\n",
      "Epoch 6803:\tTraining Loss - 0.0023\n",
      "Epoch 6804:\tTraining Loss - 0.0073\n",
      "Epoch 6805:\tTraining Loss - 0.0060\n",
      "Epoch 6806:\tTraining Loss - 0.0040\n",
      "Epoch 6807:\tTraining Loss - 0.0035\n",
      "Epoch 6808:\tTraining Loss - 0.0036\n",
      "Epoch 6809:\tTraining Loss - 0.0054\n",
      "Epoch 6810:\tTraining Loss - 0.0051\n",
      "Epoch 6811:\tTraining Loss - 0.0054\n",
      "Epoch 6812:\tTraining Loss - 0.0030\n",
      "Epoch 6813:\tTraining Loss - 0.0078\n",
      "Epoch 6814:\tTraining Loss - 0.0033\n",
      "Epoch 6815:\tTraining Loss - 0.0055\n",
      "Epoch 6816:\tTraining Loss - 0.0044\n",
      "Epoch 6817:\tTraining Loss - 0.0034\n",
      "Epoch 6818:\tTraining Loss - 0.0056\n",
      "Epoch 6819:\tTraining Loss - 0.0036\n",
      "Epoch 6820:\tTraining Loss - 0.0117\n",
      "Epoch 6821:\tTraining Loss - 0.0106\n",
      "Epoch 6822:\tTraining Loss - 0.0023\n",
      "Epoch 6823:\tTraining Loss - 0.0089\n",
      "Epoch 6824:\tTraining Loss - 0.0030\n",
      "Epoch 6825:\tTraining Loss - 0.0021\n",
      "Epoch 6826:\tTraining Loss - 0.0048\n",
      "Epoch 6827:\tTraining Loss - 0.0104\n",
      "Epoch 6828:\tTraining Loss - 0.0054\n",
      "Epoch 6829:\tTraining Loss - 0.0027\n",
      "Epoch 6830:\tTraining Loss - 0.0043\n",
      "Epoch 6831:\tTraining Loss - 0.0071\n",
      "Epoch 6832:\tTraining Loss - 0.0018\n",
      "Epoch 6833:\tTraining Loss - 0.0071\n",
      "Epoch 6834:\tTraining Loss - 0.0027\n",
      "Epoch 6835:\tTraining Loss - 0.0076\n",
      "Epoch 6836:\tTraining Loss - 0.0058\n",
      "Epoch 6837:\tTraining Loss - 0.0072\n",
      "Epoch 6838:\tTraining Loss - 0.0060\n",
      "Epoch 6839:\tTraining Loss - 0.0032\n",
      "Epoch 6840:\tTraining Loss - 0.0022\n",
      "Epoch 6841:\tTraining Loss - 0.0063\n",
      "Epoch 6842:\tTraining Loss - 0.0039\n",
      "Epoch 6843:\tTraining Loss - 0.0041\n",
      "Epoch 6844:\tTraining Loss - 0.0034\n",
      "Epoch 6845:\tTraining Loss - 0.0053\n",
      "Epoch 6846:\tTraining Loss - 0.0033\n",
      "Epoch 6847:\tTraining Loss - 0.0046\n",
      "Epoch 6848:\tTraining Loss - 0.0087\n",
      "Epoch 6849:\tTraining Loss - 0.0072\n",
      "Epoch 6850:\tTraining Loss - 0.0048\n",
      "Epoch 6851:\tTraining Loss - 0.0043\n",
      "Epoch 6852:\tTraining Loss - 0.0054\n",
      "Epoch 6853:\tTraining Loss - 0.0071\n",
      "Epoch 6854:\tTraining Loss - 0.0043\n",
      "Epoch 6855:\tTraining Loss - 0.0027\n",
      "Epoch 6856:\tTraining Loss - 0.0047\n",
      "Epoch 6857:\tTraining Loss - 0.0092\n",
      "Epoch 6858:\tTraining Loss - 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6859:\tTraining Loss - 0.0038\n",
      "Epoch 6860:\tTraining Loss - 0.0028\n",
      "Epoch 6861:\tTraining Loss - 0.0101\n",
      "Epoch 6862:\tTraining Loss - 0.0089\n",
      "Epoch 6863:\tTraining Loss - 0.0067\n",
      "Epoch 6864:\tTraining Loss - 0.0052\n",
      "Epoch 6865:\tTraining Loss - 0.0042\n",
      "Epoch 6866:\tTraining Loss - 0.0055\n",
      "Epoch 6867:\tTraining Loss - 0.0043\n",
      "Epoch 6868:\tTraining Loss - 0.0020\n",
      "Epoch 6869:\tTraining Loss - 0.0057\n",
      "Epoch 6870:\tTraining Loss - 0.0034\n",
      "Epoch 6871:\tTraining Loss - 0.0038\n",
      "Epoch 6872:\tTraining Loss - 0.0046\n",
      "Epoch 6873:\tTraining Loss - 0.0069\n",
      "Epoch 6874:\tTraining Loss - 0.0054\n",
      "Epoch 6875:\tTraining Loss - 0.0059\n",
      "Epoch 6876:\tTraining Loss - 0.0044\n",
      "Epoch 6877:\tTraining Loss - 0.0033\n",
      "Epoch 6878:\tTraining Loss - 0.0036\n",
      "Epoch 6879:\tTraining Loss - 0.0045\n",
      "Epoch 6880:\tTraining Loss - 0.0043\n",
      "Epoch 6881:\tTraining Loss - 0.0039\n",
      "Epoch 6882:\tTraining Loss - 0.0033\n",
      "Epoch 6883:\tTraining Loss - 0.0046\n",
      "Epoch 6884:\tTraining Loss - 0.0028\n",
      "Epoch 6885:\tTraining Loss - 0.0029\n",
      "Epoch 6886:\tTraining Loss - 0.0030\n",
      "Epoch 6887:\tTraining Loss - 0.0042\n",
      "Epoch 6888:\tTraining Loss - 0.0074\n",
      "Epoch 6889:\tTraining Loss - 0.0038\n",
      "Epoch 6890:\tTraining Loss - 0.0044\n",
      "Epoch 6891:\tTraining Loss - 0.0058\n",
      "Epoch 6892:\tTraining Loss - 0.0044\n",
      "Epoch 6893:\tTraining Loss - 0.0027\n",
      "Epoch 6894:\tTraining Loss - 0.0074\n",
      "Epoch 6895:\tTraining Loss - 0.0040\n",
      "Epoch 6896:\tTraining Loss - 0.0030\n",
      "Epoch 6897:\tTraining Loss - 0.0041\n",
      "Epoch 6898:\tTraining Loss - 0.0032\n",
      "Epoch 6899:\tTraining Loss - 0.0037\n",
      "Epoch 6900:\tTraining Loss - 0.0053\n",
      "Epoch 6901:\tTraining Loss - 0.0040\n",
      "Epoch 6902:\tTraining Loss - 0.0049\n",
      "Epoch 6903:\tTraining Loss - 0.0085\n",
      "Epoch 6904:\tTraining Loss - 0.0026\n",
      "Epoch 6905:\tTraining Loss - 0.0039\n",
      "Epoch 6906:\tTraining Loss - 0.0041\n",
      "Epoch 6907:\tTraining Loss - 0.0079\n",
      "Epoch 6908:\tTraining Loss - 0.0040\n",
      "Epoch 6909:\tTraining Loss - 0.0013\n",
      "Epoch 6910:\tTraining Loss - 0.0028\n",
      "Epoch 6911:\tTraining Loss - 0.0042\n",
      "Epoch 6912:\tTraining Loss - 0.0070\n",
      "Epoch 6913:\tTraining Loss - 0.0064\n",
      "Epoch 6914:\tTraining Loss - 0.0042\n",
      "Epoch 6915:\tTraining Loss - 0.0025\n",
      "Epoch 6916:\tTraining Loss - 0.0040\n",
      "Epoch 6917:\tTraining Loss - 0.0065\n",
      "Epoch 6918:\tTraining Loss - 0.0033\n",
      "Epoch 6919:\tTraining Loss - 0.0038\n",
      "Epoch 6920:\tTraining Loss - 0.0061\n",
      "Epoch 6921:\tTraining Loss - 0.0022\n",
      "Epoch 6922:\tTraining Loss - 0.0054\n",
      "Epoch 6923:\tTraining Loss - 0.0060\n",
      "Epoch 6924:\tTraining Loss - 0.0058\n",
      "Epoch 6925:\tTraining Loss - 0.0064\n",
      "Epoch 6926:\tTraining Loss - 0.0072\n",
      "Epoch 6927:\tTraining Loss - 0.0045\n",
      "Epoch 6928:\tTraining Loss - 0.0044\n",
      "Epoch 6929:\tTraining Loss - 0.0020\n",
      "Epoch 6930:\tTraining Loss - 0.0070\n",
      "Epoch 6931:\tTraining Loss - 0.0021\n",
      "Epoch 6932:\tTraining Loss - 0.0061\n",
      "Epoch 6933:\tTraining Loss - 0.0042\n",
      "Epoch 6934:\tTraining Loss - 0.0034\n",
      "Epoch 6935:\tTraining Loss - 0.0073\n",
      "Epoch 6936:\tTraining Loss - 0.0065\n",
      "Epoch 6937:\tTraining Loss - 0.0037\n",
      "Epoch 6938:\tTraining Loss - 0.0040\n",
      "Epoch 6939:\tTraining Loss - 0.0044\n",
      "Epoch 6940:\tTraining Loss - 0.0018\n",
      "Epoch 6941:\tTraining Loss - 0.0015\n",
      "Epoch 6942:\tTraining Loss - 0.0048\n",
      "Epoch 6943:\tTraining Loss - 0.0024\n",
      "Epoch 6944:\tTraining Loss - 0.0066\n",
      "Epoch 6945:\tTraining Loss - 0.0058\n",
      "Epoch 6946:\tTraining Loss - 0.0030\n",
      "Epoch 6947:\tTraining Loss - 0.0069\n",
      "Epoch 6948:\tTraining Loss - 0.0026\n",
      "Epoch 6949:\tTraining Loss - 0.0025\n",
      "Epoch 6950:\tTraining Loss - 0.0049\n",
      "Epoch 6951:\tTraining Loss - 0.0033\n",
      "Epoch 6952:\tTraining Loss - 0.0036\n",
      "Epoch 6953:\tTraining Loss - 0.0035\n",
      "Epoch 6954:\tTraining Loss - 0.0024\n",
      "Epoch 6955:\tTraining Loss - 0.0032\n",
      "Epoch 6956:\tTraining Loss - 0.0046\n",
      "Epoch 6957:\tTraining Loss - 0.0058\n",
      "Epoch 6958:\tTraining Loss - 0.0032\n",
      "Epoch 6959:\tTraining Loss - 0.0030\n",
      "Epoch 6960:\tTraining Loss - 0.0039\n",
      "Epoch 6961:\tTraining Loss - 0.0052\n",
      "Epoch 6962:\tTraining Loss - 0.0084\n",
      "Epoch 6963:\tTraining Loss - 0.0049\n",
      "Epoch 6964:\tTraining Loss - 0.0103\n",
      "Epoch 6965:\tTraining Loss - 0.0063\n",
      "Epoch 6966:\tTraining Loss - 0.0027\n",
      "Epoch 6967:\tTraining Loss - 0.0016\n",
      "Epoch 6968:\tTraining Loss - 0.0019\n",
      "Epoch 6969:\tTraining Loss - 0.0040\n",
      "Epoch 6970:\tTraining Loss - 0.0070\n",
      "Epoch 6971:\tTraining Loss - 0.0043\n",
      "Epoch 6972:\tTraining Loss - 0.0052\n",
      "Epoch 6973:\tTraining Loss - 0.0071\n",
      "Epoch 6974:\tTraining Loss - 0.0033\n",
      "Epoch 6975:\tTraining Loss - 0.0032\n",
      "Epoch 6976:\tTraining Loss - 0.0040\n",
      "Epoch 6977:\tTraining Loss - 0.0054\n",
      "Epoch 6978:\tTraining Loss - 0.0029\n",
      "Epoch 6979:\tTraining Loss - 0.0031\n",
      "Epoch 6980:\tTraining Loss - 0.0048\n",
      "Epoch 6981:\tTraining Loss - 0.0035\n",
      "Epoch 6982:\tTraining Loss - 0.0047\n",
      "Epoch 6983:\tTraining Loss - 0.0061\n",
      "Epoch 6984:\tTraining Loss - 0.0046\n",
      "Epoch 6985:\tTraining Loss - 0.0042\n",
      "Epoch 6986:\tTraining Loss - 0.0027\n",
      "Epoch 6987:\tTraining Loss - 0.0059\n",
      "Epoch 6988:\tTraining Loss - 0.0040\n",
      "Epoch 6989:\tTraining Loss - 0.0040\n",
      "Epoch 6990:\tTraining Loss - 0.0058\n",
      "Epoch 6991:\tTraining Loss - 0.0053\n",
      "Epoch 6992:\tTraining Loss - 0.0049\n",
      "Epoch 6993:\tTraining Loss - 0.0064\n",
      "Epoch 6994:\tTraining Loss - 0.0045\n",
      "Epoch 6995:\tTraining Loss - 0.0058\n",
      "Epoch 6996:\tTraining Loss - 0.0061\n",
      "Epoch 6997:\tTraining Loss - 0.0010\n",
      "Epoch 6998:\tTraining Loss - 0.0045\n",
      "Epoch 6999:\tTraining Loss - 0.0060\n",
      "Epoch 7000:\tTraining Loss - 0.0048\n",
      "Epoch 7001:\tTraining Loss - 0.0070\n",
      "Epoch 7002:\tTraining Loss - 0.0024\n",
      "Epoch 7003:\tTraining Loss - 0.0027\n",
      "Epoch 7004:\tTraining Loss - 0.0046\n",
      "Epoch 7005:\tTraining Loss - 0.0035\n",
      "Epoch 7006:\tTraining Loss - 0.0027\n",
      "Epoch 7007:\tTraining Loss - 0.0043\n",
      "Epoch 7008:\tTraining Loss - 0.0032\n",
      "Epoch 7009:\tTraining Loss - 0.0042\n",
      "Epoch 7010:\tTraining Loss - 0.0042\n",
      "Epoch 7011:\tTraining Loss - 0.0065\n",
      "Epoch 7012:\tTraining Loss - 0.0033\n",
      "Epoch 7013:\tTraining Loss - 0.0102\n",
      "Epoch 7014:\tTraining Loss - 0.0018\n",
      "Epoch 7015:\tTraining Loss - 0.0026\n",
      "Epoch 7016:\tTraining Loss - 0.0049\n",
      "Epoch 7017:\tTraining Loss - 0.0018\n",
      "Epoch 7018:\tTraining Loss - 0.0045\n",
      "Epoch 7019:\tTraining Loss - 0.0038\n",
      "Epoch 7020:\tTraining Loss - 0.0043\n",
      "Epoch 7021:\tTraining Loss - 0.0033\n",
      "Epoch 7022:\tTraining Loss - 0.0062\n",
      "Epoch 7023:\tTraining Loss - 0.0047\n",
      "Epoch 7024:\tTraining Loss - 0.0100\n",
      "Epoch 7025:\tTraining Loss - 0.0044\n",
      "Epoch 7026:\tTraining Loss - 0.0054\n",
      "Epoch 7027:\tTraining Loss - 0.0041\n",
      "Epoch 7028:\tTraining Loss - 0.0046\n",
      "Epoch 7029:\tTraining Loss - 0.0027\n",
      "Epoch 7030:\tTraining Loss - 0.0039\n",
      "Epoch 7031:\tTraining Loss - 0.0048\n",
      "Epoch 7032:\tTraining Loss - 0.0054\n",
      "Epoch 7033:\tTraining Loss - 0.0083\n",
      "Epoch 7034:\tTraining Loss - 0.0052\n",
      "Epoch 7035:\tTraining Loss - 0.0040\n",
      "Epoch 7036:\tTraining Loss - 0.0035\n",
      "Epoch 7037:\tTraining Loss - 0.0021\n",
      "Epoch 7038:\tTraining Loss - 0.0030\n",
      "Epoch 7039:\tTraining Loss - 0.0060\n",
      "Epoch 7040:\tTraining Loss - 0.0074\n",
      "Epoch 7041:\tTraining Loss - 0.0062\n",
      "Epoch 7042:\tTraining Loss - 0.0057\n",
      "Epoch 7043:\tTraining Loss - 0.0011\n",
      "Epoch 7044:\tTraining Loss - 0.0058\n",
      "Epoch 7045:\tTraining Loss - 0.0028\n",
      "Epoch 7046:\tTraining Loss - 0.0025\n",
      "Epoch 7047:\tTraining Loss - 0.0028\n",
      "Epoch 7048:\tTraining Loss - 0.0052\n",
      "Epoch 7049:\tTraining Loss - 0.0091\n",
      "Epoch 7050:\tTraining Loss - 0.0016\n",
      "Epoch 7051:\tTraining Loss - 0.0047\n",
      "Epoch 7052:\tTraining Loss - 0.0044\n",
      "Epoch 7053:\tTraining Loss - 0.0026\n",
      "Epoch 7054:\tTraining Loss - 0.0093\n",
      "Epoch 7055:\tTraining Loss - 0.0059\n",
      "Epoch 7056:\tTraining Loss - 0.0048\n",
      "Epoch 7057:\tTraining Loss - 0.0066\n",
      "Epoch 7058:\tTraining Loss - 0.0031\n",
      "Epoch 7059:\tTraining Loss - 0.0033\n",
      "Epoch 7060:\tTraining Loss - 0.0047\n",
      "Epoch 7061:\tTraining Loss - 0.0056\n",
      "Epoch 7062:\tTraining Loss - 0.0020\n",
      "Epoch 7063:\tTraining Loss - 0.0022\n",
      "Epoch 7064:\tTraining Loss - 0.0051\n",
      "Epoch 7065:\tTraining Loss - 0.0035\n",
      "Epoch 7066:\tTraining Loss - 0.0016\n",
      "Epoch 7067:\tTraining Loss - 0.0015\n",
      "Epoch 7068:\tTraining Loss - 0.0059\n",
      "Epoch 7069:\tTraining Loss - 0.0031\n",
      "Epoch 7070:\tTraining Loss - 0.0050\n",
      "Epoch 7071:\tTraining Loss - 0.0051\n",
      "Epoch 7072:\tTraining Loss - 0.0088\n",
      "Epoch 7073:\tTraining Loss - 0.0081\n",
      "Epoch 7074:\tTraining Loss - 0.0064\n",
      "Epoch 7075:\tTraining Loss - 0.0064\n",
      "Epoch 7076:\tTraining Loss - 0.0046\n",
      "Epoch 7077:\tTraining Loss - 0.0043\n",
      "Epoch 7078:\tTraining Loss - 0.0061\n",
      "Epoch 7079:\tTraining Loss - 0.0038\n",
      "Epoch 7080:\tTraining Loss - 0.0030\n",
      "Epoch 7081:\tTraining Loss - 0.0016\n",
      "Epoch 7082:\tTraining Loss - 0.0050\n",
      "Epoch 7083:\tTraining Loss - 0.0065\n",
      "Epoch 7084:\tTraining Loss - 0.0056\n",
      "Epoch 7085:\tTraining Loss - 0.0047\n",
      "Epoch 7086:\tTraining Loss - 0.0045\n",
      "Epoch 7087:\tTraining Loss - 0.0020\n",
      "Epoch 7088:\tTraining Loss - 0.0036\n",
      "Epoch 7089:\tTraining Loss - 0.0070\n",
      "Epoch 7090:\tTraining Loss - 0.0021\n",
      "Epoch 7091:\tTraining Loss - 0.0051\n",
      "Epoch 7092:\tTraining Loss - 0.0032\n",
      "Epoch 7093:\tTraining Loss - 0.0047\n",
      "Epoch 7094:\tTraining Loss - 0.0098\n",
      "Epoch 7095:\tTraining Loss - 0.0023\n",
      "Epoch 7096:\tTraining Loss - 0.0041\n",
      "Epoch 7097:\tTraining Loss - 0.0043\n",
      "Epoch 7098:\tTraining Loss - 0.0073\n",
      "Epoch 7099:\tTraining Loss - 0.0046\n",
      "Epoch 7100:\tTraining Loss - 0.0043\n",
      "Epoch 7101:\tTraining Loss - 0.0057\n",
      "Epoch 7102:\tTraining Loss - 0.0029\n",
      "Epoch 7103:\tTraining Loss - 0.0015\n",
      "Epoch 7104:\tTraining Loss - 0.0024\n",
      "Epoch 7105:\tTraining Loss - 0.0036\n",
      "Epoch 7106:\tTraining Loss - 0.0023\n",
      "Epoch 7107:\tTraining Loss - 0.0036\n",
      "Epoch 7108:\tTraining Loss - 0.0092\n",
      "Epoch 7109:\tTraining Loss - 0.0061\n",
      "Epoch 7110:\tTraining Loss - 0.0048\n",
      "Epoch 7111:\tTraining Loss - 0.0016\n",
      "Epoch 7112:\tTraining Loss - 0.0031\n",
      "Epoch 7113:\tTraining Loss - 0.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7114:\tTraining Loss - 0.0042\n",
      "Epoch 7115:\tTraining Loss - 0.0032\n",
      "Epoch 7116:\tTraining Loss - 0.0043\n",
      "Epoch 7117:\tTraining Loss - 0.0031\n",
      "Epoch 7118:\tTraining Loss - 0.0056\n",
      "Epoch 7119:\tTraining Loss - 0.0091\n",
      "Epoch 7120:\tTraining Loss - 0.0019\n",
      "Epoch 7121:\tTraining Loss - 0.0034\n",
      "Epoch 7122:\tTraining Loss - 0.0057\n",
      "Epoch 7123:\tTraining Loss - 0.0050\n",
      "Epoch 7124:\tTraining Loss - 0.0035\n",
      "Epoch 7125:\tTraining Loss - 0.0046\n",
      "Epoch 7126:\tTraining Loss - 0.0022\n",
      "Epoch 7127:\tTraining Loss - 0.0041\n",
      "Epoch 7128:\tTraining Loss - 0.0102\n",
      "Epoch 7129:\tTraining Loss - 0.0044\n",
      "Epoch 7130:\tTraining Loss - 0.0037\n",
      "Epoch 7131:\tTraining Loss - 0.0064\n",
      "Epoch 7132:\tTraining Loss - 0.0039\n",
      "Epoch 7133:\tTraining Loss - 0.0067\n",
      "Epoch 7134:\tTraining Loss - 0.0042\n",
      "Epoch 7135:\tTraining Loss - 0.0049\n",
      "Epoch 7136:\tTraining Loss - 0.0031\n",
      "Epoch 7137:\tTraining Loss - 0.0058\n",
      "Epoch 7138:\tTraining Loss - 0.0070\n",
      "Epoch 7139:\tTraining Loss - 0.0050\n",
      "Epoch 7140:\tTraining Loss - 0.0050\n",
      "Epoch 7141:\tTraining Loss - 0.0040\n",
      "Epoch 7142:\tTraining Loss - 0.0077\n",
      "Epoch 7143:\tTraining Loss - 0.0023\n",
      "Epoch 7144:\tTraining Loss - 0.0050\n",
      "Epoch 7145:\tTraining Loss - 0.0042\n",
      "Epoch 7146:\tTraining Loss - 0.0050\n",
      "Epoch 7147:\tTraining Loss - 0.0052\n",
      "Epoch 7148:\tTraining Loss - 0.0042\n",
      "Epoch 7149:\tTraining Loss - 0.0033\n",
      "Epoch 7150:\tTraining Loss - 0.0032\n",
      "Epoch 7151:\tTraining Loss - 0.0049\n",
      "Epoch 7152:\tTraining Loss - 0.0021\n",
      "Epoch 7153:\tTraining Loss - 0.0030\n",
      "Epoch 7154:\tTraining Loss - 0.0032\n",
      "Epoch 7155:\tTraining Loss - 0.0068\n",
      "Epoch 7156:\tTraining Loss - 0.0017\n",
      "Epoch 7157:\tTraining Loss - 0.0025\n",
      "Epoch 7158:\tTraining Loss - 0.0041\n",
      "Epoch 7159:\tTraining Loss - 0.0083\n",
      "Epoch 7160:\tTraining Loss - 0.0018\n",
      "Epoch 7161:\tTraining Loss - 0.0064\n",
      "Epoch 7162:\tTraining Loss - 0.0008\n",
      "Epoch 7163:\tTraining Loss - 0.0048\n",
      "Epoch 7164:\tTraining Loss - 0.0024\n",
      "Epoch 7165:\tTraining Loss - 0.0045\n",
      "Epoch 7166:\tTraining Loss - 0.0052\n",
      "Epoch 7167:\tTraining Loss - 0.0057\n",
      "Epoch 7168:\tTraining Loss - 0.0066\n",
      "Epoch 7169:\tTraining Loss - 0.0074\n",
      "Epoch 7170:\tTraining Loss - 0.0077\n",
      "Epoch 7171:\tTraining Loss - 0.0059\n",
      "Epoch 7172:\tTraining Loss - 0.0049\n",
      "Epoch 7173:\tTraining Loss - 0.0031\n",
      "Epoch 7174:\tTraining Loss - 0.0059\n",
      "Epoch 7175:\tTraining Loss - 0.0028\n",
      "Epoch 7176:\tTraining Loss - 0.0031\n",
      "Epoch 7177:\tTraining Loss - 0.0041\n",
      "Epoch 7178:\tTraining Loss - 0.0028\n",
      "Epoch 7179:\tTraining Loss - 0.0070\n",
      "Epoch 7180:\tTraining Loss - 0.0034\n",
      "Epoch 7181:\tTraining Loss - 0.0034\n",
      "Epoch 7182:\tTraining Loss - 0.0051\n",
      "Epoch 7183:\tTraining Loss - 0.0061\n",
      "Epoch 7184:\tTraining Loss - 0.0044\n",
      "Epoch 7185:\tTraining Loss - 0.0043\n",
      "Epoch 7186:\tTraining Loss - 0.0051\n",
      "Epoch 7187:\tTraining Loss - 0.0014\n",
      "Epoch 7188:\tTraining Loss - 0.0096\n",
      "Epoch 7189:\tTraining Loss - 0.0047\n",
      "Epoch 7190:\tTraining Loss - 0.0061\n",
      "Epoch 7191:\tTraining Loss - 0.0034\n",
      "Epoch 7192:\tTraining Loss - 0.0021\n",
      "Epoch 7193:\tTraining Loss - 0.0081\n",
      "Epoch 7194:\tTraining Loss - 0.0048\n",
      "Epoch 7195:\tTraining Loss - 0.0054\n",
      "Epoch 7196:\tTraining Loss - 0.0052\n",
      "Epoch 7197:\tTraining Loss - 0.0018\n",
      "Epoch 7198:\tTraining Loss - 0.0034\n",
      "Epoch 7199:\tTraining Loss - 0.0020\n",
      "Epoch 7200:\tTraining Loss - 0.0055\n",
      "Epoch 7201:\tTraining Loss - 0.0049\n",
      "Epoch 7202:\tTraining Loss - 0.0049\n",
      "Epoch 7203:\tTraining Loss - 0.0032\n",
      "Epoch 7204:\tTraining Loss - 0.0034\n",
      "Epoch 7205:\tTraining Loss - 0.0041\n",
      "Epoch 7206:\tTraining Loss - 0.0041\n",
      "Epoch 7207:\tTraining Loss - 0.0057\n",
      "Epoch 7208:\tTraining Loss - 0.0082\n",
      "Epoch 7209:\tTraining Loss - 0.0044\n",
      "Epoch 7210:\tTraining Loss - 0.0040\n",
      "Epoch 7211:\tTraining Loss - 0.0027\n",
      "Epoch 7212:\tTraining Loss - 0.0062\n",
      "Epoch 7213:\tTraining Loss - 0.0043\n",
      "Epoch 7214:\tTraining Loss - 0.0035\n",
      "Epoch 7215:\tTraining Loss - 0.0039\n",
      "Epoch 7216:\tTraining Loss - 0.0041\n",
      "Epoch 7217:\tTraining Loss - 0.0036\n",
      "Epoch 7218:\tTraining Loss - 0.0019\n",
      "Epoch 7219:\tTraining Loss - 0.0032\n",
      "Epoch 7220:\tTraining Loss - 0.0061\n",
      "Epoch 7221:\tTraining Loss - 0.0049\n",
      "Epoch 7222:\tTraining Loss - 0.0027\n",
      "Epoch 7223:\tTraining Loss - 0.0066\n",
      "Epoch 7224:\tTraining Loss - 0.0043\n",
      "Epoch 7225:\tTraining Loss - 0.0064\n",
      "Epoch 7226:\tTraining Loss - 0.0026\n",
      "Epoch 7227:\tTraining Loss - 0.0048\n",
      "Epoch 7228:\tTraining Loss - 0.0071\n",
      "Epoch 7229:\tTraining Loss - 0.0037\n",
      "Epoch 7230:\tTraining Loss - 0.0039\n",
      "Epoch 7231:\tTraining Loss - 0.0048\n",
      "Epoch 7232:\tTraining Loss - 0.0100\n",
      "Epoch 7233:\tTraining Loss - 0.0043\n",
      "Epoch 7234:\tTraining Loss - 0.0026\n",
      "Epoch 7235:\tTraining Loss - 0.0066\n",
      "Epoch 7236:\tTraining Loss - 0.0060\n",
      "Epoch 7237:\tTraining Loss - 0.0018\n",
      "Epoch 7238:\tTraining Loss - 0.0051\n",
      "Epoch 7239:\tTraining Loss - 0.0066\n",
      "Epoch 7240:\tTraining Loss - 0.0037\n",
      "Epoch 7241:\tTraining Loss - 0.0019\n",
      "Epoch 7242:\tTraining Loss - 0.0040\n",
      "Epoch 7243:\tTraining Loss - 0.0057\n",
      "Epoch 7244:\tTraining Loss - 0.0080\n",
      "Epoch 7245:\tTraining Loss - 0.0016\n",
      "Epoch 7246:\tTraining Loss - 0.0053\n",
      "Epoch 7247:\tTraining Loss - 0.0032\n",
      "Epoch 7248:\tTraining Loss - 0.0038\n",
      "Epoch 7249:\tTraining Loss - 0.0071\n",
      "Epoch 7250:\tTraining Loss - 0.0040\n",
      "Epoch 7251:\tTraining Loss - 0.0021\n",
      "Epoch 7252:\tTraining Loss - 0.0072\n",
      "Epoch 7253:\tTraining Loss - 0.0022\n",
      "Epoch 7254:\tTraining Loss - 0.0068\n",
      "Epoch 7255:\tTraining Loss - 0.0056\n",
      "Epoch 7256:\tTraining Loss - 0.0035\n",
      "Epoch 7257:\tTraining Loss - 0.0038\n",
      "Epoch 7258:\tTraining Loss - 0.0043\n",
      "Epoch 7259:\tTraining Loss - 0.0043\n",
      "Epoch 7260:\tTraining Loss - 0.0044\n",
      "Epoch 7261:\tTraining Loss - 0.0026\n",
      "Epoch 7262:\tTraining Loss - 0.0040\n",
      "Epoch 7263:\tTraining Loss - 0.0039\n",
      "Epoch 7264:\tTraining Loss - 0.0044\n",
      "Epoch 7265:\tTraining Loss - 0.0039\n",
      "Epoch 7266:\tTraining Loss - 0.0061\n",
      "Epoch 7267:\tTraining Loss - 0.0034\n",
      "Epoch 7268:\tTraining Loss - 0.0018\n",
      "Epoch 7269:\tTraining Loss - 0.0039\n",
      "Epoch 7270:\tTraining Loss - 0.0052\n",
      "Epoch 7271:\tTraining Loss - 0.0063\n",
      "Epoch 7272:\tTraining Loss - 0.0038\n",
      "Epoch 7273:\tTraining Loss - 0.0057\n",
      "Epoch 7274:\tTraining Loss - 0.0097\n",
      "Epoch 7275:\tTraining Loss - 0.0039\n",
      "Epoch 7276:\tTraining Loss - 0.0036\n",
      "Epoch 7277:\tTraining Loss - 0.0082\n",
      "Epoch 7278:\tTraining Loss - 0.0048\n",
      "Epoch 7279:\tTraining Loss - 0.0039\n",
      "Epoch 7280:\tTraining Loss - 0.0047\n",
      "Epoch 7281:\tTraining Loss - 0.0052\n",
      "Epoch 7282:\tTraining Loss - 0.0040\n",
      "Epoch 7283:\tTraining Loss - 0.0022\n",
      "Epoch 7284:\tTraining Loss - 0.0031\n",
      "Epoch 7285:\tTraining Loss - 0.0059\n",
      "Epoch 7286:\tTraining Loss - 0.0032\n",
      "Epoch 7287:\tTraining Loss - 0.0031\n",
      "Epoch 7288:\tTraining Loss - 0.0034\n",
      "Epoch 7289:\tTraining Loss - 0.0046\n",
      "Epoch 7290:\tTraining Loss - 0.0072\n",
      "Epoch 7291:\tTraining Loss - 0.0044\n",
      "Epoch 7292:\tTraining Loss - 0.0026\n",
      "Epoch 7293:\tTraining Loss - 0.0045\n",
      "Epoch 7294:\tTraining Loss - 0.0018\n",
      "Epoch 7295:\tTraining Loss - 0.0039\n",
      "Epoch 7296:\tTraining Loss - 0.0031\n",
      "Epoch 7297:\tTraining Loss - 0.0111\n",
      "Epoch 7298:\tTraining Loss - 0.0050\n",
      "Epoch 7299:\tTraining Loss - 0.0015\n",
      "Epoch 7300:\tTraining Loss - 0.0052\n",
      "Epoch 7301:\tTraining Loss - 0.0045\n",
      "Epoch 7302:\tTraining Loss - 0.0023\n",
      "Epoch 7303:\tTraining Loss - 0.0038\n",
      "Epoch 7304:\tTraining Loss - 0.0045\n",
      "Epoch 7305:\tTraining Loss - 0.0049\n",
      "Epoch 7306:\tTraining Loss - 0.0087\n",
      "Epoch 7307:\tTraining Loss - 0.0070\n",
      "Epoch 7308:\tTraining Loss - 0.0050\n",
      "Epoch 7309:\tTraining Loss - 0.0029\n",
      "Epoch 7310:\tTraining Loss - 0.0047\n",
      "Epoch 7311:\tTraining Loss - 0.0046\n",
      "Epoch 7312:\tTraining Loss - 0.0023\n",
      "Epoch 7313:\tTraining Loss - 0.0039\n",
      "Epoch 7314:\tTraining Loss - 0.0043\n",
      "Epoch 7315:\tTraining Loss - 0.0046\n",
      "Epoch 7316:\tTraining Loss - 0.0052\n",
      "Epoch 7317:\tTraining Loss - 0.0016\n",
      "Epoch 7318:\tTraining Loss - 0.0048\n",
      "Epoch 7319:\tTraining Loss - 0.0030\n",
      "Epoch 7320:\tTraining Loss - 0.0039\n",
      "Epoch 7321:\tTraining Loss - 0.0022\n",
      "Epoch 7322:\tTraining Loss - 0.0041\n",
      "Epoch 7323:\tTraining Loss - 0.0043\n",
      "Epoch 7324:\tTraining Loss - 0.0024\n",
      "Epoch 7325:\tTraining Loss - 0.0053\n",
      "Epoch 7326:\tTraining Loss - 0.0066\n",
      "Epoch 7327:\tTraining Loss - 0.0041\n",
      "Epoch 7328:\tTraining Loss - 0.0055\n",
      "Epoch 7329:\tTraining Loss - 0.0039\n",
      "Epoch 7330:\tTraining Loss - 0.0036\n",
      "Epoch 7331:\tTraining Loss - 0.0029\n",
      "Epoch 7332:\tTraining Loss - 0.0020\n",
      "Epoch 7333:\tTraining Loss - 0.0044\n",
      "Epoch 7334:\tTraining Loss - 0.0026\n",
      "Epoch 7335:\tTraining Loss - 0.0019\n",
      "Epoch 7336:\tTraining Loss - 0.0062\n",
      "Epoch 7337:\tTraining Loss - 0.0020\n",
      "Epoch 7338:\tTraining Loss - 0.0086\n",
      "Epoch 7339:\tTraining Loss - 0.0064\n",
      "Epoch 7340:\tTraining Loss - 0.0053\n",
      "Epoch 7341:\tTraining Loss - 0.0069\n",
      "Epoch 7342:\tTraining Loss - 0.0028\n",
      "Epoch 7343:\tTraining Loss - 0.0081\n",
      "Epoch 7344:\tTraining Loss - 0.0039\n",
      "Epoch 7345:\tTraining Loss - 0.0066\n",
      "Epoch 7346:\tTraining Loss - 0.0022\n",
      "Epoch 7347:\tTraining Loss - 0.0043\n",
      "Epoch 7348:\tTraining Loss - 0.0067\n",
      "Epoch 7349:\tTraining Loss - 0.0050\n",
      "Epoch 7350:\tTraining Loss - 0.0019\n",
      "Epoch 7351:\tTraining Loss - 0.0045\n",
      "Epoch 7352:\tTraining Loss - 0.0034\n",
      "Epoch 7353:\tTraining Loss - 0.0023\n",
      "Epoch 7354:\tTraining Loss - 0.0043\n",
      "Epoch 7355:\tTraining Loss - 0.0137\n",
      "Epoch 7356:\tTraining Loss - 0.0038\n",
      "Epoch 7357:\tTraining Loss - 0.0031\n",
      "Epoch 7358:\tTraining Loss - 0.0047\n",
      "Epoch 7359:\tTraining Loss - 0.0055\n",
      "Epoch 7360:\tTraining Loss - 0.0060\n",
      "Epoch 7361:\tTraining Loss - 0.0050\n",
      "Epoch 7362:\tTraining Loss - 0.0040\n",
      "Epoch 7363:\tTraining Loss - 0.0039\n",
      "Epoch 7364:\tTraining Loss - 0.0041\n",
      "Epoch 7365:\tTraining Loss - 0.0020\n",
      "Epoch 7366:\tTraining Loss - 0.0086\n",
      "Epoch 7367:\tTraining Loss - 0.0056\n",
      "Epoch 7368:\tTraining Loss - 0.0033\n",
      "Epoch 7369:\tTraining Loss - 0.0037\n",
      "Epoch 7370:\tTraining Loss - 0.0033\n",
      "Epoch 7371:\tTraining Loss - 0.0047\n",
      "Epoch 7372:\tTraining Loss - 0.0101\n",
      "Epoch 7373:\tTraining Loss - 0.0030\n",
      "Epoch 7374:\tTraining Loss - 0.0054\n",
      "Epoch 7375:\tTraining Loss - 0.0089\n",
      "Epoch 7376:\tTraining Loss - 0.0032\n",
      "Epoch 7377:\tTraining Loss - 0.0032\n",
      "Epoch 7378:\tTraining Loss - 0.0040\n",
      "Epoch 7379:\tTraining Loss - 0.0053\n",
      "Epoch 7380:\tTraining Loss - 0.0034\n",
      "Epoch 7381:\tTraining Loss - 0.0027\n",
      "Epoch 7382:\tTraining Loss - 0.0037\n",
      "Epoch 7383:\tTraining Loss - 0.0064\n",
      "Epoch 7384:\tTraining Loss - 0.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7385:\tTraining Loss - 0.0014\n",
      "Epoch 7386:\tTraining Loss - 0.0033\n",
      "Epoch 7387:\tTraining Loss - 0.0038\n",
      "Epoch 7388:\tTraining Loss - 0.0066\n",
      "Epoch 7389:\tTraining Loss - 0.0019\n",
      "Epoch 7390:\tTraining Loss - 0.0065\n",
      "Epoch 7391:\tTraining Loss - 0.0022\n",
      "Epoch 7392:\tTraining Loss - 0.0095\n",
      "Epoch 7393:\tTraining Loss - 0.0058\n",
      "Epoch 7394:\tTraining Loss - 0.0073\n",
      "Epoch 7395:\tTraining Loss - 0.0043\n",
      "Epoch 7396:\tTraining Loss - 0.0024\n",
      "Epoch 7397:\tTraining Loss - 0.0066\n",
      "Epoch 7398:\tTraining Loss - 0.0019\n",
      "Epoch 7399:\tTraining Loss - 0.0032\n",
      "Epoch 7400:\tTraining Loss - 0.0043\n",
      "Epoch 7401:\tTraining Loss - 0.0048\n",
      "Epoch 7402:\tTraining Loss - 0.0032\n",
      "Epoch 7403:\tTraining Loss - 0.0049\n",
      "Epoch 7404:\tTraining Loss - 0.0040\n",
      "Epoch 7405:\tTraining Loss - 0.0029\n",
      "Epoch 7406:\tTraining Loss - 0.0057\n",
      "Epoch 7407:\tTraining Loss - 0.0032\n",
      "Epoch 7408:\tTraining Loss - 0.0045\n",
      "Epoch 7409:\tTraining Loss - 0.0046\n",
      "Epoch 7410:\tTraining Loss - 0.0023\n",
      "Epoch 7411:\tTraining Loss - 0.0014\n",
      "Epoch 7412:\tTraining Loss - 0.0065\n",
      "Epoch 7413:\tTraining Loss - 0.0021\n",
      "Epoch 7414:\tTraining Loss - 0.0030\n",
      "Epoch 7415:\tTraining Loss - 0.0041\n",
      "Epoch 7416:\tTraining Loss - 0.0030\n",
      "Epoch 7417:\tTraining Loss - 0.0038\n",
      "Epoch 7418:\tTraining Loss - 0.0057\n",
      "Epoch 7419:\tTraining Loss - 0.0023\n",
      "Epoch 7420:\tTraining Loss - 0.0061\n",
      "Epoch 7421:\tTraining Loss - 0.0014\n",
      "Epoch 7422:\tTraining Loss - 0.0086\n",
      "Epoch 7423:\tTraining Loss - 0.0109\n",
      "Epoch 7424:\tTraining Loss - 0.0054\n",
      "Epoch 7425:\tTraining Loss - 0.0051\n",
      "Epoch 7426:\tTraining Loss - 0.0040\n",
      "Epoch 7427:\tTraining Loss - 0.0037\n",
      "Epoch 7428:\tTraining Loss - 0.0055\n",
      "Epoch 7429:\tTraining Loss - 0.0085\n",
      "Epoch 7430:\tTraining Loss - 0.0053\n",
      "Epoch 7431:\tTraining Loss - 0.0037\n",
      "Epoch 7432:\tTraining Loss - 0.0053\n",
      "Epoch 7433:\tTraining Loss - 0.0078\n",
      "Epoch 7434:\tTraining Loss - 0.0045\n",
      "Epoch 7435:\tTraining Loss - 0.0028\n",
      "Epoch 7436:\tTraining Loss - 0.0039\n",
      "Epoch 7437:\tTraining Loss - 0.0061\n",
      "Epoch 7438:\tTraining Loss - 0.0023\n",
      "Epoch 7439:\tTraining Loss - 0.0041\n",
      "Epoch 7440:\tTraining Loss - 0.0051\n",
      "Epoch 7441:\tTraining Loss - 0.0077\n",
      "Epoch 7442:\tTraining Loss - 0.0023\n",
      "Epoch 7443:\tTraining Loss - 0.0073\n",
      "Epoch 7444:\tTraining Loss - 0.0031\n",
      "Epoch 7445:\tTraining Loss - 0.0069\n",
      "Epoch 7446:\tTraining Loss - 0.0037\n",
      "Epoch 7447:\tTraining Loss - 0.0022\n",
      "Epoch 7448:\tTraining Loss - 0.0070\n",
      "Epoch 7449:\tTraining Loss - 0.0074\n",
      "Epoch 7450:\tTraining Loss - 0.0096\n",
      "Epoch 7451:\tTraining Loss - 0.0052\n",
      "Epoch 7452:\tTraining Loss - 0.0034\n",
      "Epoch 7453:\tTraining Loss - 0.0038\n",
      "Epoch 7454:\tTraining Loss - 0.0051\n",
      "Epoch 7455:\tTraining Loss - 0.0033\n",
      "Epoch 7456:\tTraining Loss - 0.0042\n",
      "Epoch 7457:\tTraining Loss - 0.0036\n",
      "Epoch 7458:\tTraining Loss - 0.0066\n",
      "Epoch 7459:\tTraining Loss - 0.0059\n",
      "Epoch 7460:\tTraining Loss - 0.0075\n",
      "Epoch 7461:\tTraining Loss - 0.0072\n",
      "Epoch 7462:\tTraining Loss - 0.0042\n",
      "Epoch 7463:\tTraining Loss - 0.0059\n",
      "Epoch 7464:\tTraining Loss - 0.0073\n",
      "Epoch 7465:\tTraining Loss - 0.0035\n",
      "Epoch 7466:\tTraining Loss - 0.0028\n",
      "Epoch 7467:\tTraining Loss - 0.0049\n",
      "Epoch 7468:\tTraining Loss - 0.0029\n",
      "Epoch 7469:\tTraining Loss - 0.0034\n",
      "Epoch 7470:\tTraining Loss - 0.0013\n",
      "Epoch 7471:\tTraining Loss - 0.0037\n",
      "Epoch 7472:\tTraining Loss - 0.0053\n",
      "Epoch 7473:\tTraining Loss - 0.0038\n",
      "Epoch 7474:\tTraining Loss - 0.0037\n",
      "Epoch 7475:\tTraining Loss - 0.0083\n",
      "Epoch 7476:\tTraining Loss - 0.0038\n",
      "Epoch 7477:\tTraining Loss - 0.0046\n",
      "Epoch 7478:\tTraining Loss - 0.0042\n",
      "Epoch 7479:\tTraining Loss - 0.0043\n",
      "Epoch 7480:\tTraining Loss - 0.0024\n",
      "Epoch 7481:\tTraining Loss - 0.0043\n",
      "Epoch 7482:\tTraining Loss - 0.0025\n",
      "Epoch 7483:\tTraining Loss - 0.0046\n",
      "Epoch 7484:\tTraining Loss - 0.0086\n",
      "Epoch 7485:\tTraining Loss - 0.0102\n",
      "Epoch 7486:\tTraining Loss - 0.0091\n",
      "Epoch 7487:\tTraining Loss - 0.0096\n",
      "Epoch 7488:\tTraining Loss - 0.0029\n",
      "Epoch 7489:\tTraining Loss - 0.0086\n",
      "Epoch 7490:\tTraining Loss - 0.0059\n",
      "Epoch 7491:\tTraining Loss - 0.0030\n",
      "Epoch 7492:\tTraining Loss - 0.0035\n",
      "Epoch 7493:\tTraining Loss - 0.0064\n",
      "Epoch 7494:\tTraining Loss - 0.0025\n",
      "Epoch 7495:\tTraining Loss - 0.0036\n",
      "Epoch 7496:\tTraining Loss - 0.0024\n",
      "Epoch 7497:\tTraining Loss - 0.0036\n",
      "Epoch 7498:\tTraining Loss - 0.0075\n",
      "Epoch 7499:\tTraining Loss - 0.0021\n",
      "Epoch 7500:\tTraining Loss - 0.0045\n",
      "Epoch 7501:\tTraining Loss - 0.0050\n",
      "Epoch 7502:\tTraining Loss - 0.0078\n",
      "Epoch 7503:\tTraining Loss - 0.0063\n",
      "Epoch 7504:\tTraining Loss - 0.0054\n",
      "Epoch 7505:\tTraining Loss - 0.0039\n",
      "Epoch 7506:\tTraining Loss - 0.0038\n",
      "Epoch 7507:\tTraining Loss - 0.0056\n",
      "Epoch 7508:\tTraining Loss - 0.0015\n",
      "Epoch 7509:\tTraining Loss - 0.0070\n",
      "Epoch 7510:\tTraining Loss - 0.0061\n",
      "Epoch 7511:\tTraining Loss - 0.0067\n",
      "Epoch 7512:\tTraining Loss - 0.0043\n",
      "Epoch 7513:\tTraining Loss - 0.0054\n",
      "Epoch 7514:\tTraining Loss - 0.0046\n",
      "Epoch 7515:\tTraining Loss - 0.0046\n",
      "Epoch 7516:\tTraining Loss - 0.0077\n",
      "Epoch 7517:\tTraining Loss - 0.0020\n",
      "Epoch 7518:\tTraining Loss - 0.0026\n",
      "Epoch 7519:\tTraining Loss - 0.0044\n",
      "Epoch 7520:\tTraining Loss - 0.0016\n",
      "Epoch 7521:\tTraining Loss - 0.0029\n",
      "Epoch 7522:\tTraining Loss - 0.0064\n",
      "Epoch 7523:\tTraining Loss - 0.0042\n",
      "Epoch 7524:\tTraining Loss - 0.0041\n",
      "Epoch 7525:\tTraining Loss - 0.0035\n",
      "Epoch 7526:\tTraining Loss - 0.0023\n",
      "Epoch 7527:\tTraining Loss - 0.0035\n",
      "Epoch 7528:\tTraining Loss - 0.0026\n",
      "Epoch 7529:\tTraining Loss - 0.0037\n",
      "Epoch 7530:\tTraining Loss - 0.0044\n",
      "Epoch 7531:\tTraining Loss - 0.0040\n",
      "Epoch 7532:\tTraining Loss - 0.0043\n",
      "Epoch 7533:\tTraining Loss - 0.0033\n",
      "Epoch 7534:\tTraining Loss - 0.0037\n",
      "Epoch 7535:\tTraining Loss - 0.0009\n",
      "Epoch 7536:\tTraining Loss - 0.0026\n",
      "Epoch 7537:\tTraining Loss - 0.0030\n",
      "Epoch 7538:\tTraining Loss - 0.0054\n",
      "Epoch 7539:\tTraining Loss - 0.0021\n",
      "Epoch 7540:\tTraining Loss - 0.0040\n",
      "Epoch 7541:\tTraining Loss - 0.0069\n",
      "Epoch 7542:\tTraining Loss - 0.0057\n",
      "Epoch 7543:\tTraining Loss - 0.0049\n",
      "Epoch 7544:\tTraining Loss - 0.0041\n",
      "Epoch 7545:\tTraining Loss - 0.0028\n",
      "Epoch 7546:\tTraining Loss - 0.0028\n",
      "Epoch 7547:\tTraining Loss - 0.0054\n",
      "Epoch 7548:\tTraining Loss - 0.0042\n",
      "Epoch 7549:\tTraining Loss - 0.0047\n",
      "Epoch 7550:\tTraining Loss - 0.0027\n",
      "Epoch 7551:\tTraining Loss - 0.0025\n",
      "Epoch 7552:\tTraining Loss - 0.0011\n",
      "Epoch 7553:\tTraining Loss - 0.0036\n",
      "Epoch 7554:\tTraining Loss - 0.0061\n",
      "Epoch 7555:\tTraining Loss - 0.0078\n",
      "Epoch 7556:\tTraining Loss - 0.0034\n",
      "Epoch 7557:\tTraining Loss - 0.0071\n",
      "Epoch 7558:\tTraining Loss - 0.0031\n",
      "Epoch 7559:\tTraining Loss - 0.0054\n",
      "Epoch 7560:\tTraining Loss - 0.0028\n",
      "Epoch 7561:\tTraining Loss - 0.0022\n",
      "Epoch 7562:\tTraining Loss - 0.0032\n",
      "Epoch 7563:\tTraining Loss - 0.0042\n",
      "Epoch 7564:\tTraining Loss - 0.0033\n",
      "Epoch 7565:\tTraining Loss - 0.0025\n",
      "Epoch 7566:\tTraining Loss - 0.0041\n",
      "Epoch 7567:\tTraining Loss - 0.0051\n",
      "Epoch 7568:\tTraining Loss - 0.0039\n",
      "Epoch 7569:\tTraining Loss - 0.0074\n",
      "Epoch 7570:\tTraining Loss - 0.0018\n",
      "Epoch 7571:\tTraining Loss - 0.0024\n",
      "Epoch 7572:\tTraining Loss - 0.0054\n",
      "Epoch 7573:\tTraining Loss - 0.0020\n",
      "Epoch 7574:\tTraining Loss - 0.0028\n",
      "Epoch 7575:\tTraining Loss - 0.0051\n",
      "Epoch 7576:\tTraining Loss - 0.0055\n",
      "Epoch 7577:\tTraining Loss - 0.0030\n",
      "Epoch 7578:\tTraining Loss - 0.0021\n",
      "Epoch 7579:\tTraining Loss - 0.0063\n",
      "Epoch 7580:\tTraining Loss - 0.0052\n",
      "Epoch 7581:\tTraining Loss - 0.0061\n",
      "Epoch 7582:\tTraining Loss - 0.0064\n",
      "Epoch 7583:\tTraining Loss - 0.0040\n",
      "Epoch 7584:\tTraining Loss - 0.0046\n",
      "Epoch 7585:\tTraining Loss - 0.0028\n",
      "Epoch 7586:\tTraining Loss - 0.0076\n",
      "Epoch 7587:\tTraining Loss - 0.0016\n",
      "Epoch 7588:\tTraining Loss - 0.0030\n",
      "Epoch 7589:\tTraining Loss - 0.0032\n",
      "Epoch 7590:\tTraining Loss - 0.0078\n",
      "Epoch 7591:\tTraining Loss - 0.0032\n",
      "Epoch 7592:\tTraining Loss - 0.0057\n",
      "Epoch 7593:\tTraining Loss - 0.0041\n",
      "Epoch 7594:\tTraining Loss - 0.0027\n",
      "Epoch 7595:\tTraining Loss - 0.0040\n",
      "Epoch 7596:\tTraining Loss - 0.0021\n",
      "Epoch 7597:\tTraining Loss - 0.0107\n",
      "Epoch 7598:\tTraining Loss - 0.0098\n",
      "Epoch 7599:\tTraining Loss - 0.0063\n",
      "Epoch 7600:\tTraining Loss - 0.0050\n",
      "Epoch 7601:\tTraining Loss - 0.0067\n",
      "Epoch 7602:\tTraining Loss - 0.0062\n",
      "Epoch 7603:\tTraining Loss - 0.0063\n",
      "Epoch 7604:\tTraining Loss - 0.0060\n",
      "Epoch 7605:\tTraining Loss - 0.0052\n",
      "Epoch 7606:\tTraining Loss - 0.0036\n",
      "Epoch 7607:\tTraining Loss - 0.0041\n",
      "Epoch 7608:\tTraining Loss - 0.0045\n",
      "Epoch 7609:\tTraining Loss - 0.0051\n",
      "Epoch 7610:\tTraining Loss - 0.0073\n",
      "Epoch 7611:\tTraining Loss - 0.0078\n",
      "Epoch 7612:\tTraining Loss - 0.0058\n",
      "Epoch 7613:\tTraining Loss - 0.0066\n",
      "Epoch 7614:\tTraining Loss - 0.0074\n",
      "Epoch 7615:\tTraining Loss - 0.0084\n",
      "Epoch 7616:\tTraining Loss - 0.0090\n",
      "Epoch 7617:\tTraining Loss - 0.0046\n",
      "Epoch 7618:\tTraining Loss - 0.0033\n",
      "Epoch 7619:\tTraining Loss - 0.0059\n",
      "Epoch 7620:\tTraining Loss - 0.0061\n",
      "Epoch 7621:\tTraining Loss - 0.0045\n",
      "Epoch 7622:\tTraining Loss - 0.0025\n",
      "Epoch 7623:\tTraining Loss - 0.0080\n",
      "Epoch 7624:\tTraining Loss - 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7625:\tTraining Loss - 0.0048\n",
      "Epoch 7626:\tTraining Loss - 0.0022\n",
      "Epoch 7627:\tTraining Loss - 0.0064\n",
      "Epoch 7628:\tTraining Loss - 0.0074\n",
      "Epoch 7629:\tTraining Loss - 0.0033\n",
      "Epoch 7630:\tTraining Loss - 0.0044\n",
      "Epoch 7631:\tTraining Loss - 0.0037\n",
      "Epoch 7632:\tTraining Loss - 0.0061\n",
      "Epoch 7633:\tTraining Loss - 0.0082\n",
      "Epoch 7634:\tTraining Loss - 0.0039\n",
      "Epoch 7635:\tTraining Loss - 0.0033\n",
      "Epoch 7636:\tTraining Loss - 0.0047\n",
      "Epoch 7637:\tTraining Loss - 0.0101\n",
      "Epoch 7638:\tTraining Loss - 0.0028\n",
      "Epoch 7639:\tTraining Loss - 0.0052\n",
      "Epoch 7640:\tTraining Loss - 0.0048\n",
      "Epoch 7641:\tTraining Loss - 0.0037\n",
      "Epoch 7642:\tTraining Loss - 0.0038\n",
      "Epoch 7643:\tTraining Loss - 0.0046\n",
      "Epoch 7644:\tTraining Loss - 0.0033\n",
      "Epoch 7645:\tTraining Loss - 0.0067\n",
      "Epoch 7646:\tTraining Loss - 0.0055\n",
      "Epoch 7647:\tTraining Loss - 0.0035\n",
      "Epoch 7648:\tTraining Loss - 0.0039\n",
      "Epoch 7649:\tTraining Loss - 0.0019\n",
      "Epoch 7650:\tTraining Loss - 0.0036\n",
      "Epoch 7651:\tTraining Loss - 0.0036\n",
      "Epoch 7652:\tTraining Loss - 0.0026\n",
      "Epoch 7653:\tTraining Loss - 0.0046\n",
      "Epoch 7654:\tTraining Loss - 0.0047\n",
      "Epoch 7655:\tTraining Loss - 0.0039\n",
      "Epoch 7656:\tTraining Loss - 0.0029\n",
      "Epoch 7657:\tTraining Loss - 0.0058\n",
      "Epoch 7658:\tTraining Loss - 0.0071\n",
      "Epoch 7659:\tTraining Loss - 0.0033\n",
      "Epoch 7660:\tTraining Loss - 0.0026\n",
      "Epoch 7661:\tTraining Loss - 0.0039\n",
      "Epoch 7662:\tTraining Loss - 0.0073\n",
      "Epoch 7663:\tTraining Loss - 0.0042\n",
      "Epoch 7664:\tTraining Loss - 0.0025\n",
      "Epoch 7665:\tTraining Loss - 0.0062\n",
      "Epoch 7666:\tTraining Loss - 0.0053\n",
      "Epoch 7667:\tTraining Loss - 0.0039\n",
      "Epoch 7668:\tTraining Loss - 0.0024\n",
      "Epoch 7669:\tTraining Loss - 0.0018\n",
      "Epoch 7670:\tTraining Loss - 0.0040\n",
      "Epoch 7671:\tTraining Loss - 0.0025\n",
      "Epoch 7672:\tTraining Loss - 0.0041\n",
      "Epoch 7673:\tTraining Loss - 0.0037\n",
      "Epoch 7674:\tTraining Loss - 0.0044\n",
      "Epoch 7675:\tTraining Loss - 0.0030\n",
      "Epoch 7676:\tTraining Loss - 0.0030\n",
      "Epoch 7677:\tTraining Loss - 0.0035\n",
      "Epoch 7678:\tTraining Loss - 0.0054\n",
      "Epoch 7679:\tTraining Loss - 0.0033\n",
      "Epoch 7680:\tTraining Loss - 0.0037\n",
      "Epoch 7681:\tTraining Loss - 0.0067\n",
      "Epoch 7682:\tTraining Loss - 0.0075\n",
      "Epoch 7683:\tTraining Loss - 0.0090\n",
      "Epoch 7684:\tTraining Loss - 0.0031\n",
      "Epoch 7685:\tTraining Loss - 0.0037\n",
      "Epoch 7686:\tTraining Loss - 0.0024\n",
      "Epoch 7687:\tTraining Loss - 0.0037\n",
      "Epoch 7688:\tTraining Loss - 0.0039\n",
      "Epoch 7689:\tTraining Loss - 0.0070\n",
      "Epoch 7690:\tTraining Loss - 0.0039\n",
      "Epoch 7691:\tTraining Loss - 0.0029\n",
      "Epoch 7692:\tTraining Loss - 0.0040\n",
      "Epoch 7693:\tTraining Loss - 0.0039\n",
      "Epoch 7694:\tTraining Loss - 0.0083\n",
      "Epoch 7695:\tTraining Loss - 0.0047\n",
      "Epoch 7696:\tTraining Loss - 0.0014\n",
      "Epoch 7697:\tTraining Loss - 0.0009\n",
      "Epoch 7698:\tTraining Loss - 0.0048\n",
      "Epoch 7699:\tTraining Loss - 0.0021\n",
      "Epoch 7700:\tTraining Loss - 0.0056\n",
      "Epoch 7701:\tTraining Loss - 0.0044\n",
      "Epoch 7702:\tTraining Loss - 0.0072\n",
      "Epoch 7703:\tTraining Loss - 0.0050\n",
      "Epoch 7704:\tTraining Loss - 0.0034\n",
      "Epoch 7705:\tTraining Loss - 0.0071\n",
      "Epoch 7706:\tTraining Loss - 0.0030\n",
      "Epoch 7707:\tTraining Loss - 0.0038\n",
      "Epoch 7708:\tTraining Loss - 0.0023\n",
      "Epoch 7709:\tTraining Loss - 0.0044\n",
      "Epoch 7710:\tTraining Loss - 0.0051\n",
      "Epoch 7711:\tTraining Loss - 0.0058\n",
      "Epoch 7712:\tTraining Loss - 0.0095\n",
      "Epoch 7713:\tTraining Loss - 0.0030\n",
      "Epoch 7714:\tTraining Loss - 0.0028\n",
      "Epoch 7715:\tTraining Loss - 0.0079\n",
      "Epoch 7716:\tTraining Loss - 0.0048\n",
      "Epoch 7717:\tTraining Loss - 0.0017\n",
      "Epoch 7718:\tTraining Loss - 0.0084\n",
      "Epoch 7719:\tTraining Loss - 0.0027\n",
      "Epoch 7720:\tTraining Loss - 0.0027\n",
      "Epoch 7721:\tTraining Loss - 0.0017\n",
      "Epoch 7722:\tTraining Loss - 0.0018\n",
      "Epoch 7723:\tTraining Loss - 0.0046\n",
      "Epoch 7724:\tTraining Loss - 0.0051\n",
      "Epoch 7725:\tTraining Loss - 0.0018\n",
      "Epoch 7726:\tTraining Loss - 0.0045\n",
      "Epoch 7727:\tTraining Loss - 0.0075\n",
      "Epoch 7728:\tTraining Loss - 0.0087\n",
      "Epoch 7729:\tTraining Loss - 0.0010\n",
      "Epoch 7730:\tTraining Loss - 0.0048\n",
      "Epoch 7731:\tTraining Loss - 0.0036\n",
      "Epoch 7732:\tTraining Loss - 0.0025\n",
      "Epoch 7733:\tTraining Loss - 0.0013\n",
      "Epoch 7734:\tTraining Loss - 0.0060\n",
      "Epoch 7735:\tTraining Loss - 0.0038\n",
      "Epoch 7736:\tTraining Loss - 0.0061\n",
      "Epoch 7737:\tTraining Loss - 0.0054\n",
      "Epoch 7738:\tTraining Loss - 0.0037\n",
      "Epoch 7739:\tTraining Loss - 0.0044\n",
      "Epoch 7740:\tTraining Loss - 0.0036\n",
      "Epoch 7741:\tTraining Loss - 0.0036\n",
      "Epoch 7742:\tTraining Loss - 0.0060\n",
      "Epoch 7743:\tTraining Loss - 0.0053\n",
      "Epoch 7744:\tTraining Loss - 0.0056\n",
      "Epoch 7745:\tTraining Loss - 0.0053\n",
      "Epoch 7746:\tTraining Loss - 0.0028\n",
      "Epoch 7747:\tTraining Loss - 0.0027\n",
      "Epoch 7748:\tTraining Loss - 0.0016\n",
      "Epoch 7749:\tTraining Loss - 0.0033\n",
      "Epoch 7750:\tTraining Loss - 0.0021\n",
      "Epoch 7751:\tTraining Loss - 0.0075\n",
      "Epoch 7752:\tTraining Loss - 0.0033\n",
      "Epoch 7753:\tTraining Loss - 0.0031\n",
      "Epoch 7754:\tTraining Loss - 0.0043\n",
      "Epoch 7755:\tTraining Loss - 0.0043\n",
      "Epoch 7756:\tTraining Loss - 0.0019\n",
      "Epoch 7757:\tTraining Loss - 0.0030\n",
      "Epoch 7758:\tTraining Loss - 0.0029\n",
      "Epoch 7759:\tTraining Loss - 0.0021\n",
      "Epoch 7760:\tTraining Loss - 0.0093\n",
      "Epoch 7761:\tTraining Loss - 0.0037\n",
      "Epoch 7762:\tTraining Loss - 0.0047\n",
      "Epoch 7763:\tTraining Loss - 0.0025\n",
      "Epoch 7764:\tTraining Loss - 0.0076\n",
      "Epoch 7765:\tTraining Loss - 0.0054\n",
      "Epoch 7766:\tTraining Loss - 0.0044\n",
      "Epoch 7767:\tTraining Loss - 0.0024\n",
      "Epoch 7768:\tTraining Loss - 0.0026\n",
      "Epoch 7769:\tTraining Loss - 0.0060\n",
      "Epoch 7770:\tTraining Loss - 0.0034\n",
      "Epoch 7771:\tTraining Loss - 0.0051\n",
      "Epoch 7772:\tTraining Loss - 0.0081\n",
      "Epoch 7773:\tTraining Loss - 0.0035\n",
      "Epoch 7774:\tTraining Loss - 0.0063\n",
      "Epoch 7775:\tTraining Loss - 0.0028\n",
      "Epoch 7776:\tTraining Loss - 0.0023\n",
      "Epoch 7777:\tTraining Loss - 0.0052\n",
      "Epoch 7778:\tTraining Loss - 0.0017\n",
      "Epoch 7779:\tTraining Loss - 0.0029\n",
      "Epoch 7780:\tTraining Loss - 0.0021\n",
      "Epoch 7781:\tTraining Loss - 0.0028\n",
      "Epoch 7782:\tTraining Loss - 0.0071\n",
      "Epoch 7783:\tTraining Loss - 0.0026\n",
      "Epoch 7784:\tTraining Loss - 0.0092\n",
      "Epoch 7785:\tTraining Loss - 0.0051\n",
      "Epoch 7786:\tTraining Loss - 0.0036\n",
      "Epoch 7787:\tTraining Loss - 0.0068\n",
      "Epoch 7788:\tTraining Loss - 0.0038\n",
      "Epoch 7789:\tTraining Loss - 0.0050\n",
      "Epoch 7790:\tTraining Loss - 0.0047\n",
      "Epoch 7791:\tTraining Loss - 0.0055\n",
      "Epoch 7792:\tTraining Loss - 0.0037\n",
      "Epoch 7793:\tTraining Loss - 0.0030\n",
      "Epoch 7794:\tTraining Loss - 0.0049\n",
      "Epoch 7795:\tTraining Loss - 0.0039\n",
      "Epoch 7796:\tTraining Loss - 0.0030\n",
      "Epoch 7797:\tTraining Loss - 0.0027\n",
      "Epoch 7798:\tTraining Loss - 0.0038\n",
      "Epoch 7799:\tTraining Loss - 0.0024\n",
      "Epoch 7800:\tTraining Loss - 0.0021\n",
      "Epoch 7801:\tTraining Loss - 0.0083\n",
      "Epoch 7802:\tTraining Loss - 0.0059\n",
      "Epoch 7803:\tTraining Loss - 0.0025\n",
      "Epoch 7804:\tTraining Loss - 0.0019\n",
      "Epoch 7805:\tTraining Loss - 0.0041\n",
      "Epoch 7806:\tTraining Loss - 0.0034\n",
      "Epoch 7807:\tTraining Loss - 0.0022\n",
      "Epoch 7808:\tTraining Loss - 0.0018\n",
      "Epoch 7809:\tTraining Loss - 0.0033\n",
      "Epoch 7810:\tTraining Loss - 0.0053\n",
      "Epoch 7811:\tTraining Loss - 0.0023\n",
      "Epoch 7812:\tTraining Loss - 0.0036\n",
      "Epoch 7813:\tTraining Loss - 0.0046\n",
      "Epoch 7814:\tTraining Loss - 0.0044\n",
      "Epoch 7815:\tTraining Loss - 0.0038\n",
      "Epoch 7816:\tTraining Loss - 0.0058\n",
      "Epoch 7817:\tTraining Loss - 0.0087\n",
      "Epoch 7818:\tTraining Loss - 0.0056\n",
      "Epoch 7819:\tTraining Loss - 0.0053\n",
      "Epoch 7820:\tTraining Loss - 0.0043\n",
      "Epoch 7821:\tTraining Loss - 0.0055\n",
      "Epoch 7822:\tTraining Loss - 0.0081\n",
      "Epoch 7823:\tTraining Loss - 0.0018\n",
      "Epoch 7824:\tTraining Loss - 0.0036\n",
      "Epoch 7825:\tTraining Loss - 0.0027\n",
      "Epoch 7826:\tTraining Loss - 0.0013\n",
      "Epoch 7827:\tTraining Loss - 0.0044\n",
      "Epoch 7828:\tTraining Loss - 0.0028\n",
      "Epoch 7829:\tTraining Loss - 0.0099\n",
      "Epoch 7830:\tTraining Loss - 0.0037\n",
      "Epoch 7831:\tTraining Loss - 0.0025\n",
      "Epoch 7832:\tTraining Loss - 0.0051\n",
      "Epoch 7833:\tTraining Loss - 0.0034\n",
      "Epoch 7834:\tTraining Loss - 0.0037\n",
      "Epoch 7835:\tTraining Loss - 0.0068\n",
      "Epoch 7836:\tTraining Loss - 0.0025\n",
      "Epoch 7837:\tTraining Loss - 0.0088\n",
      "Epoch 7838:\tTraining Loss - 0.0085\n",
      "Epoch 7839:\tTraining Loss - 0.0064\n",
      "Epoch 7840:\tTraining Loss - 0.0068\n",
      "Epoch 7841:\tTraining Loss - 0.0050\n",
      "Epoch 7842:\tTraining Loss - 0.0074\n",
      "Epoch 7843:\tTraining Loss - 0.0033\n",
      "Epoch 7844:\tTraining Loss - 0.0040\n",
      "Epoch 7845:\tTraining Loss - 0.0027\n",
      "Epoch 7846:\tTraining Loss - 0.0036\n",
      "Epoch 7847:\tTraining Loss - 0.0032\n",
      "Epoch 7848:\tTraining Loss - 0.0053\n",
      "Epoch 7849:\tTraining Loss - 0.0046\n",
      "Epoch 7850:\tTraining Loss - 0.0051\n",
      "Epoch 7851:\tTraining Loss - 0.0085\n",
      "Epoch 7852:\tTraining Loss - 0.0034\n",
      "Epoch 7853:\tTraining Loss - 0.0032\n",
      "Epoch 7854:\tTraining Loss - 0.0036\n",
      "Epoch 7855:\tTraining Loss - 0.0050\n",
      "Epoch 7856:\tTraining Loss - 0.0031\n",
      "Epoch 7857:\tTraining Loss - 0.0052\n",
      "Epoch 7858:\tTraining Loss - 0.0042\n",
      "Epoch 7859:\tTraining Loss - 0.0055\n",
      "Epoch 7860:\tTraining Loss - 0.0030\n",
      "Epoch 7861:\tTraining Loss - 0.0030\n",
      "Epoch 7862:\tTraining Loss - 0.0026\n",
      "Epoch 7863:\tTraining Loss - 0.0029\n",
      "Epoch 7864:\tTraining Loss - 0.0024\n",
      "Epoch 7865:\tTraining Loss - 0.0038\n",
      "Epoch 7866:\tTraining Loss - 0.0035\n",
      "Epoch 7867:\tTraining Loss - 0.0028\n",
      "Epoch 7868:\tTraining Loss - 0.0031\n",
      "Epoch 7869:\tTraining Loss - 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7870:\tTraining Loss - 0.0042\n",
      "Epoch 7871:\tTraining Loss - 0.0019\n",
      "Epoch 7872:\tTraining Loss - 0.0066\n",
      "Epoch 7873:\tTraining Loss - 0.0057\n",
      "Epoch 7874:\tTraining Loss - 0.0046\n",
      "Epoch 7875:\tTraining Loss - 0.0066\n",
      "Epoch 7876:\tTraining Loss - 0.0018\n",
      "Epoch 7877:\tTraining Loss - 0.0035\n",
      "Epoch 7878:\tTraining Loss - 0.0034\n",
      "Epoch 7879:\tTraining Loss - 0.0089\n",
      "Epoch 7880:\tTraining Loss - 0.0060\n",
      "Epoch 7881:\tTraining Loss - 0.0018\n",
      "Epoch 7882:\tTraining Loss - 0.0048\n",
      "Epoch 7883:\tTraining Loss - 0.0034\n",
      "Epoch 7884:\tTraining Loss - 0.0015\n",
      "Epoch 7885:\tTraining Loss - 0.0048\n",
      "Epoch 7886:\tTraining Loss - 0.0051\n",
      "Epoch 7887:\tTraining Loss - 0.0061\n",
      "Epoch 7888:\tTraining Loss - 0.0024\n",
      "Epoch 7889:\tTraining Loss - 0.0067\n",
      "Epoch 7890:\tTraining Loss - 0.0033\n",
      "Epoch 7891:\tTraining Loss - 0.0055\n",
      "Epoch 7892:\tTraining Loss - 0.0038\n",
      "Epoch 7893:\tTraining Loss - 0.0022\n",
      "Epoch 7894:\tTraining Loss - 0.0032\n",
      "Epoch 7895:\tTraining Loss - 0.0014\n",
      "Epoch 7896:\tTraining Loss - 0.0076\n",
      "Epoch 7897:\tTraining Loss - 0.0036\n",
      "Epoch 7898:\tTraining Loss - 0.0087\n",
      "Epoch 7899:\tTraining Loss - 0.0039\n",
      "Epoch 7900:\tTraining Loss - 0.0034\n",
      "Epoch 7901:\tTraining Loss - 0.0076\n",
      "Epoch 7902:\tTraining Loss - 0.0025\n",
      "Epoch 7903:\tTraining Loss - 0.0031\n",
      "Epoch 7904:\tTraining Loss - 0.0056\n",
      "Epoch 7905:\tTraining Loss - 0.0023\n",
      "Epoch 7906:\tTraining Loss - 0.0036\n",
      "Epoch 7907:\tTraining Loss - 0.0031\n",
      "Epoch 7908:\tTraining Loss - 0.0034\n",
      "Epoch 7909:\tTraining Loss - 0.0029\n",
      "Epoch 7910:\tTraining Loss - 0.0081\n",
      "Epoch 7911:\tTraining Loss - 0.0040\n",
      "Epoch 7912:\tTraining Loss - 0.0017\n",
      "Epoch 7913:\tTraining Loss - 0.0009\n",
      "Epoch 7914:\tTraining Loss - 0.0056\n",
      "Epoch 7915:\tTraining Loss - 0.0021\n",
      "Epoch 7916:\tTraining Loss - 0.0030\n",
      "Epoch 7917:\tTraining Loss - 0.0039\n",
      "Epoch 7918:\tTraining Loss - 0.0030\n",
      "Epoch 7919:\tTraining Loss - 0.0049\n",
      "Epoch 7920:\tTraining Loss - 0.0024\n",
      "Epoch 7921:\tTraining Loss - 0.0007\n",
      "Epoch 7922:\tTraining Loss - 0.0023\n",
      "Epoch 7923:\tTraining Loss - 0.0027\n",
      "Epoch 7924:\tTraining Loss - 0.0080\n",
      "Epoch 7925:\tTraining Loss - 0.0050\n",
      "Epoch 7926:\tTraining Loss - 0.0024\n",
      "Epoch 7927:\tTraining Loss - 0.0064\n",
      "Epoch 7928:\tTraining Loss - 0.0027\n",
      "Epoch 7929:\tTraining Loss - 0.0017\n",
      "Epoch 7930:\tTraining Loss - 0.0037\n",
      "Epoch 7931:\tTraining Loss - 0.0024\n",
      "Epoch 7932:\tTraining Loss - 0.0063\n",
      "Epoch 7933:\tTraining Loss - 0.0060\n",
      "Epoch 7934:\tTraining Loss - 0.0057\n",
      "Epoch 7935:\tTraining Loss - 0.0026\n",
      "Epoch 7936:\tTraining Loss - 0.0048\n",
      "Epoch 7937:\tTraining Loss - 0.0087\n",
      "Epoch 7938:\tTraining Loss - 0.0048\n",
      "Epoch 7939:\tTraining Loss - 0.0010\n",
      "Epoch 7940:\tTraining Loss - 0.0035\n",
      "Epoch 7941:\tTraining Loss - 0.0068\n",
      "Epoch 7942:\tTraining Loss - 0.0043\n",
      "Epoch 7943:\tTraining Loss - 0.0032\n",
      "Epoch 7944:\tTraining Loss - 0.0046\n",
      "Epoch 7945:\tTraining Loss - 0.0082\n",
      "Epoch 7946:\tTraining Loss - 0.0049\n",
      "Epoch 7947:\tTraining Loss - 0.0023\n",
      "Epoch 7948:\tTraining Loss - 0.0027\n",
      "Epoch 7949:\tTraining Loss - 0.0068\n",
      "Epoch 7950:\tTraining Loss - 0.0049\n",
      "Epoch 7951:\tTraining Loss - 0.0020\n",
      "Epoch 7952:\tTraining Loss - 0.0018\n",
      "Epoch 7953:\tTraining Loss - 0.0026\n",
      "Epoch 7954:\tTraining Loss - 0.0021\n",
      "Epoch 7955:\tTraining Loss - 0.0024\n",
      "Epoch 7956:\tTraining Loss - 0.0028\n",
      "Epoch 7957:\tTraining Loss - 0.0034\n",
      "Epoch 7958:\tTraining Loss - 0.0086\n",
      "Epoch 7959:\tTraining Loss - 0.0026\n",
      "Epoch 7960:\tTraining Loss - 0.0047\n",
      "Epoch 7961:\tTraining Loss - 0.0035\n",
      "Epoch 7962:\tTraining Loss - 0.0056\n",
      "Epoch 7963:\tTraining Loss - 0.0044\n",
      "Epoch 7964:\tTraining Loss - 0.0050\n",
      "Epoch 7965:\tTraining Loss - 0.0054\n",
      "Epoch 7966:\tTraining Loss - 0.0023\n",
      "Epoch 7967:\tTraining Loss - 0.0033\n",
      "Epoch 7968:\tTraining Loss - 0.0021\n",
      "Epoch 7969:\tTraining Loss - 0.0063\n",
      "Epoch 7970:\tTraining Loss - 0.0063\n",
      "Epoch 7971:\tTraining Loss - 0.0034\n",
      "Epoch 7972:\tTraining Loss - 0.0060\n",
      "Epoch 7973:\tTraining Loss - 0.0051\n",
      "Epoch 7974:\tTraining Loss - 0.0020\n",
      "Epoch 7975:\tTraining Loss - 0.0033\n",
      "Epoch 7976:\tTraining Loss - 0.0016\n",
      "Epoch 7977:\tTraining Loss - 0.0040\n",
      "Epoch 7978:\tTraining Loss - 0.0026\n",
      "Epoch 7979:\tTraining Loss - 0.0022\n",
      "Epoch 7980:\tTraining Loss - 0.0061\n",
      "Epoch 7981:\tTraining Loss - 0.0037\n",
      "Epoch 7982:\tTraining Loss - 0.0076\n",
      "Epoch 7983:\tTraining Loss - 0.0034\n",
      "Epoch 7984:\tTraining Loss - 0.0039\n",
      "Epoch 7985:\tTraining Loss - 0.0030\n",
      "Epoch 7986:\tTraining Loss - 0.0027\n",
      "Epoch 7987:\tTraining Loss - 0.0040\n",
      "Epoch 7988:\tTraining Loss - 0.0056\n",
      "Epoch 7989:\tTraining Loss - 0.0024\n",
      "Epoch 7990:\tTraining Loss - 0.0038\n",
      "Epoch 7991:\tTraining Loss - 0.0072\n",
      "Epoch 7992:\tTraining Loss - 0.0025\n",
      "Epoch 7993:\tTraining Loss - 0.0051\n",
      "Epoch 7994:\tTraining Loss - 0.0093\n",
      "Epoch 7995:\tTraining Loss - 0.0037\n",
      "Epoch 7996:\tTraining Loss - 0.0073\n",
      "Epoch 7997:\tTraining Loss - 0.0026\n",
      "Epoch 7998:\tTraining Loss - 0.0064\n",
      "Epoch 7999:\tTraining Loss - 0.0059\n",
      "Epoch 8000:\tTraining Loss - 0.0072\n",
      "Epoch 8001:\tTraining Loss - 0.0049\n",
      "Epoch 8002:\tTraining Loss - 0.0052\n",
      "Epoch 8003:\tTraining Loss - 0.0054\n",
      "Epoch 8004:\tTraining Loss - 0.0017\n",
      "Epoch 8005:\tTraining Loss - 0.0040\n",
      "Epoch 8006:\tTraining Loss - 0.0037\n",
      "Epoch 8007:\tTraining Loss - 0.0040\n",
      "Epoch 8008:\tTraining Loss - 0.0052\n",
      "Epoch 8009:\tTraining Loss - 0.0041\n",
      "Epoch 8010:\tTraining Loss - 0.0056\n",
      "Epoch 8011:\tTraining Loss - 0.0035\n",
      "Epoch 8012:\tTraining Loss - 0.0039\n",
      "Epoch 8013:\tTraining Loss - 0.0024\n",
      "Epoch 8014:\tTraining Loss - 0.0017\n",
      "Epoch 8015:\tTraining Loss - 0.0025\n",
      "Epoch 8016:\tTraining Loss - 0.0043\n",
      "Epoch 8017:\tTraining Loss - 0.0023\n",
      "Epoch 8018:\tTraining Loss - 0.0060\n",
      "Epoch 8019:\tTraining Loss - 0.0037\n",
      "Epoch 8020:\tTraining Loss - 0.0025\n",
      "Epoch 8021:\tTraining Loss - 0.0022\n",
      "Epoch 8022:\tTraining Loss - 0.0025\n",
      "Epoch 8023:\tTraining Loss - 0.0057\n",
      "Epoch 8024:\tTraining Loss - 0.0022\n",
      "Epoch 8025:\tTraining Loss - 0.0059\n",
      "Epoch 8026:\tTraining Loss - 0.0062\n",
      "Epoch 8027:\tTraining Loss - 0.0057\n",
      "Epoch 8028:\tTraining Loss - 0.0023\n",
      "Epoch 8029:\tTraining Loss - 0.0037\n",
      "Epoch 8030:\tTraining Loss - 0.0031\n",
      "Epoch 8031:\tTraining Loss - 0.0021\n",
      "Epoch 8032:\tTraining Loss - 0.0028\n",
      "Epoch 8033:\tTraining Loss - 0.0032\n",
      "Epoch 8034:\tTraining Loss - 0.0032\n",
      "Epoch 8035:\tTraining Loss - 0.0038\n",
      "Epoch 8036:\tTraining Loss - 0.0022\n",
      "Epoch 8037:\tTraining Loss - 0.0033\n",
      "Epoch 8038:\tTraining Loss - 0.0032\n",
      "Epoch 8039:\tTraining Loss - 0.0019\n",
      "Epoch 8040:\tTraining Loss - 0.0030\n",
      "Epoch 8041:\tTraining Loss - 0.0038\n",
      "Epoch 8042:\tTraining Loss - 0.0029\n",
      "Epoch 8043:\tTraining Loss - 0.0030\n",
      "Epoch 8044:\tTraining Loss - 0.0042\n",
      "Epoch 8045:\tTraining Loss - 0.0030\n",
      "Epoch 8046:\tTraining Loss - 0.0038\n",
      "Epoch 8047:\tTraining Loss - 0.0031\n",
      "Epoch 8048:\tTraining Loss - 0.0045\n",
      "Epoch 8049:\tTraining Loss - 0.0028\n",
      "Epoch 8050:\tTraining Loss - 0.0027\n",
      "Epoch 8051:\tTraining Loss - 0.0047\n",
      "Epoch 8052:\tTraining Loss - 0.0026\n",
      "Epoch 8053:\tTraining Loss - 0.0026\n",
      "Epoch 8054:\tTraining Loss - 0.0042\n",
      "Epoch 8055:\tTraining Loss - 0.0051\n",
      "Epoch 8056:\tTraining Loss - 0.0051\n",
      "Epoch 8057:\tTraining Loss - 0.0047\n",
      "Epoch 8058:\tTraining Loss - 0.0044\n",
      "Epoch 8059:\tTraining Loss - 0.0057\n",
      "Epoch 8060:\tTraining Loss - 0.0058\n",
      "Epoch 8061:\tTraining Loss - 0.0017\n",
      "Epoch 8062:\tTraining Loss - 0.0026\n",
      "Epoch 8063:\tTraining Loss - 0.0017\n",
      "Epoch 8064:\tTraining Loss - 0.0031\n",
      "Epoch 8065:\tTraining Loss - 0.0033\n",
      "Epoch 8066:\tTraining Loss - 0.0037\n",
      "Epoch 8067:\tTraining Loss - 0.0045\n",
      "Epoch 8068:\tTraining Loss - 0.0042\n",
      "Epoch 8069:\tTraining Loss - 0.0044\n",
      "Epoch 8070:\tTraining Loss - 0.0067\n",
      "Epoch 8071:\tTraining Loss - 0.0036\n",
      "Epoch 8072:\tTraining Loss - 0.0038\n",
      "Epoch 8073:\tTraining Loss - 0.0046\n",
      "Epoch 8074:\tTraining Loss - 0.0024\n",
      "Epoch 8075:\tTraining Loss - 0.0033\n",
      "Epoch 8076:\tTraining Loss - 0.0025\n",
      "Epoch 8077:\tTraining Loss - 0.0053\n",
      "Epoch 8078:\tTraining Loss - 0.0068\n",
      "Epoch 8079:\tTraining Loss - 0.0039\n",
      "Epoch 8080:\tTraining Loss - 0.0039\n",
      "Epoch 8081:\tTraining Loss - 0.0051\n",
      "Epoch 8082:\tTraining Loss - 0.0034\n",
      "Epoch 8083:\tTraining Loss - 0.0061\n",
      "Epoch 8084:\tTraining Loss - 0.0040\n",
      "Epoch 8085:\tTraining Loss - 0.0070\n",
      "Epoch 8086:\tTraining Loss - 0.0031\n",
      "Epoch 8087:\tTraining Loss - 0.0028\n",
      "Epoch 8088:\tTraining Loss - 0.0016\n",
      "Epoch 8089:\tTraining Loss - 0.0036\n",
      "Epoch 8090:\tTraining Loss - 0.0072\n",
      "Epoch 8091:\tTraining Loss - 0.0063\n",
      "Epoch 8092:\tTraining Loss - 0.0059\n",
      "Epoch 8093:\tTraining Loss - 0.0079\n",
      "Epoch 8094:\tTraining Loss - 0.0048\n",
      "Epoch 8095:\tTraining Loss - 0.0025\n",
      "Epoch 8096:\tTraining Loss - 0.0041\n",
      "Epoch 8097:\tTraining Loss - 0.0015\n",
      "Epoch 8098:\tTraining Loss - 0.0042\n",
      "Epoch 8099:\tTraining Loss - 0.0051\n",
      "Epoch 8100:\tTraining Loss - 0.0044\n",
      "Epoch 8101:\tTraining Loss - 0.0044\n",
      "Epoch 8102:\tTraining Loss - 0.0041\n",
      "Epoch 8103:\tTraining Loss - 0.0022\n",
      "Epoch 8104:\tTraining Loss - 0.0031\n",
      "Epoch 8105:\tTraining Loss - 0.0062\n",
      "Epoch 8106:\tTraining Loss - 0.0092\n",
      "Epoch 8107:\tTraining Loss - 0.0032\n",
      "Epoch 8108:\tTraining Loss - 0.0065\n",
      "Epoch 8109:\tTraining Loss - 0.0064\n",
      "Epoch 8110:\tTraining Loss - 0.0033\n",
      "Epoch 8111:\tTraining Loss - 0.0056\n",
      "Epoch 8112:\tTraining Loss - 0.0086\n",
      "Epoch 8113:\tTraining Loss - 0.0040\n",
      "Epoch 8114:\tTraining Loss - 0.0023\n",
      "Epoch 8115:\tTraining Loss - 0.0044\n",
      "Epoch 8116:\tTraining Loss - 0.0042\n",
      "Epoch 8117:\tTraining Loss - 0.0037\n",
      "Epoch 8118:\tTraining Loss - 0.0041\n",
      "Epoch 8119:\tTraining Loss - 0.0037\n",
      "Epoch 8120:\tTraining Loss - 0.0034\n",
      "Epoch 8121:\tTraining Loss - 0.0060\n",
      "Epoch 8122:\tTraining Loss - 0.0055\n",
      "Epoch 8123:\tTraining Loss - 0.0039\n",
      "Epoch 8124:\tTraining Loss - 0.0071\n",
      "Epoch 8125:\tTraining Loss - 0.0066\n",
      "Epoch 8126:\tTraining Loss - 0.0035\n",
      "Epoch 8127:\tTraining Loss - 0.0023\n",
      "Epoch 8128:\tTraining Loss - 0.0060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8129:\tTraining Loss - 0.0029\n",
      "Epoch 8130:\tTraining Loss - 0.0019\n",
      "Epoch 8131:\tTraining Loss - 0.0082\n",
      "Epoch 8132:\tTraining Loss - 0.0052\n",
      "Epoch 8133:\tTraining Loss - 0.0020\n",
      "Epoch 8134:\tTraining Loss - 0.0037\n",
      "Epoch 8135:\tTraining Loss - 0.0033\n",
      "Epoch 8136:\tTraining Loss - 0.0047\n",
      "Epoch 8137:\tTraining Loss - 0.0052\n",
      "Epoch 8138:\tTraining Loss - 0.0070\n",
      "Epoch 8139:\tTraining Loss - 0.0018\n",
      "Epoch 8140:\tTraining Loss - 0.0041\n",
      "Epoch 8141:\tTraining Loss - 0.0024\n",
      "Epoch 8142:\tTraining Loss - 0.0048\n",
      "Epoch 8143:\tTraining Loss - 0.0055\n",
      "Epoch 8144:\tTraining Loss - 0.0059\n",
      "Epoch 8145:\tTraining Loss - 0.0038\n",
      "Epoch 8146:\tTraining Loss - 0.0036\n",
      "Epoch 8147:\tTraining Loss - 0.0022\n",
      "Epoch 8148:\tTraining Loss - 0.0034\n",
      "Epoch 8149:\tTraining Loss - 0.0033\n",
      "Epoch 8150:\tTraining Loss - 0.0045\n",
      "Epoch 8151:\tTraining Loss - 0.0056\n",
      "Epoch 8152:\tTraining Loss - 0.0017\n",
      "Epoch 8153:\tTraining Loss - 0.0023\n",
      "Epoch 8154:\tTraining Loss - 0.0079\n",
      "Epoch 8155:\tTraining Loss - 0.0062\n",
      "Epoch 8156:\tTraining Loss - 0.0032\n",
      "Epoch 8157:\tTraining Loss - 0.0028\n",
      "Epoch 8158:\tTraining Loss - 0.0050\n",
      "Epoch 8159:\tTraining Loss - 0.0051\n",
      "Epoch 8160:\tTraining Loss - 0.0057\n",
      "Epoch 8161:\tTraining Loss - 0.0065\n",
      "Epoch 8162:\tTraining Loss - 0.0022\n",
      "Epoch 8163:\tTraining Loss - 0.0061\n",
      "Epoch 8164:\tTraining Loss - 0.0059\n",
      "Epoch 8165:\tTraining Loss - 0.0036\n",
      "Epoch 8166:\tTraining Loss - 0.0032\n",
      "Epoch 8167:\tTraining Loss - 0.0013\n",
      "Epoch 8168:\tTraining Loss - 0.0032\n",
      "Epoch 8169:\tTraining Loss - 0.0070\n",
      "Epoch 8170:\tTraining Loss - 0.0038\n",
      "Epoch 8171:\tTraining Loss - 0.0031\n",
      "Epoch 8172:\tTraining Loss - 0.0033\n",
      "Epoch 8173:\tTraining Loss - 0.0050\n",
      "Epoch 8174:\tTraining Loss - 0.0036\n",
      "Epoch 8175:\tTraining Loss - 0.0031\n",
      "Epoch 8176:\tTraining Loss - 0.0032\n",
      "Epoch 8177:\tTraining Loss - 0.0072\n",
      "Epoch 8178:\tTraining Loss - 0.0036\n",
      "Epoch 8179:\tTraining Loss - 0.0060\n",
      "Epoch 8180:\tTraining Loss - 0.0056\n",
      "Epoch 8181:\tTraining Loss - 0.0079\n",
      "Epoch 8182:\tTraining Loss - 0.0072\n",
      "Epoch 8183:\tTraining Loss - 0.0032\n",
      "Epoch 8184:\tTraining Loss - 0.0037\n",
      "Epoch 8185:\tTraining Loss - 0.0049\n",
      "Epoch 8186:\tTraining Loss - 0.0017\n",
      "Epoch 8187:\tTraining Loss - 0.0018\n",
      "Epoch 8188:\tTraining Loss - 0.0034\n",
      "Epoch 8189:\tTraining Loss - 0.0048\n",
      "Epoch 8190:\tTraining Loss - 0.0052\n",
      "Epoch 8191:\tTraining Loss - 0.0042\n",
      "Epoch 8192:\tTraining Loss - 0.0017\n",
      "Epoch 8193:\tTraining Loss - 0.0030\n",
      "Epoch 8194:\tTraining Loss - 0.0012\n",
      "Epoch 8195:\tTraining Loss - 0.0065\n",
      "Epoch 8196:\tTraining Loss - 0.0062\n",
      "Epoch 8197:\tTraining Loss - 0.0072\n",
      "Epoch 8198:\tTraining Loss - 0.0028\n",
      "Epoch 8199:\tTraining Loss - 0.0036\n",
      "Epoch 8200:\tTraining Loss - 0.0020\n",
      "Epoch 8201:\tTraining Loss - 0.0039\n",
      "Epoch 8202:\tTraining Loss - 0.0052\n",
      "Epoch 8203:\tTraining Loss - 0.0022\n",
      "Epoch 8204:\tTraining Loss - 0.0013\n",
      "Epoch 8205:\tTraining Loss - 0.0069\n",
      "Epoch 8206:\tTraining Loss - 0.0029\n",
      "Epoch 8207:\tTraining Loss - 0.0050\n",
      "Epoch 8208:\tTraining Loss - 0.0034\n",
      "Epoch 8209:\tTraining Loss - 0.0021\n",
      "Epoch 8210:\tTraining Loss - 0.0040\n",
      "Epoch 8211:\tTraining Loss - 0.0030\n",
      "Epoch 8212:\tTraining Loss - 0.0035\n",
      "Epoch 8213:\tTraining Loss - 0.0081\n",
      "Epoch 8214:\tTraining Loss - 0.0044\n",
      "Epoch 8215:\tTraining Loss - 0.0077\n",
      "Epoch 8216:\tTraining Loss - 0.0027\n",
      "Epoch 8217:\tTraining Loss - 0.0045\n",
      "Epoch 8218:\tTraining Loss - 0.0023\n",
      "Epoch 8219:\tTraining Loss - 0.0016\n",
      "Epoch 8220:\tTraining Loss - 0.0020\n",
      "Epoch 8221:\tTraining Loss - 0.0077\n",
      "Epoch 8222:\tTraining Loss - 0.0080\n",
      "Epoch 8223:\tTraining Loss - 0.0018\n",
      "Epoch 8224:\tTraining Loss - 0.0051\n",
      "Epoch 8225:\tTraining Loss - 0.0038\n",
      "Epoch 8226:\tTraining Loss - 0.0018\n",
      "Epoch 8227:\tTraining Loss - 0.0053\n",
      "Epoch 8228:\tTraining Loss - 0.0033\n",
      "Epoch 8229:\tTraining Loss - 0.0011\n",
      "Epoch 8230:\tTraining Loss - 0.0037\n",
      "Epoch 8231:\tTraining Loss - 0.0053\n",
      "Epoch 8232:\tTraining Loss - 0.0036\n",
      "Epoch 8233:\tTraining Loss - 0.0047\n",
      "Epoch 8234:\tTraining Loss - 0.0051\n",
      "Epoch 8235:\tTraining Loss - 0.0033\n",
      "Epoch 8236:\tTraining Loss - 0.0055\n",
      "Epoch 8237:\tTraining Loss - 0.0034\n",
      "Epoch 8238:\tTraining Loss - 0.0038\n",
      "Epoch 8239:\tTraining Loss - 0.0062\n",
      "Epoch 8240:\tTraining Loss - 0.0018\n",
      "Epoch 8241:\tTraining Loss - 0.0020\n",
      "Epoch 8242:\tTraining Loss - 0.0082\n",
      "Epoch 8243:\tTraining Loss - 0.0023\n",
      "Epoch 8244:\tTraining Loss - 0.0071\n",
      "Epoch 8245:\tTraining Loss - 0.0113\n",
      "Epoch 8246:\tTraining Loss - 0.0025\n",
      "Epoch 8247:\tTraining Loss - 0.0033\n",
      "Epoch 8248:\tTraining Loss - 0.0050\n",
      "Epoch 8249:\tTraining Loss - 0.0037\n",
      "Epoch 8250:\tTraining Loss - 0.0041\n",
      "Epoch 8251:\tTraining Loss - 0.0073\n",
      "Epoch 8252:\tTraining Loss - 0.0020\n",
      "Epoch 8253:\tTraining Loss - 0.0024\n",
      "Epoch 8254:\tTraining Loss - 0.0048\n",
      "Epoch 8255:\tTraining Loss - 0.0054\n",
      "Epoch 8256:\tTraining Loss - 0.0054\n",
      "Epoch 8257:\tTraining Loss - 0.0029\n",
      "Epoch 8258:\tTraining Loss - 0.0050\n",
      "Epoch 8259:\tTraining Loss - 0.0061\n",
      "Epoch 8260:\tTraining Loss - 0.0036\n",
      "Epoch 8261:\tTraining Loss - 0.0091\n",
      "Epoch 8262:\tTraining Loss - 0.0023\n",
      "Epoch 8263:\tTraining Loss - 0.0036\n",
      "Epoch 8264:\tTraining Loss - 0.0019\n",
      "Epoch 8265:\tTraining Loss - 0.0047\n",
      "Epoch 8266:\tTraining Loss - 0.0043\n",
      "Epoch 8267:\tTraining Loss - 0.0014\n",
      "Epoch 8268:\tTraining Loss - 0.0050\n",
      "Epoch 8269:\tTraining Loss - 0.0043\n",
      "Epoch 8270:\tTraining Loss - 0.0026\n",
      "Epoch 8271:\tTraining Loss - 0.0016\n",
      "Epoch 8272:\tTraining Loss - 0.0049\n",
      "Epoch 8273:\tTraining Loss - 0.0086\n",
      "Epoch 8274:\tTraining Loss - 0.0039\n",
      "Epoch 8275:\tTraining Loss - 0.0023\n",
      "Epoch 8276:\tTraining Loss - 0.0054\n",
      "Epoch 8277:\tTraining Loss - 0.0064\n",
      "Epoch 8278:\tTraining Loss - 0.0048\n",
      "Epoch 8279:\tTraining Loss - 0.0044\n",
      "Epoch 8280:\tTraining Loss - 0.0027\n",
      "Epoch 8281:\tTraining Loss - 0.0077\n",
      "Epoch 8282:\tTraining Loss - 0.0033\n",
      "Epoch 8283:\tTraining Loss - 0.0046\n",
      "Epoch 8284:\tTraining Loss - 0.0009\n",
      "Epoch 8285:\tTraining Loss - 0.0019\n",
      "Epoch 8286:\tTraining Loss - 0.0042\n",
      "Epoch 8287:\tTraining Loss - 0.0033\n",
      "Epoch 8288:\tTraining Loss - 0.0032\n",
      "Epoch 8289:\tTraining Loss - 0.0047\n",
      "Epoch 8290:\tTraining Loss - 0.0026\n",
      "Epoch 8291:\tTraining Loss - 0.0039\n",
      "Epoch 8292:\tTraining Loss - 0.0042\n",
      "Epoch 8293:\tTraining Loss - 0.0034\n",
      "Epoch 8294:\tTraining Loss - 0.0032\n",
      "Epoch 8295:\tTraining Loss - 0.0040\n",
      "Epoch 8296:\tTraining Loss - 0.0082\n",
      "Epoch 8297:\tTraining Loss - 0.0037\n",
      "Epoch 8298:\tTraining Loss - 0.0065\n",
      "Epoch 8299:\tTraining Loss - 0.0071\n",
      "Epoch 8300:\tTraining Loss - 0.0042\n",
      "Epoch 8301:\tTraining Loss - 0.0039\n",
      "Epoch 8302:\tTraining Loss - 0.0078\n",
      "Epoch 8303:\tTraining Loss - 0.0028\n",
      "Epoch 8304:\tTraining Loss - 0.0034\n",
      "Epoch 8305:\tTraining Loss - 0.0048\n",
      "Epoch 8306:\tTraining Loss - 0.0070\n",
      "Epoch 8307:\tTraining Loss - 0.0034\n",
      "Epoch 8308:\tTraining Loss - 0.0039\n",
      "Epoch 8309:\tTraining Loss - 0.0037\n",
      "Epoch 8310:\tTraining Loss - 0.0047\n",
      "Epoch 8311:\tTraining Loss - 0.0047\n",
      "Epoch 8312:\tTraining Loss - 0.0082\n",
      "Epoch 8313:\tTraining Loss - 0.0021\n",
      "Epoch 8314:\tTraining Loss - 0.0024\n",
      "Epoch 8315:\tTraining Loss - 0.0059\n",
      "Epoch 8316:\tTraining Loss - 0.0038\n",
      "Epoch 8317:\tTraining Loss - 0.0044\n",
      "Epoch 8318:\tTraining Loss - 0.0024\n",
      "Epoch 8319:\tTraining Loss - 0.0048\n",
      "Epoch 8320:\tTraining Loss - 0.0051\n",
      "Epoch 8321:\tTraining Loss - 0.0048\n",
      "Epoch 8322:\tTraining Loss - 0.0043\n",
      "Epoch 8323:\tTraining Loss - 0.0026\n",
      "Epoch 8324:\tTraining Loss - 0.0071\n",
      "Epoch 8325:\tTraining Loss - 0.0031\n",
      "Epoch 8326:\tTraining Loss - 0.0025\n",
      "Epoch 8327:\tTraining Loss - 0.0070\n",
      "Epoch 8328:\tTraining Loss - 0.0019\n",
      "Epoch 8329:\tTraining Loss - 0.0024\n",
      "Epoch 8330:\tTraining Loss - 0.0021\n",
      "Epoch 8331:\tTraining Loss - 0.0033\n",
      "Epoch 8332:\tTraining Loss - 0.0047\n",
      "Epoch 8333:\tTraining Loss - 0.0047\n",
      "Epoch 8334:\tTraining Loss - 0.0057\n",
      "Epoch 8335:\tTraining Loss - 0.0085\n",
      "Epoch 8336:\tTraining Loss - 0.0065\n",
      "Epoch 8337:\tTraining Loss - 0.0039\n",
      "Epoch 8338:\tTraining Loss - 0.0049\n",
      "Epoch 8339:\tTraining Loss - 0.0053\n",
      "Epoch 8340:\tTraining Loss - 0.0072\n",
      "Epoch 8341:\tTraining Loss - 0.0023\n",
      "Epoch 8342:\tTraining Loss - 0.0026\n",
      "Epoch 8343:\tTraining Loss - 0.0027\n",
      "Epoch 8344:\tTraining Loss - 0.0055\n",
      "Epoch 8345:\tTraining Loss - 0.0024\n",
      "Epoch 8346:\tTraining Loss - 0.0017\n",
      "Epoch 8347:\tTraining Loss - 0.0024\n",
      "Epoch 8348:\tTraining Loss - 0.0039\n",
      "Epoch 8349:\tTraining Loss - 0.0049\n",
      "Epoch 8350:\tTraining Loss - 0.0018\n",
      "Epoch 8351:\tTraining Loss - 0.0040\n",
      "Epoch 8352:\tTraining Loss - 0.0023\n",
      "Epoch 8353:\tTraining Loss - 0.0096\n",
      "Epoch 8354:\tTraining Loss - 0.0023\n",
      "Epoch 8355:\tTraining Loss - 0.0024\n",
      "Epoch 8356:\tTraining Loss - 0.0070\n",
      "Epoch 8357:\tTraining Loss - 0.0052\n",
      "Epoch 8358:\tTraining Loss - 0.0050\n",
      "Epoch 8359:\tTraining Loss - 0.0028\n",
      "Epoch 8360:\tTraining Loss - 0.0055\n",
      "Epoch 8361:\tTraining Loss - 0.0023\n",
      "Epoch 8362:\tTraining Loss - 0.0037\n",
      "Epoch 8363:\tTraining Loss - 0.0082\n",
      "Epoch 8364:\tTraining Loss - 0.0064\n",
      "Epoch 8365:\tTraining Loss - 0.0045\n",
      "Epoch 8366:\tTraining Loss - 0.0022\n",
      "Epoch 8367:\tTraining Loss - 0.0077\n",
      "Epoch 8368:\tTraining Loss - 0.0052\n",
      "Epoch 8369:\tTraining Loss - 0.0049\n",
      "Epoch 8370:\tTraining Loss - 0.0033\n",
      "Epoch 8371:\tTraining Loss - 0.0043\n",
      "Epoch 8372:\tTraining Loss - 0.0075\n",
      "Epoch 8373:\tTraining Loss - 0.0037\n",
      "Epoch 8374:\tTraining Loss - 0.0044\n",
      "Epoch 8375:\tTraining Loss - 0.0028\n",
      "Epoch 8376:\tTraining Loss - 0.0060\n",
      "Epoch 8377:\tTraining Loss - 0.0035\n",
      "Epoch 8378:\tTraining Loss - 0.0041\n",
      "Epoch 8379:\tTraining Loss - 0.0045\n",
      "Epoch 8380:\tTraining Loss - 0.0046\n",
      "Epoch 8381:\tTraining Loss - 0.0020\n",
      "Epoch 8382:\tTraining Loss - 0.0049\n",
      "Epoch 8383:\tTraining Loss - 0.0052\n",
      "Epoch 8384:\tTraining Loss - 0.0078\n",
      "Epoch 8385:\tTraining Loss - 0.0041\n",
      "Epoch 8386:\tTraining Loss - 0.0028\n",
      "Epoch 8387:\tTraining Loss - 0.0015\n",
      "Epoch 8388:\tTraining Loss - 0.0038\n",
      "Epoch 8389:\tTraining Loss - 0.0029\n",
      "Epoch 8390:\tTraining Loss - 0.0046\n",
      "Epoch 8391:\tTraining Loss - 0.0099\n",
      "Epoch 8392:\tTraining Loss - 0.0029\n",
      "Epoch 8393:\tTraining Loss - 0.0068\n",
      "Epoch 8394:\tTraining Loss - 0.0018\n",
      "Epoch 8395:\tTraining Loss - 0.0017\n",
      "Epoch 8396:\tTraining Loss - 0.0032\n",
      "Epoch 8397:\tTraining Loss - 0.0035\n",
      "Epoch 8398:\tTraining Loss - 0.0026\n",
      "Epoch 8399:\tTraining Loss - 0.0056\n",
      "Epoch 8400:\tTraining Loss - 0.0065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8401:\tTraining Loss - 0.0044\n",
      "Epoch 8402:\tTraining Loss - 0.0050\n",
      "Epoch 8403:\tTraining Loss - 0.0026\n",
      "Epoch 8404:\tTraining Loss - 0.0046\n",
      "Epoch 8405:\tTraining Loss - 0.0053\n",
      "Epoch 8406:\tTraining Loss - 0.0025\n",
      "Epoch 8407:\tTraining Loss - 0.0028\n",
      "Epoch 8408:\tTraining Loss - 0.0043\n",
      "Epoch 8409:\tTraining Loss - 0.0054\n",
      "Epoch 8410:\tTraining Loss - 0.0042\n",
      "Epoch 8411:\tTraining Loss - 0.0023\n",
      "Epoch 8412:\tTraining Loss - 0.0027\n",
      "Epoch 8413:\tTraining Loss - 0.0085\n",
      "Epoch 8414:\tTraining Loss - 0.0058\n",
      "Epoch 8415:\tTraining Loss - 0.0017\n",
      "Epoch 8416:\tTraining Loss - 0.0049\n",
      "Epoch 8417:\tTraining Loss - 0.0059\n",
      "Epoch 8418:\tTraining Loss - 0.0059\n",
      "Epoch 8419:\tTraining Loss - 0.0065\n",
      "Epoch 8420:\tTraining Loss - 0.0033\n",
      "Epoch 8421:\tTraining Loss - 0.0040\n",
      "Epoch 8422:\tTraining Loss - 0.0028\n",
      "Epoch 8423:\tTraining Loss - 0.0030\n",
      "Epoch 8424:\tTraining Loss - 0.0016\n",
      "Epoch 8425:\tTraining Loss - 0.0053\n",
      "Epoch 8426:\tTraining Loss - 0.0016\n",
      "Epoch 8427:\tTraining Loss - 0.0031\n",
      "Epoch 8428:\tTraining Loss - 0.0030\n",
      "Epoch 8429:\tTraining Loss - 0.0049\n",
      "Epoch 8430:\tTraining Loss - 0.0054\n",
      "Epoch 8431:\tTraining Loss - 0.0062\n",
      "Epoch 8432:\tTraining Loss - 0.0056\n",
      "Epoch 8433:\tTraining Loss - 0.0111\n",
      "Epoch 8434:\tTraining Loss - 0.0031\n",
      "Epoch 8435:\tTraining Loss - 0.0051\n",
      "Epoch 8436:\tTraining Loss - 0.0100\n",
      "Epoch 8437:\tTraining Loss - 0.0055\n",
      "Epoch 8438:\tTraining Loss - 0.0040\n",
      "Epoch 8439:\tTraining Loss - 0.0050\n",
      "Epoch 8440:\tTraining Loss - 0.0066\n",
      "Epoch 8441:\tTraining Loss - 0.0053\n",
      "Epoch 8442:\tTraining Loss - 0.0020\n",
      "Epoch 8443:\tTraining Loss - 0.0045\n",
      "Epoch 8444:\tTraining Loss - 0.0039\n",
      "Epoch 8445:\tTraining Loss - 0.0028\n",
      "Epoch 8446:\tTraining Loss - 0.0029\n",
      "Epoch 8447:\tTraining Loss - 0.0070\n",
      "Epoch 8448:\tTraining Loss - 0.0035\n",
      "Epoch 8449:\tTraining Loss - 0.0016\n",
      "Epoch 8450:\tTraining Loss - 0.0046\n",
      "Epoch 8451:\tTraining Loss - 0.0016\n",
      "Epoch 8452:\tTraining Loss - 0.0097\n",
      "Epoch 8453:\tTraining Loss - 0.0039\n",
      "Epoch 8454:\tTraining Loss - 0.0028\n",
      "Epoch 8455:\tTraining Loss - 0.0042\n",
      "Epoch 8456:\tTraining Loss - 0.0027\n",
      "Epoch 8457:\tTraining Loss - 0.0041\n",
      "Epoch 8458:\tTraining Loss - 0.0045\n",
      "Epoch 8459:\tTraining Loss - 0.0038\n",
      "Epoch 8460:\tTraining Loss - 0.0038\n",
      "Epoch 8461:\tTraining Loss - 0.0025\n",
      "Epoch 8462:\tTraining Loss - 0.0024\n",
      "Epoch 8463:\tTraining Loss - 0.0076\n",
      "Epoch 8464:\tTraining Loss - 0.0039\n",
      "Epoch 8465:\tTraining Loss - 0.0050\n",
      "Epoch 8466:\tTraining Loss - 0.0036\n",
      "Epoch 8467:\tTraining Loss - 0.0047\n",
      "Epoch 8468:\tTraining Loss - 0.0018\n",
      "Epoch 8469:\tTraining Loss - 0.0020\n",
      "Epoch 8470:\tTraining Loss - 0.0014\n",
      "Epoch 8471:\tTraining Loss - 0.0021\n",
      "Epoch 8472:\tTraining Loss - 0.0024\n",
      "Epoch 8473:\tTraining Loss - 0.0023\n",
      "Epoch 8474:\tTraining Loss - 0.0031\n",
      "Epoch 8475:\tTraining Loss - 0.0044\n",
      "Epoch 8476:\tTraining Loss - 0.0074\n",
      "Epoch 8477:\tTraining Loss - 0.0081\n",
      "Epoch 8478:\tTraining Loss - 0.0041\n",
      "Epoch 8479:\tTraining Loss - 0.0067\n",
      "Epoch 8480:\tTraining Loss - 0.0024\n",
      "Epoch 8481:\tTraining Loss - 0.0024\n",
      "Epoch 8482:\tTraining Loss - 0.0076\n",
      "Epoch 8483:\tTraining Loss - 0.0030\n",
      "Epoch 8484:\tTraining Loss - 0.0025\n",
      "Epoch 8485:\tTraining Loss - 0.0015\n",
      "Epoch 8486:\tTraining Loss - 0.0062\n",
      "Epoch 8487:\tTraining Loss - 0.0026\n",
      "Epoch 8488:\tTraining Loss - 0.0073\n",
      "Epoch 8489:\tTraining Loss - 0.0020\n",
      "Epoch 8490:\tTraining Loss - 0.0045\n",
      "Epoch 8491:\tTraining Loss - 0.0022\n",
      "Epoch 8492:\tTraining Loss - 0.0023\n",
      "Epoch 8493:\tTraining Loss - 0.0040\n",
      "Epoch 8494:\tTraining Loss - 0.0017\n",
      "Epoch 8495:\tTraining Loss - 0.0038\n",
      "Epoch 8496:\tTraining Loss - 0.0078\n",
      "Epoch 8497:\tTraining Loss - 0.0048\n",
      "Epoch 8498:\tTraining Loss - 0.0072\n",
      "Epoch 8499:\tTraining Loss - 0.0033\n",
      "Epoch 8500:\tTraining Loss - 0.0025\n",
      "Epoch 8501:\tTraining Loss - 0.0066\n",
      "Epoch 8502:\tTraining Loss - 0.0053\n",
      "Epoch 8503:\tTraining Loss - 0.0079\n",
      "Epoch 8504:\tTraining Loss - 0.0049\n",
      "Epoch 8505:\tTraining Loss - 0.0025\n",
      "Epoch 8506:\tTraining Loss - 0.0026\n",
      "Epoch 8507:\tTraining Loss - 0.0050\n",
      "Epoch 8508:\tTraining Loss - 0.0045\n",
      "Epoch 8509:\tTraining Loss - 0.0074\n",
      "Epoch 8510:\tTraining Loss - 0.0030\n",
      "Epoch 8511:\tTraining Loss - 0.0058\n",
      "Epoch 8512:\tTraining Loss - 0.0112\n",
      "Epoch 8513:\tTraining Loss - 0.0027\n",
      "Epoch 8514:\tTraining Loss - 0.0093\n",
      "Epoch 8515:\tTraining Loss - 0.0030\n",
      "Epoch 8516:\tTraining Loss - 0.0051\n",
      "Epoch 8517:\tTraining Loss - 0.0032\n",
      "Epoch 8518:\tTraining Loss - 0.0038\n",
      "Epoch 8519:\tTraining Loss - 0.0037\n",
      "Epoch 8520:\tTraining Loss - 0.0045\n",
      "Epoch 8521:\tTraining Loss - 0.0039\n",
      "Epoch 8522:\tTraining Loss - 0.0026\n",
      "Epoch 8523:\tTraining Loss - 0.0031\n",
      "Epoch 8524:\tTraining Loss - 0.0032\n",
      "Epoch 8525:\tTraining Loss - 0.0030\n",
      "Epoch 8526:\tTraining Loss - 0.0099\n",
      "Epoch 8527:\tTraining Loss - 0.0054\n",
      "Epoch 8528:\tTraining Loss - 0.0054\n",
      "Epoch 8529:\tTraining Loss - 0.0057\n",
      "Epoch 8530:\tTraining Loss - 0.0059\n",
      "Epoch 8531:\tTraining Loss - 0.0043\n",
      "Epoch 8532:\tTraining Loss - 0.0032\n",
      "Epoch 8533:\tTraining Loss - 0.0029\n",
      "Epoch 8534:\tTraining Loss - 0.0021\n",
      "Epoch 8535:\tTraining Loss - 0.0054\n",
      "Epoch 8536:\tTraining Loss - 0.0056\n",
      "Epoch 8537:\tTraining Loss - 0.0056\n",
      "Epoch 8538:\tTraining Loss - 0.0034\n",
      "Epoch 8539:\tTraining Loss - 0.0027\n",
      "Epoch 8540:\tTraining Loss - 0.0036\n",
      "Epoch 8541:\tTraining Loss - 0.0042\n",
      "Epoch 8542:\tTraining Loss - 0.0028\n",
      "Epoch 8543:\tTraining Loss - 0.0053\n",
      "Epoch 8544:\tTraining Loss - 0.0083\n",
      "Epoch 8545:\tTraining Loss - 0.0028\n",
      "Epoch 8546:\tTraining Loss - 0.0061\n",
      "Epoch 8547:\tTraining Loss - 0.0030\n",
      "Epoch 8548:\tTraining Loss - 0.0031\n",
      "Epoch 8549:\tTraining Loss - 0.0023\n",
      "Epoch 8550:\tTraining Loss - 0.0021\n",
      "Epoch 8551:\tTraining Loss - 0.0016\n",
      "Epoch 8552:\tTraining Loss - 0.0029\n",
      "Epoch 8553:\tTraining Loss - 0.0054\n",
      "Epoch 8554:\tTraining Loss - 0.0030\n",
      "Epoch 8555:\tTraining Loss - 0.0040\n",
      "Epoch 8556:\tTraining Loss - 0.0034\n",
      "Epoch 8557:\tTraining Loss - 0.0053\n",
      "Epoch 8558:\tTraining Loss - 0.0057\n",
      "Epoch 8559:\tTraining Loss - 0.0070\n",
      "Epoch 8560:\tTraining Loss - 0.0020\n",
      "Epoch 8561:\tTraining Loss - 0.0039\n",
      "Epoch 8562:\tTraining Loss - 0.0026\n",
      "Epoch 8563:\tTraining Loss - 0.0029\n",
      "Epoch 8564:\tTraining Loss - 0.0023\n",
      "Epoch 8565:\tTraining Loss - 0.0029\n",
      "Epoch 8566:\tTraining Loss - 0.0027\n",
      "Epoch 8567:\tTraining Loss - 0.0036\n",
      "Epoch 8568:\tTraining Loss - 0.0077\n",
      "Epoch 8569:\tTraining Loss - 0.0036\n",
      "Epoch 8570:\tTraining Loss - 0.0028\n",
      "Epoch 8571:\tTraining Loss - 0.0056\n",
      "Epoch 8572:\tTraining Loss - 0.0046\n",
      "Epoch 8573:\tTraining Loss - 0.0057\n",
      "Epoch 8574:\tTraining Loss - 0.0025\n",
      "Epoch 8575:\tTraining Loss - 0.0082\n",
      "Epoch 8576:\tTraining Loss - 0.0018\n",
      "Epoch 8577:\tTraining Loss - 0.0026\n",
      "Epoch 8578:\tTraining Loss - 0.0018\n",
      "Epoch 8579:\tTraining Loss - 0.0054\n",
      "Epoch 8580:\tTraining Loss - 0.0035\n",
      "Epoch 8581:\tTraining Loss - 0.0043\n",
      "Epoch 8582:\tTraining Loss - 0.0022\n",
      "Epoch 8583:\tTraining Loss - 0.0015\n",
      "Epoch 8584:\tTraining Loss - 0.0038\n",
      "Epoch 8585:\tTraining Loss - 0.0062\n",
      "Epoch 8586:\tTraining Loss - 0.0051\n",
      "Epoch 8587:\tTraining Loss - 0.0032\n",
      "Epoch 8588:\tTraining Loss - 0.0045\n",
      "Epoch 8589:\tTraining Loss - 0.0021\n",
      "Epoch 8590:\tTraining Loss - 0.0021\n",
      "Epoch 8591:\tTraining Loss - 0.0014\n",
      "Epoch 8592:\tTraining Loss - 0.0035\n",
      "Epoch 8593:\tTraining Loss - 0.0058\n",
      "Epoch 8594:\tTraining Loss - 0.0044\n",
      "Epoch 8595:\tTraining Loss - 0.0044\n",
      "Epoch 8596:\tTraining Loss - 0.0018\n",
      "Epoch 8597:\tTraining Loss - 0.0038\n",
      "Epoch 8598:\tTraining Loss - 0.0036\n",
      "Epoch 8599:\tTraining Loss - 0.0046\n",
      "Epoch 8600:\tTraining Loss - 0.0020\n",
      "Epoch 8601:\tTraining Loss - 0.0029\n",
      "Epoch 8602:\tTraining Loss - 0.0040\n",
      "Epoch 8603:\tTraining Loss - 0.0083\n",
      "Epoch 8604:\tTraining Loss - 0.0014\n",
      "Epoch 8605:\tTraining Loss - 0.0028\n",
      "Epoch 8606:\tTraining Loss - 0.0042\n",
      "Epoch 8607:\tTraining Loss - 0.0053\n",
      "Epoch 8608:\tTraining Loss - 0.0077\n",
      "Epoch 8609:\tTraining Loss - 0.0022\n",
      "Epoch 8610:\tTraining Loss - 0.0007\n",
      "Epoch 8611:\tTraining Loss - 0.0038\n",
      "Epoch 8612:\tTraining Loss - 0.0008\n",
      "Epoch 8613:\tTraining Loss - 0.0042\n",
      "Epoch 8614:\tTraining Loss - 0.0047\n",
      "Epoch 8615:\tTraining Loss - 0.0071\n",
      "Epoch 8616:\tTraining Loss - 0.0019\n",
      "Epoch 8617:\tTraining Loss - 0.0015\n",
      "Epoch 8618:\tTraining Loss - 0.0041\n",
      "Epoch 8619:\tTraining Loss - 0.0050\n",
      "Epoch 8620:\tTraining Loss - 0.0041\n",
      "Epoch 8621:\tTraining Loss - 0.0025\n",
      "Epoch 8622:\tTraining Loss - 0.0061\n",
      "Epoch 8623:\tTraining Loss - 0.0037\n",
      "Epoch 8624:\tTraining Loss - 0.0093\n",
      "Epoch 8625:\tTraining Loss - 0.0023\n",
      "Epoch 8626:\tTraining Loss - 0.0049\n",
      "Epoch 8627:\tTraining Loss - 0.0055\n",
      "Epoch 8628:\tTraining Loss - 0.0017\n",
      "Epoch 8629:\tTraining Loss - 0.0021\n",
      "Epoch 8630:\tTraining Loss - 0.0038\n",
      "Epoch 8631:\tTraining Loss - 0.0026\n",
      "Epoch 8632:\tTraining Loss - 0.0030\n",
      "Epoch 8633:\tTraining Loss - 0.0042\n",
      "Epoch 8634:\tTraining Loss - 0.0008\n",
      "Epoch 8635:\tTraining Loss - 0.0053\n",
      "Epoch 8636:\tTraining Loss - 0.0048\n",
      "Epoch 8637:\tTraining Loss - 0.0027\n",
      "Epoch 8638:\tTraining Loss - 0.0046\n",
      "Epoch 8639:\tTraining Loss - 0.0085\n",
      "Epoch 8640:\tTraining Loss - 0.0033\n",
      "Epoch 8641:\tTraining Loss - 0.0039\n",
      "Epoch 8642:\tTraining Loss - 0.0020\n",
      "Epoch 8643:\tTraining Loss - 0.0031\n",
      "Epoch 8644:\tTraining Loss - 0.0037\n",
      "Epoch 8645:\tTraining Loss - 0.0039\n",
      "Epoch 8646:\tTraining Loss - 0.0039\n",
      "Epoch 8647:\tTraining Loss - 0.0016\n",
      "Epoch 8648:\tTraining Loss - 0.0017\n",
      "Epoch 8649:\tTraining Loss - 0.0026\n",
      "Epoch 8650:\tTraining Loss - 0.0031\n",
      "Epoch 8651:\tTraining Loss - 0.0028\n",
      "Epoch 8652:\tTraining Loss - 0.0023\n",
      "Epoch 8653:\tTraining Loss - 0.0014\n",
      "Epoch 8654:\tTraining Loss - 0.0024\n",
      "Epoch 8655:\tTraining Loss - 0.0039\n",
      "Epoch 8656:\tTraining Loss - 0.0021\n",
      "Epoch 8657:\tTraining Loss - 0.0067\n",
      "Epoch 8658:\tTraining Loss - 0.0058\n",
      "Epoch 8659:\tTraining Loss - 0.0044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8660:\tTraining Loss - 0.0029\n",
      "Epoch 8661:\tTraining Loss - 0.0036\n",
      "Epoch 8662:\tTraining Loss - 0.0028\n",
      "Epoch 8663:\tTraining Loss - 0.0052\n",
      "Epoch 8664:\tTraining Loss - 0.0028\n",
      "Epoch 8665:\tTraining Loss - 0.0024\n",
      "Epoch 8666:\tTraining Loss - 0.0029\n",
      "Epoch 8667:\tTraining Loss - 0.0024\n",
      "Epoch 8668:\tTraining Loss - 0.0036\n",
      "Epoch 8669:\tTraining Loss - 0.0055\n",
      "Epoch 8670:\tTraining Loss - 0.0013\n",
      "Epoch 8671:\tTraining Loss - 0.0014\n",
      "Epoch 8672:\tTraining Loss - 0.0017\n",
      "Epoch 8673:\tTraining Loss - 0.0014\n",
      "Epoch 8674:\tTraining Loss - 0.0057\n",
      "Epoch 8675:\tTraining Loss - 0.0027\n",
      "Epoch 8676:\tTraining Loss - 0.0010\n",
      "Epoch 8677:\tTraining Loss - 0.0042\n",
      "Epoch 8678:\tTraining Loss - 0.0017\n",
      "Epoch 8679:\tTraining Loss - 0.0051\n",
      "Epoch 8680:\tTraining Loss - 0.0052\n",
      "Epoch 8681:\tTraining Loss - 0.0019\n",
      "Epoch 8682:\tTraining Loss - 0.0024\n",
      "Epoch 8683:\tTraining Loss - 0.0030\n",
      "Epoch 8684:\tTraining Loss - 0.0026\n",
      "Epoch 8685:\tTraining Loss - 0.0056\n",
      "Epoch 8686:\tTraining Loss - 0.0043\n",
      "Epoch 8687:\tTraining Loss - 0.0062\n",
      "Epoch 8688:\tTraining Loss - 0.0022\n",
      "Epoch 8689:\tTraining Loss - 0.0030\n",
      "Epoch 8690:\tTraining Loss - 0.0025\n",
      "Epoch 8691:\tTraining Loss - 0.0019\n",
      "Epoch 8692:\tTraining Loss - 0.0045\n",
      "Epoch 8693:\tTraining Loss - 0.0023\n",
      "Epoch 8694:\tTraining Loss - 0.0030\n",
      "Epoch 8695:\tTraining Loss - 0.0026\n",
      "Epoch 8696:\tTraining Loss - 0.0047\n",
      "Epoch 8697:\tTraining Loss - 0.0053\n",
      "Epoch 8698:\tTraining Loss - 0.0065\n",
      "Epoch 8699:\tTraining Loss - 0.0010\n",
      "Epoch 8700:\tTraining Loss - 0.0090\n",
      "Epoch 8701:\tTraining Loss - 0.0041\n",
      "Epoch 8702:\tTraining Loss - 0.0041\n",
      "Epoch 8703:\tTraining Loss - 0.0031\n",
      "Epoch 8704:\tTraining Loss - 0.0018\n",
      "Epoch 8705:\tTraining Loss - 0.0028\n",
      "Epoch 8706:\tTraining Loss - 0.0034\n",
      "Epoch 8707:\tTraining Loss - 0.0017\n",
      "Epoch 8708:\tTraining Loss - 0.0013\n",
      "Epoch 8709:\tTraining Loss - 0.0083\n",
      "Epoch 8710:\tTraining Loss - 0.0033\n",
      "Epoch 8711:\tTraining Loss - 0.0023\n",
      "Epoch 8712:\tTraining Loss - 0.0049\n",
      "Epoch 8713:\tTraining Loss - 0.0031\n",
      "Epoch 8714:\tTraining Loss - 0.0007\n",
      "Epoch 8715:\tTraining Loss - 0.0018\n",
      "Epoch 8716:\tTraining Loss - 0.0057\n",
      "Epoch 8717:\tTraining Loss - 0.0025\n",
      "Epoch 8718:\tTraining Loss - 0.0041\n",
      "Epoch 8719:\tTraining Loss - 0.0025\n",
      "Epoch 8720:\tTraining Loss - 0.0057\n",
      "Epoch 8721:\tTraining Loss - 0.0017\n",
      "Epoch 8722:\tTraining Loss - 0.0024\n",
      "Epoch 8723:\tTraining Loss - 0.0024\n",
      "Epoch 8724:\tTraining Loss - 0.0070\n",
      "Epoch 8725:\tTraining Loss - 0.0017\n",
      "Epoch 8726:\tTraining Loss - 0.0034\n",
      "Epoch 8727:\tTraining Loss - 0.0017\n",
      "Epoch 8728:\tTraining Loss - 0.0017\n",
      "Epoch 8729:\tTraining Loss - 0.0035\n",
      "Epoch 8730:\tTraining Loss - 0.0023\n",
      "Epoch 8731:\tTraining Loss - 0.0069\n",
      "Epoch 8732:\tTraining Loss - 0.0031\n",
      "Epoch 8733:\tTraining Loss - 0.0012\n",
      "Epoch 8734:\tTraining Loss - 0.0030\n",
      "Epoch 8735:\tTraining Loss - 0.0083\n",
      "Epoch 8736:\tTraining Loss - 0.0040\n",
      "Epoch 8737:\tTraining Loss - 0.0026\n",
      "Epoch 8738:\tTraining Loss - 0.0051\n",
      "Epoch 8739:\tTraining Loss - 0.0039\n",
      "Epoch 8740:\tTraining Loss - 0.0049\n",
      "Epoch 8741:\tTraining Loss - 0.0045\n",
      "Epoch 8742:\tTraining Loss - 0.0020\n",
      "Epoch 8743:\tTraining Loss - 0.0032\n",
      "Epoch 8744:\tTraining Loss - 0.0029\n",
      "Epoch 8745:\tTraining Loss - 0.0045\n",
      "Epoch 8746:\tTraining Loss - 0.0076\n",
      "Epoch 8747:\tTraining Loss - 0.0020\n",
      "Epoch 8748:\tTraining Loss - 0.0027\n",
      "Epoch 8749:\tTraining Loss - 0.0096\n",
      "Epoch 8750:\tTraining Loss - 0.0016\n",
      "Epoch 8751:\tTraining Loss - 0.0022\n",
      "Epoch 8752:\tTraining Loss - 0.0039\n",
      "Epoch 8753:\tTraining Loss - 0.0057\n",
      "Epoch 8754:\tTraining Loss - 0.0038\n",
      "Epoch 8755:\tTraining Loss - 0.0044\n",
      "Epoch 8756:\tTraining Loss - 0.0072\n",
      "Epoch 8757:\tTraining Loss - 0.0024\n",
      "Epoch 8758:\tTraining Loss - 0.0038\n",
      "Epoch 8759:\tTraining Loss - 0.0059\n",
      "Epoch 8760:\tTraining Loss - 0.0065\n",
      "Epoch 8761:\tTraining Loss - 0.0036\n",
      "Epoch 8762:\tTraining Loss - 0.0045\n",
      "Epoch 8763:\tTraining Loss - 0.0014\n",
      "Epoch 8764:\tTraining Loss - 0.0040\n",
      "Epoch 8765:\tTraining Loss - 0.0036\n",
      "Epoch 8766:\tTraining Loss - 0.0027\n",
      "Epoch 8767:\tTraining Loss - 0.0025\n",
      "Epoch 8768:\tTraining Loss - 0.0010\n",
      "Epoch 8769:\tTraining Loss - 0.0030\n",
      "Epoch 8770:\tTraining Loss - 0.0014\n",
      "Epoch 8771:\tTraining Loss - 0.0056\n",
      "Epoch 8772:\tTraining Loss - 0.0029\n",
      "Epoch 8773:\tTraining Loss - 0.0024\n",
      "Epoch 8774:\tTraining Loss - 0.0021\n",
      "Epoch 8775:\tTraining Loss - 0.0015\n",
      "Epoch 8776:\tTraining Loss - 0.0014\n",
      "Epoch 8777:\tTraining Loss - 0.0071\n",
      "Epoch 8778:\tTraining Loss - 0.0024\n",
      "Epoch 8779:\tTraining Loss - 0.0093\n",
      "Epoch 8780:\tTraining Loss - 0.0048\n",
      "Epoch 8781:\tTraining Loss - 0.0026\n",
      "Epoch 8782:\tTraining Loss - 0.0044\n",
      "Epoch 8783:\tTraining Loss - 0.0029\n",
      "Epoch 8784:\tTraining Loss - 0.0025\n",
      "Epoch 8785:\tTraining Loss - 0.0039\n",
      "Epoch 8786:\tTraining Loss - 0.0046\n",
      "Epoch 8787:\tTraining Loss - 0.0060\n",
      "Epoch 8788:\tTraining Loss - 0.0059\n",
      "Epoch 8789:\tTraining Loss - 0.0072\n",
      "Epoch 8790:\tTraining Loss - 0.0027\n",
      "Epoch 8791:\tTraining Loss - 0.0034\n",
      "Epoch 8792:\tTraining Loss - 0.0016\n",
      "Epoch 8793:\tTraining Loss - 0.0027\n",
      "Epoch 8794:\tTraining Loss - 0.0031\n",
      "Epoch 8795:\tTraining Loss - 0.0026\n",
      "Epoch 8796:\tTraining Loss - 0.0020\n",
      "Epoch 8797:\tTraining Loss - 0.0030\n",
      "Epoch 8798:\tTraining Loss - 0.0019\n",
      "Epoch 8799:\tTraining Loss - 0.0091\n",
      "Epoch 8800:\tTraining Loss - 0.0079\n",
      "Epoch 8801:\tTraining Loss - 0.0058\n",
      "Epoch 8802:\tTraining Loss - 0.0050\n",
      "Epoch 8803:\tTraining Loss - 0.0050\n",
      "Epoch 8804:\tTraining Loss - 0.0028\n",
      "Epoch 8805:\tTraining Loss - 0.0043\n",
      "Epoch 8806:\tTraining Loss - 0.0032\n",
      "Epoch 8807:\tTraining Loss - 0.0025\n",
      "Epoch 8808:\tTraining Loss - 0.0029\n",
      "Epoch 8809:\tTraining Loss - 0.0071\n",
      "Epoch 8810:\tTraining Loss - 0.0063\n",
      "Epoch 8811:\tTraining Loss - 0.0027\n",
      "Epoch 8812:\tTraining Loss - 0.0067\n",
      "Epoch 8813:\tTraining Loss - 0.0024\n",
      "Epoch 8814:\tTraining Loss - 0.0044\n",
      "Epoch 8815:\tTraining Loss - 0.0015\n",
      "Epoch 8816:\tTraining Loss - 0.0051\n",
      "Epoch 8817:\tTraining Loss - 0.0029\n",
      "Epoch 8818:\tTraining Loss - 0.0053\n",
      "Epoch 8819:\tTraining Loss - 0.0055\n",
      "Epoch 8820:\tTraining Loss - 0.0091\n",
      "Epoch 8821:\tTraining Loss - 0.0044\n",
      "Epoch 8822:\tTraining Loss - 0.0042\n",
      "Epoch 8823:\tTraining Loss - 0.0024\n",
      "Epoch 8824:\tTraining Loss - 0.0015\n",
      "Epoch 8825:\tTraining Loss - 0.0025\n",
      "Epoch 8826:\tTraining Loss - 0.0040\n",
      "Epoch 8827:\tTraining Loss - 0.0028\n",
      "Epoch 8828:\tTraining Loss - 0.0045\n",
      "Epoch 8829:\tTraining Loss - 0.0043\n",
      "Epoch 8830:\tTraining Loss - 0.0064\n",
      "Epoch 8831:\tTraining Loss - 0.0033\n",
      "Epoch 8832:\tTraining Loss - 0.0057\n",
      "Epoch 8833:\tTraining Loss - 0.0052\n",
      "Epoch 8834:\tTraining Loss - 0.0014\n",
      "Epoch 8835:\tTraining Loss - 0.0057\n",
      "Epoch 8836:\tTraining Loss - 0.0048\n",
      "Epoch 8837:\tTraining Loss - 0.0033\n",
      "Epoch 8838:\tTraining Loss - 0.0069\n",
      "Epoch 8839:\tTraining Loss - 0.0057\n",
      "Epoch 8840:\tTraining Loss - 0.0038\n",
      "Epoch 8841:\tTraining Loss - 0.0042\n",
      "Epoch 8842:\tTraining Loss - 0.0052\n",
      "Epoch 8843:\tTraining Loss - 0.0016\n",
      "Epoch 8844:\tTraining Loss - 0.0012\n",
      "Epoch 8845:\tTraining Loss - 0.0023\n",
      "Epoch 8846:\tTraining Loss - 0.0036\n",
      "Epoch 8847:\tTraining Loss - 0.0031\n",
      "Epoch 8848:\tTraining Loss - 0.0076\n",
      "Epoch 8849:\tTraining Loss - 0.0017\n",
      "Epoch 8850:\tTraining Loss - 0.0022\n",
      "Epoch 8851:\tTraining Loss - 0.0043\n",
      "Epoch 8852:\tTraining Loss - 0.0029\n",
      "Epoch 8853:\tTraining Loss - 0.0041\n",
      "Epoch 8854:\tTraining Loss - 0.0036\n",
      "Epoch 8855:\tTraining Loss - 0.0024\n",
      "Epoch 8856:\tTraining Loss - 0.0057\n",
      "Epoch 8857:\tTraining Loss - 0.0052\n",
      "Epoch 8858:\tTraining Loss - 0.0032\n",
      "Epoch 8859:\tTraining Loss - 0.0026\n",
      "Epoch 8860:\tTraining Loss - 0.0046\n",
      "Epoch 8861:\tTraining Loss - 0.0081\n",
      "Epoch 8862:\tTraining Loss - 0.0043\n",
      "Epoch 8863:\tTraining Loss - 0.0017\n",
      "Epoch 8864:\tTraining Loss - 0.0005\n",
      "Epoch 8865:\tTraining Loss - 0.0024\n",
      "Epoch 8866:\tTraining Loss - 0.0029\n",
      "Epoch 8867:\tTraining Loss - 0.0024\n",
      "Epoch 8868:\tTraining Loss - 0.0065\n",
      "Epoch 8869:\tTraining Loss - 0.0024\n",
      "Epoch 8870:\tTraining Loss - 0.0049\n",
      "Epoch 8871:\tTraining Loss - 0.0022\n",
      "Epoch 8872:\tTraining Loss - 0.0023\n",
      "Epoch 8873:\tTraining Loss - 0.0072\n",
      "Epoch 8874:\tTraining Loss - 0.0017\n",
      "Epoch 8875:\tTraining Loss - 0.0056\n",
      "Epoch 8876:\tTraining Loss - 0.0014\n",
      "Epoch 8877:\tTraining Loss - 0.0025\n",
      "Epoch 8878:\tTraining Loss - 0.0015\n",
      "Epoch 8879:\tTraining Loss - 0.0063\n",
      "Epoch 8880:\tTraining Loss - 0.0045\n",
      "Epoch 8881:\tTraining Loss - 0.0071\n",
      "Epoch 8882:\tTraining Loss - 0.0085\n",
      "Epoch 8883:\tTraining Loss - 0.0022\n",
      "Epoch 8884:\tTraining Loss - 0.0045\n",
      "Epoch 8885:\tTraining Loss - 0.0022\n",
      "Epoch 8886:\tTraining Loss - 0.0034\n",
      "Epoch 8887:\tTraining Loss - 0.0032\n",
      "Epoch 8888:\tTraining Loss - 0.0059\n",
      "Epoch 8889:\tTraining Loss - 0.0066\n",
      "Epoch 8890:\tTraining Loss - 0.0049\n",
      "Epoch 8891:\tTraining Loss - 0.0046\n",
      "Epoch 8892:\tTraining Loss - 0.0055\n",
      "Epoch 8893:\tTraining Loss - 0.0030\n",
      "Epoch 8894:\tTraining Loss - 0.0017\n",
      "Epoch 8895:\tTraining Loss - 0.0069\n",
      "Epoch 8896:\tTraining Loss - 0.0038\n",
      "Epoch 8897:\tTraining Loss - 0.0029\n",
      "Epoch 8898:\tTraining Loss - 0.0038\n",
      "Epoch 8899:\tTraining Loss - 0.0043\n",
      "Epoch 8900:\tTraining Loss - 0.0049\n",
      "Epoch 8901:\tTraining Loss - 0.0048\n",
      "Epoch 8902:\tTraining Loss - 0.0042\n",
      "Epoch 8903:\tTraining Loss - 0.0017\n",
      "Epoch 8904:\tTraining Loss - 0.0052\n",
      "Epoch 8905:\tTraining Loss - 0.0025\n",
      "Epoch 8906:\tTraining Loss - 0.0051\n",
      "Epoch 8907:\tTraining Loss - 0.0008\n",
      "Epoch 8908:\tTraining Loss - 0.0054\n",
      "Epoch 8909:\tTraining Loss - 0.0058\n",
      "Epoch 8910:\tTraining Loss - 0.0027\n",
      "Epoch 8911:\tTraining Loss - 0.0053\n",
      "Epoch 8912:\tTraining Loss - 0.0038\n",
      "Epoch 8913:\tTraining Loss - 0.0070\n",
      "Epoch 8914:\tTraining Loss - 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8915:\tTraining Loss - 0.0038\n",
      "Epoch 8916:\tTraining Loss - 0.0070\n",
      "Epoch 8917:\tTraining Loss - 0.0033\n",
      "Epoch 8918:\tTraining Loss - 0.0043\n",
      "Epoch 8919:\tTraining Loss - 0.0016\n",
      "Epoch 8920:\tTraining Loss - 0.0015\n",
      "Epoch 8921:\tTraining Loss - 0.0043\n",
      "Epoch 8922:\tTraining Loss - 0.0031\n",
      "Epoch 8923:\tTraining Loss - 0.0046\n",
      "Epoch 8924:\tTraining Loss - 0.0034\n",
      "Epoch 8925:\tTraining Loss - 0.0094\n",
      "Epoch 8926:\tTraining Loss - 0.0049\n",
      "Epoch 8927:\tTraining Loss - 0.0033\n",
      "Epoch 8928:\tTraining Loss - 0.0023\n",
      "Epoch 8929:\tTraining Loss - 0.0020\n",
      "Epoch 8930:\tTraining Loss - 0.0028\n",
      "Epoch 8931:\tTraining Loss - 0.0042\n",
      "Epoch 8932:\tTraining Loss - 0.0024\n",
      "Epoch 8933:\tTraining Loss - 0.0032\n",
      "Epoch 8934:\tTraining Loss - 0.0036\n",
      "Epoch 8935:\tTraining Loss - 0.0033\n",
      "Epoch 8936:\tTraining Loss - 0.0051\n",
      "Epoch 8937:\tTraining Loss - 0.0044\n",
      "Epoch 8938:\tTraining Loss - 0.0036\n",
      "Epoch 8939:\tTraining Loss - 0.0023\n",
      "Epoch 8940:\tTraining Loss - 0.0013\n",
      "Epoch 8941:\tTraining Loss - 0.0018\n",
      "Epoch 8942:\tTraining Loss - 0.0032\n",
      "Epoch 8943:\tTraining Loss - 0.0035\n",
      "Epoch 8944:\tTraining Loss - 0.0050\n",
      "Epoch 8945:\tTraining Loss - 0.0022\n",
      "Epoch 8946:\tTraining Loss - 0.0023\n",
      "Epoch 8947:\tTraining Loss - 0.0051\n",
      "Epoch 8948:\tTraining Loss - 0.0047\n",
      "Epoch 8949:\tTraining Loss - 0.0021\n",
      "Epoch 8950:\tTraining Loss - 0.0033\n",
      "Epoch 8951:\tTraining Loss - 0.0037\n",
      "Epoch 8952:\tTraining Loss - 0.0032\n",
      "Epoch 8953:\tTraining Loss - 0.0069\n",
      "Epoch 8954:\tTraining Loss - 0.0011\n",
      "Epoch 8955:\tTraining Loss - 0.0027\n",
      "Epoch 8956:\tTraining Loss - 0.0076\n",
      "Epoch 8957:\tTraining Loss - 0.0019\n",
      "Epoch 8958:\tTraining Loss - 0.0028\n",
      "Epoch 8959:\tTraining Loss - 0.0038\n",
      "Epoch 8960:\tTraining Loss - 0.0030\n",
      "Epoch 8961:\tTraining Loss - 0.0064\n",
      "Epoch 8962:\tTraining Loss - 0.0117\n",
      "Epoch 8963:\tTraining Loss - 0.0032\n",
      "Epoch 8964:\tTraining Loss - 0.0037\n",
      "Epoch 8965:\tTraining Loss - 0.0043\n",
      "Epoch 8966:\tTraining Loss - 0.0047\n",
      "Epoch 8967:\tTraining Loss - 0.0047\n",
      "Epoch 8968:\tTraining Loss - 0.0010\n",
      "Epoch 8969:\tTraining Loss - 0.0021\n",
      "Epoch 8970:\tTraining Loss - 0.0046\n",
      "Epoch 8971:\tTraining Loss - 0.0056\n",
      "Epoch 8972:\tTraining Loss - 0.0054\n",
      "Epoch 8973:\tTraining Loss - 0.0042\n",
      "Epoch 8974:\tTraining Loss - 0.0015\n",
      "Epoch 8975:\tTraining Loss - 0.0034\n",
      "Epoch 8976:\tTraining Loss - 0.0091\n",
      "Epoch 8977:\tTraining Loss - 0.0085\n",
      "Epoch 8978:\tTraining Loss - 0.0042\n",
      "Epoch 8979:\tTraining Loss - 0.0050\n",
      "Epoch 8980:\tTraining Loss - 0.0034\n",
      "Epoch 8981:\tTraining Loss - 0.0037\n",
      "Epoch 8982:\tTraining Loss - 0.0051\n",
      "Epoch 8983:\tTraining Loss - 0.0048\n",
      "Epoch 8984:\tTraining Loss - 0.0026\n",
      "Epoch 8985:\tTraining Loss - 0.0043\n",
      "Epoch 8986:\tTraining Loss - 0.0016\n",
      "Epoch 8987:\tTraining Loss - 0.0037\n",
      "Epoch 8988:\tTraining Loss - 0.0028\n",
      "Epoch 8989:\tTraining Loss - 0.0023\n",
      "Epoch 8990:\tTraining Loss - 0.0027\n",
      "Epoch 8991:\tTraining Loss - 0.0041\n",
      "Epoch 8992:\tTraining Loss - 0.0052\n",
      "Epoch 8993:\tTraining Loss - 0.0040\n",
      "Epoch 8994:\tTraining Loss - 0.0046\n",
      "Epoch 8995:\tTraining Loss - 0.0050\n",
      "Epoch 8996:\tTraining Loss - 0.0069\n",
      "Epoch 8997:\tTraining Loss - 0.0051\n",
      "Epoch 8998:\tTraining Loss - 0.0040\n",
      "Epoch 8999:\tTraining Loss - 0.0028\n",
      "Epoch 9000:\tTraining Loss - 0.0032\n",
      "Epoch 9001:\tTraining Loss - 0.0037\n",
      "Epoch 9002:\tTraining Loss - 0.0044\n",
      "Epoch 9003:\tTraining Loss - 0.0026\n",
      "Epoch 9004:\tTraining Loss - 0.0035\n",
      "Epoch 9005:\tTraining Loss - 0.0015\n",
      "Epoch 9006:\tTraining Loss - 0.0028\n",
      "Epoch 9007:\tTraining Loss - 0.0035\n",
      "Epoch 9008:\tTraining Loss - 0.0046\n",
      "Epoch 9009:\tTraining Loss - 0.0030\n",
      "Epoch 9010:\tTraining Loss - 0.0069\n",
      "Epoch 9011:\tTraining Loss - 0.0037\n",
      "Epoch 9012:\tTraining Loss - 0.0033\n",
      "Epoch 9013:\tTraining Loss - 0.0071\n",
      "Epoch 9014:\tTraining Loss - 0.0042\n",
      "Epoch 9015:\tTraining Loss - 0.0033\n",
      "Epoch 9016:\tTraining Loss - 0.0043\n",
      "Epoch 9017:\tTraining Loss - 0.0029\n",
      "Epoch 9018:\tTraining Loss - 0.0026\n",
      "Epoch 9019:\tTraining Loss - 0.0070\n",
      "Epoch 9020:\tTraining Loss - 0.0016\n",
      "Epoch 9021:\tTraining Loss - 0.0052\n",
      "Epoch 9022:\tTraining Loss - 0.0044\n",
      "Epoch 9023:\tTraining Loss - 0.0033\n",
      "Epoch 9024:\tTraining Loss - 0.0053\n",
      "Epoch 9025:\tTraining Loss - 0.0033\n",
      "Epoch 9026:\tTraining Loss - 0.0050\n",
      "Epoch 9027:\tTraining Loss - 0.0053\n",
      "Epoch 9028:\tTraining Loss - 0.0018\n",
      "Epoch 9029:\tTraining Loss - 0.0045\n",
      "Epoch 9030:\tTraining Loss - 0.0032\n",
      "Epoch 9031:\tTraining Loss - 0.0026\n",
      "Epoch 9032:\tTraining Loss - 0.0023\n",
      "Epoch 9033:\tTraining Loss - 0.0057\n",
      "Epoch 9034:\tTraining Loss - 0.0025\n",
      "Epoch 9035:\tTraining Loss - 0.0062\n",
      "Epoch 9036:\tTraining Loss - 0.0049\n",
      "Epoch 9037:\tTraining Loss - 0.0043\n",
      "Epoch 9038:\tTraining Loss - 0.0039\n",
      "Epoch 9039:\tTraining Loss - 0.0043\n",
      "Epoch 9040:\tTraining Loss - 0.0059\n",
      "Epoch 9041:\tTraining Loss - 0.0042\n",
      "Epoch 9042:\tTraining Loss - 0.0050\n",
      "Epoch 9043:\tTraining Loss - 0.0100\n",
      "Epoch 9044:\tTraining Loss - 0.0020\n",
      "Epoch 9045:\tTraining Loss - 0.0013\n",
      "Epoch 9046:\tTraining Loss - 0.0053\n",
      "Epoch 9047:\tTraining Loss - 0.0026\n",
      "Epoch 9048:\tTraining Loss - 0.0060\n",
      "Epoch 9049:\tTraining Loss - 0.0042\n",
      "Epoch 9050:\tTraining Loss - 0.0028\n",
      "Epoch 9051:\tTraining Loss - 0.0025\n",
      "Epoch 9052:\tTraining Loss - 0.0034\n",
      "Epoch 9053:\tTraining Loss - 0.0079\n",
      "Epoch 9054:\tTraining Loss - 0.0046\n",
      "Epoch 9055:\tTraining Loss - 0.0032\n",
      "Epoch 9056:\tTraining Loss - 0.0047\n",
      "Epoch 9057:\tTraining Loss - 0.0036\n",
      "Epoch 9058:\tTraining Loss - 0.0033\n",
      "Epoch 9059:\tTraining Loss - 0.0018\n",
      "Epoch 9060:\tTraining Loss - 0.0066\n",
      "Epoch 9061:\tTraining Loss - 0.0027\n",
      "Epoch 9062:\tTraining Loss - 0.0055\n",
      "Epoch 9063:\tTraining Loss - 0.0065\n",
      "Epoch 9064:\tTraining Loss - 0.0045\n",
      "Epoch 9065:\tTraining Loss - 0.0035\n",
      "Epoch 9066:\tTraining Loss - 0.0036\n",
      "Epoch 9067:\tTraining Loss - 0.0037\n",
      "Epoch 9068:\tTraining Loss - 0.0026\n",
      "Epoch 9069:\tTraining Loss - 0.0062\n",
      "Epoch 9070:\tTraining Loss - 0.0029\n",
      "Epoch 9071:\tTraining Loss - 0.0044\n",
      "Epoch 9072:\tTraining Loss - 0.0030\n",
      "Epoch 9073:\tTraining Loss - 0.0059\n",
      "Epoch 9074:\tTraining Loss - 0.0032\n",
      "Epoch 9075:\tTraining Loss - 0.0059\n",
      "Epoch 9076:\tTraining Loss - 0.0028\n",
      "Epoch 9077:\tTraining Loss - 0.0022\n",
      "Epoch 9078:\tTraining Loss - 0.0034\n",
      "Epoch 9079:\tTraining Loss - 0.0035\n",
      "Epoch 9080:\tTraining Loss - 0.0071\n",
      "Epoch 9081:\tTraining Loss - 0.0045\n",
      "Epoch 9082:\tTraining Loss - 0.0042\n",
      "Epoch 9083:\tTraining Loss - 0.0030\n",
      "Epoch 9084:\tTraining Loss - 0.0041\n",
      "Epoch 9085:\tTraining Loss - 0.0039\n",
      "Epoch 9086:\tTraining Loss - 0.0038\n",
      "Epoch 9087:\tTraining Loss - 0.0029\n",
      "Epoch 9088:\tTraining Loss - 0.0061\n",
      "Epoch 9089:\tTraining Loss - 0.0047\n",
      "Epoch 9090:\tTraining Loss - 0.0015\n",
      "Epoch 9091:\tTraining Loss - 0.0069\n",
      "Epoch 9092:\tTraining Loss - 0.0024\n",
      "Epoch 9093:\tTraining Loss - 0.0040\n",
      "Epoch 9094:\tTraining Loss - 0.0032\n",
      "Epoch 9095:\tTraining Loss - 0.0041\n",
      "Epoch 9096:\tTraining Loss - 0.0042\n",
      "Epoch 9097:\tTraining Loss - 0.0021\n",
      "Epoch 9098:\tTraining Loss - 0.0063\n",
      "Epoch 9099:\tTraining Loss - 0.0040\n",
      "Epoch 9100:\tTraining Loss - 0.0046\n",
      "Epoch 9101:\tTraining Loss - 0.0050\n",
      "Epoch 9102:\tTraining Loss - 0.0038\n",
      "Epoch 9103:\tTraining Loss - 0.0062\n",
      "Epoch 9104:\tTraining Loss - 0.0068\n",
      "Epoch 9105:\tTraining Loss - 0.0029\n",
      "Epoch 9106:\tTraining Loss - 0.0084\n",
      "Epoch 9107:\tTraining Loss - 0.0024\n",
      "Epoch 9108:\tTraining Loss - 0.0023\n",
      "Epoch 9109:\tTraining Loss - 0.0028\n",
      "Epoch 9110:\tTraining Loss - 0.0042\n",
      "Epoch 9111:\tTraining Loss - 0.0036\n",
      "Epoch 9112:\tTraining Loss - 0.0043\n",
      "Epoch 9113:\tTraining Loss - 0.0028\n",
      "Epoch 9114:\tTraining Loss - 0.0020\n",
      "Epoch 9115:\tTraining Loss - 0.0083\n",
      "Epoch 9116:\tTraining Loss - 0.0035\n",
      "Epoch 9117:\tTraining Loss - 0.0079\n",
      "Epoch 9118:\tTraining Loss - 0.0023\n",
      "Epoch 9119:\tTraining Loss - 0.0031\n",
      "Epoch 9120:\tTraining Loss - 0.0028\n",
      "Epoch 9121:\tTraining Loss - 0.0045\n",
      "Epoch 9122:\tTraining Loss - 0.0037\n",
      "Epoch 9123:\tTraining Loss - 0.0038\n",
      "Epoch 9124:\tTraining Loss - 0.0082\n",
      "Epoch 9125:\tTraining Loss - 0.0038\n",
      "Epoch 9126:\tTraining Loss - 0.0039\n",
      "Epoch 9127:\tTraining Loss - 0.0036\n",
      "Epoch 9128:\tTraining Loss - 0.0043\n",
      "Epoch 9129:\tTraining Loss - 0.0037\n",
      "Epoch 9130:\tTraining Loss - 0.0023\n",
      "Epoch 9131:\tTraining Loss - 0.0030\n",
      "Epoch 9132:\tTraining Loss - 0.0059\n",
      "Epoch 9133:\tTraining Loss - 0.0026\n",
      "Epoch 9134:\tTraining Loss - 0.0037\n",
      "Epoch 9135:\tTraining Loss - 0.0051\n",
      "Epoch 9136:\tTraining Loss - 0.0033\n",
      "Epoch 9137:\tTraining Loss - 0.0052\n",
      "Epoch 9138:\tTraining Loss - 0.0030\n",
      "Epoch 9139:\tTraining Loss - 0.0064\n",
      "Epoch 9140:\tTraining Loss - 0.0036\n",
      "Epoch 9141:\tTraining Loss - 0.0056\n",
      "Epoch 9142:\tTraining Loss - 0.0043\n",
      "Epoch 9143:\tTraining Loss - 0.0036\n",
      "Epoch 9144:\tTraining Loss - 0.0037\n",
      "Epoch 9145:\tTraining Loss - 0.0023\n",
      "Epoch 9146:\tTraining Loss - 0.0032\n",
      "Epoch 9147:\tTraining Loss - 0.0019\n",
      "Epoch 9148:\tTraining Loss - 0.0033\n",
      "Epoch 9149:\tTraining Loss - 0.0030\n",
      "Epoch 9150:\tTraining Loss - 0.0033\n",
      "Epoch 9151:\tTraining Loss - 0.0039\n",
      "Epoch 9152:\tTraining Loss - 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9153:\tTraining Loss - 0.0028\n",
      "Epoch 9154:\tTraining Loss - 0.0048\n",
      "Epoch 9155:\tTraining Loss - 0.0057\n",
      "Epoch 9156:\tTraining Loss - 0.0019\n",
      "Epoch 9157:\tTraining Loss - 0.0050\n",
      "Epoch 9158:\tTraining Loss - 0.0061\n",
      "Epoch 9159:\tTraining Loss - 0.0022\n",
      "Epoch 9160:\tTraining Loss - 0.0118\n",
      "Epoch 9161:\tTraining Loss - 0.0046\n",
      "Epoch 9162:\tTraining Loss - 0.0039\n",
      "Epoch 9163:\tTraining Loss - 0.0038\n",
      "Epoch 9164:\tTraining Loss - 0.0032\n",
      "Epoch 9165:\tTraining Loss - 0.0038\n",
      "Epoch 9166:\tTraining Loss - 0.0038\n",
      "Epoch 9167:\tTraining Loss - 0.0032\n",
      "Epoch 9168:\tTraining Loss - 0.0103\n",
      "Epoch 9169:\tTraining Loss - 0.0071\n",
      "Epoch 9170:\tTraining Loss - 0.0027\n",
      "Epoch 9171:\tTraining Loss - 0.0064\n",
      "Epoch 9172:\tTraining Loss - 0.0017\n",
      "Epoch 9173:\tTraining Loss - 0.0062\n",
      "Epoch 9174:\tTraining Loss - 0.0046\n",
      "Epoch 9175:\tTraining Loss - 0.0011\n",
      "Epoch 9176:\tTraining Loss - 0.0035\n",
      "Epoch 9177:\tTraining Loss - 0.0076\n",
      "Epoch 9178:\tTraining Loss - 0.0037\n",
      "Epoch 9179:\tTraining Loss - 0.0044\n",
      "Epoch 9180:\tTraining Loss - 0.0064\n",
      "Epoch 9181:\tTraining Loss - 0.0050\n",
      "Epoch 9182:\tTraining Loss - 0.0030\n",
      "Epoch 9183:\tTraining Loss - 0.0049\n",
      "Epoch 9184:\tTraining Loss - 0.0062\n",
      "Epoch 9185:\tTraining Loss - 0.0033\n",
      "Epoch 9186:\tTraining Loss - 0.0028\n",
      "Epoch 9187:\tTraining Loss - 0.0043\n",
      "Epoch 9188:\tTraining Loss - 0.0055\n",
      "Epoch 9189:\tTraining Loss - 0.0020\n",
      "Epoch 9190:\tTraining Loss - 0.0030\n",
      "Epoch 9191:\tTraining Loss - 0.0065\n",
      "Epoch 9192:\tTraining Loss - 0.0018\n",
      "Epoch 9193:\tTraining Loss - 0.0037\n",
      "Epoch 9194:\tTraining Loss - 0.0026\n",
      "Epoch 9195:\tTraining Loss - 0.0078\n",
      "Epoch 9196:\tTraining Loss - 0.0033\n",
      "Epoch 9197:\tTraining Loss - 0.0022\n",
      "Epoch 9198:\tTraining Loss - 0.0071\n",
      "Epoch 9199:\tTraining Loss - 0.0082\n",
      "Epoch 9200:\tTraining Loss - 0.0082\n",
      "Epoch 9201:\tTraining Loss - 0.0027\n",
      "Epoch 9202:\tTraining Loss - 0.0021\n",
      "Epoch 9203:\tTraining Loss - 0.0046\n",
      "Epoch 9204:\tTraining Loss - 0.0019\n",
      "Epoch 9205:\tTraining Loss - 0.0023\n",
      "Epoch 9206:\tTraining Loss - 0.0042\n",
      "Epoch 9207:\tTraining Loss - 0.0019\n",
      "Epoch 9208:\tTraining Loss - 0.0039\n",
      "Epoch 9209:\tTraining Loss - 0.0034\n",
      "Epoch 9210:\tTraining Loss - 0.0031\n",
      "Epoch 9211:\tTraining Loss - 0.0030\n",
      "Epoch 9212:\tTraining Loss - 0.0106\n",
      "Epoch 9213:\tTraining Loss - 0.0089\n",
      "Epoch 9214:\tTraining Loss - 0.0041\n",
      "Epoch 9215:\tTraining Loss - 0.0038\n",
      "Epoch 9216:\tTraining Loss - 0.0033\n",
      "Epoch 9217:\tTraining Loss - 0.0061\n",
      "Epoch 9218:\tTraining Loss - 0.0047\n",
      "Epoch 9219:\tTraining Loss - 0.0030\n",
      "Epoch 9220:\tTraining Loss - 0.0039\n",
      "Epoch 9221:\tTraining Loss - 0.0073\n",
      "Epoch 9222:\tTraining Loss - 0.0047\n",
      "Epoch 9223:\tTraining Loss - 0.0042\n",
      "Epoch 9224:\tTraining Loss - 0.0042\n",
      "Epoch 9225:\tTraining Loss - 0.0048\n",
      "Epoch 9226:\tTraining Loss - 0.0019\n",
      "Epoch 9227:\tTraining Loss - 0.0067\n",
      "Epoch 9228:\tTraining Loss - 0.0049\n",
      "Epoch 9229:\tTraining Loss - 0.0028\n",
      "Epoch 9230:\tTraining Loss - 0.0025\n",
      "Epoch 9231:\tTraining Loss - 0.0050\n",
      "Epoch 9232:\tTraining Loss - 0.0045\n",
      "Epoch 9233:\tTraining Loss - 0.0051\n",
      "Epoch 9234:\tTraining Loss - 0.0028\n",
      "Epoch 9235:\tTraining Loss - 0.0030\n",
      "Epoch 9236:\tTraining Loss - 0.0052\n",
      "Epoch 9237:\tTraining Loss - 0.0031\n",
      "Epoch 9238:\tTraining Loss - 0.0053\n",
      "Epoch 9239:\tTraining Loss - 0.0027\n",
      "Epoch 9240:\tTraining Loss - 0.0040\n",
      "Epoch 9241:\tTraining Loss - 0.0031\n",
      "Epoch 9242:\tTraining Loss - 0.0022\n",
      "Epoch 9243:\tTraining Loss - 0.0020\n",
      "Epoch 9244:\tTraining Loss - 0.0043\n",
      "Epoch 9245:\tTraining Loss - 0.0059\n",
      "Epoch 9246:\tTraining Loss - 0.0060\n",
      "Epoch 9247:\tTraining Loss - 0.0012\n",
      "Epoch 9248:\tTraining Loss - 0.0042\n",
      "Epoch 9249:\tTraining Loss - 0.0037\n",
      "Epoch 9250:\tTraining Loss - 0.0026\n",
      "Epoch 9251:\tTraining Loss - 0.0048\n",
      "Epoch 9252:\tTraining Loss - 0.0051\n",
      "Epoch 9253:\tTraining Loss - 0.0061\n",
      "Epoch 9254:\tTraining Loss - 0.0082\n",
      "Epoch 9255:\tTraining Loss - 0.0062\n",
      "Epoch 9256:\tTraining Loss - 0.0066\n",
      "Epoch 9257:\tTraining Loss - 0.0045\n",
      "Epoch 9258:\tTraining Loss - 0.0023\n",
      "Epoch 9259:\tTraining Loss - 0.0073\n",
      "Epoch 9260:\tTraining Loss - 0.0036\n",
      "Epoch 9261:\tTraining Loss - 0.0018\n",
      "Epoch 9262:\tTraining Loss - 0.0012\n",
      "Epoch 9263:\tTraining Loss - 0.0055\n",
      "Epoch 9264:\tTraining Loss - 0.0031\n",
      "Epoch 9265:\tTraining Loss - 0.0021\n",
      "Epoch 9266:\tTraining Loss - 0.0043\n",
      "Epoch 9267:\tTraining Loss - 0.0023\n",
      "Epoch 9268:\tTraining Loss - 0.0044\n",
      "Epoch 9269:\tTraining Loss - 0.0023\n",
      "Epoch 9270:\tTraining Loss - 0.0035\n",
      "Epoch 9271:\tTraining Loss - 0.0060\n",
      "Epoch 9272:\tTraining Loss - 0.0088\n",
      "Epoch 9273:\tTraining Loss - 0.0040\n",
      "Epoch 9274:\tTraining Loss - 0.0029\n",
      "Epoch 9275:\tTraining Loss - 0.0020\n",
      "Epoch 9276:\tTraining Loss - 0.0042\n",
      "Epoch 9277:\tTraining Loss - 0.0048\n",
      "Epoch 9278:\tTraining Loss - 0.0025\n",
      "Epoch 9279:\tTraining Loss - 0.0014\n",
      "Epoch 9280:\tTraining Loss - 0.0041\n",
      "Epoch 9281:\tTraining Loss - 0.0020\n",
      "Epoch 9282:\tTraining Loss - 0.0029\n",
      "Epoch 9283:\tTraining Loss - 0.0068\n",
      "Epoch 9284:\tTraining Loss - 0.0049\n",
      "Epoch 9285:\tTraining Loss - 0.0063\n",
      "Epoch 9286:\tTraining Loss - 0.0032\n",
      "Epoch 9287:\tTraining Loss - 0.0066\n",
      "Epoch 9288:\tTraining Loss - 0.0040\n",
      "Epoch 9289:\tTraining Loss - 0.0049\n",
      "Epoch 9290:\tTraining Loss - 0.0075\n",
      "Epoch 9291:\tTraining Loss - 0.0030\n",
      "Epoch 9292:\tTraining Loss - 0.0018\n",
      "Epoch 9293:\tTraining Loss - 0.0107\n",
      "Epoch 9294:\tTraining Loss - 0.0035\n",
      "Epoch 9295:\tTraining Loss - 0.0050\n",
      "Epoch 9296:\tTraining Loss - 0.0029\n",
      "Epoch 9297:\tTraining Loss - 0.0049\n",
      "Epoch 9298:\tTraining Loss - 0.0081\n",
      "Epoch 9299:\tTraining Loss - 0.0042\n",
      "Epoch 9300:\tTraining Loss - 0.0038\n",
      "Epoch 9301:\tTraining Loss - 0.0055\n",
      "Epoch 9302:\tTraining Loss - 0.0046\n",
      "Epoch 9303:\tTraining Loss - 0.0020\n",
      "Epoch 9304:\tTraining Loss - 0.0025\n",
      "Epoch 9305:\tTraining Loss - 0.0025\n",
      "Epoch 9306:\tTraining Loss - 0.0031\n",
      "Epoch 9307:\tTraining Loss - 0.0033\n",
      "Epoch 9308:\tTraining Loss - 0.0049\n",
      "Epoch 9309:\tTraining Loss - 0.0025\n",
      "Epoch 9310:\tTraining Loss - 0.0031\n",
      "Epoch 9311:\tTraining Loss - 0.0016\n",
      "Epoch 9312:\tTraining Loss - 0.0030\n",
      "Epoch 9313:\tTraining Loss - 0.0044\n",
      "Epoch 9314:\tTraining Loss - 0.0031\n",
      "Epoch 9315:\tTraining Loss - 0.0051\n",
      "Epoch 9316:\tTraining Loss - 0.0056\n",
      "Epoch 9317:\tTraining Loss - 0.0061\n",
      "Epoch 9318:\tTraining Loss - 0.0037\n",
      "Epoch 9319:\tTraining Loss - 0.0021\n",
      "Epoch 9320:\tTraining Loss - 0.0068\n",
      "Epoch 9321:\tTraining Loss - 0.0032\n",
      "Epoch 9322:\tTraining Loss - 0.0063\n",
      "Epoch 9323:\tTraining Loss - 0.0024\n",
      "Epoch 9324:\tTraining Loss - 0.0056\n",
      "Epoch 9325:\tTraining Loss - 0.0029\n",
      "Epoch 9326:\tTraining Loss - 0.0040\n",
      "Epoch 9327:\tTraining Loss - 0.0047\n",
      "Epoch 9328:\tTraining Loss - 0.0011\n",
      "Epoch 9329:\tTraining Loss - 0.0034\n",
      "Epoch 9330:\tTraining Loss - 0.0033\n",
      "Epoch 9331:\tTraining Loss - 0.0037\n",
      "Epoch 9332:\tTraining Loss - 0.0057\n",
      "Epoch 9333:\tTraining Loss - 0.0031\n",
      "Epoch 9334:\tTraining Loss - 0.0062\n",
      "Epoch 9335:\tTraining Loss - 0.0056\n",
      "Epoch 9336:\tTraining Loss - 0.0043\n",
      "Epoch 9337:\tTraining Loss - 0.0044\n",
      "Epoch 9338:\tTraining Loss - 0.0030\n",
      "Epoch 9339:\tTraining Loss - 0.0034\n",
      "Epoch 9340:\tTraining Loss - 0.0048\n",
      "Epoch 9341:\tTraining Loss - 0.0023\n",
      "Epoch 9342:\tTraining Loss - 0.0020\n",
      "Epoch 9343:\tTraining Loss - 0.0050\n",
      "Epoch 9344:\tTraining Loss - 0.0040\n",
      "Epoch 9345:\tTraining Loss - 0.0029\n",
      "Epoch 9346:\tTraining Loss - 0.0046\n",
      "Epoch 9347:\tTraining Loss - 0.0034\n",
      "Epoch 9348:\tTraining Loss - 0.0021\n",
      "Epoch 9349:\tTraining Loss - 0.0058\n",
      "Epoch 9350:\tTraining Loss - 0.0045\n",
      "Epoch 9351:\tTraining Loss - 0.0097\n",
      "Epoch 9352:\tTraining Loss - 0.0034\n",
      "Epoch 9353:\tTraining Loss - 0.0041\n",
      "Epoch 9354:\tTraining Loss - 0.0037\n",
      "Epoch 9355:\tTraining Loss - 0.0028\n",
      "Epoch 9356:\tTraining Loss - 0.0046\n",
      "Epoch 9357:\tTraining Loss - 0.0033\n",
      "Epoch 9358:\tTraining Loss - 0.0023\n",
      "Epoch 9359:\tTraining Loss - 0.0023\n",
      "Epoch 9360:\tTraining Loss - 0.0024\n",
      "Epoch 9361:\tTraining Loss - 0.0023\n",
      "Epoch 9362:\tTraining Loss - 0.0061\n",
      "Epoch 9363:\tTraining Loss - 0.0013\n",
      "Epoch 9364:\tTraining Loss - 0.0024\n",
      "Epoch 9365:\tTraining Loss - 0.0046\n",
      "Epoch 9366:\tTraining Loss - 0.0011\n",
      "Epoch 9367:\tTraining Loss - 0.0016\n",
      "Epoch 9368:\tTraining Loss - 0.0059\n",
      "Epoch 9369:\tTraining Loss - 0.0030\n",
      "Epoch 9370:\tTraining Loss - 0.0021\n",
      "Epoch 9371:\tTraining Loss - 0.0062\n",
      "Epoch 9372:\tTraining Loss - 0.0038\n",
      "Epoch 9373:\tTraining Loss - 0.0017\n",
      "Epoch 9374:\tTraining Loss - 0.0053\n",
      "Epoch 9375:\tTraining Loss - 0.0034\n",
      "Epoch 9376:\tTraining Loss - 0.0042\n",
      "Epoch 9377:\tTraining Loss - 0.0034\n",
      "Epoch 9378:\tTraining Loss - 0.0028\n",
      "Epoch 9379:\tTraining Loss - 0.0051\n",
      "Epoch 9380:\tTraining Loss - 0.0061\n",
      "Epoch 9381:\tTraining Loss - 0.0026\n",
      "Epoch 9382:\tTraining Loss - 0.0046\n",
      "Epoch 9383:\tTraining Loss - 0.0020\n",
      "Epoch 9384:\tTraining Loss - 0.0049\n",
      "Epoch 9385:\tTraining Loss - 0.0048\n",
      "Epoch 9386:\tTraining Loss - 0.0041\n",
      "Epoch 9387:\tTraining Loss - 0.0032\n",
      "Epoch 9388:\tTraining Loss - 0.0099\n",
      "Epoch 9389:\tTraining Loss - 0.0046\n",
      "Epoch 9390:\tTraining Loss - 0.0016\n",
      "Epoch 9391:\tTraining Loss - 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9392:\tTraining Loss - 0.0022\n",
      "Epoch 9393:\tTraining Loss - 0.0018\n",
      "Epoch 9394:\tTraining Loss - 0.0031\n",
      "Epoch 9395:\tTraining Loss - 0.0073\n",
      "Epoch 9396:\tTraining Loss - 0.0053\n",
      "Epoch 9397:\tTraining Loss - 0.0032\n",
      "Epoch 9398:\tTraining Loss - 0.0033\n",
      "Epoch 9399:\tTraining Loss - 0.0048\n",
      "Epoch 9400:\tTraining Loss - 0.0019\n",
      "Epoch 9401:\tTraining Loss - 0.0026\n",
      "Epoch 9402:\tTraining Loss - 0.0021\n",
      "Epoch 9403:\tTraining Loss - 0.0044\n",
      "Epoch 9404:\tTraining Loss - 0.0033\n",
      "Epoch 9405:\tTraining Loss - 0.0033\n",
      "Epoch 9406:\tTraining Loss - 0.0046\n",
      "Epoch 9407:\tTraining Loss - 0.0039\n",
      "Epoch 9408:\tTraining Loss - 0.0044\n",
      "Epoch 9409:\tTraining Loss - 0.0040\n",
      "Epoch 9410:\tTraining Loss - 0.0019\n",
      "Epoch 9411:\tTraining Loss - 0.0018\n",
      "Epoch 9412:\tTraining Loss - 0.0033\n",
      "Epoch 9413:\tTraining Loss - 0.0037\n",
      "Epoch 9414:\tTraining Loss - 0.0021\n",
      "Epoch 9415:\tTraining Loss - 0.0080\n",
      "Epoch 9416:\tTraining Loss - 0.0052\n",
      "Epoch 9417:\tTraining Loss - 0.0028\n",
      "Epoch 9418:\tTraining Loss - 0.0018\n",
      "Epoch 9419:\tTraining Loss - 0.0071\n",
      "Epoch 9420:\tTraining Loss - 0.0025\n",
      "Epoch 9421:\tTraining Loss - 0.0049\n",
      "Epoch 9422:\tTraining Loss - 0.0033\n",
      "Epoch 9423:\tTraining Loss - 0.0013\n",
      "Epoch 9424:\tTraining Loss - 0.0045\n",
      "Epoch 9425:\tTraining Loss - 0.0030\n",
      "Epoch 9426:\tTraining Loss - 0.0028\n",
      "Epoch 9427:\tTraining Loss - 0.0034\n",
      "Epoch 9428:\tTraining Loss - 0.0038\n",
      "Epoch 9429:\tTraining Loss - 0.0052\n",
      "Epoch 9430:\tTraining Loss - 0.0061\n",
      "Epoch 9431:\tTraining Loss - 0.0037\n",
      "Epoch 9432:\tTraining Loss - 0.0055\n",
      "Epoch 9433:\tTraining Loss - 0.0028\n",
      "Epoch 9434:\tTraining Loss - 0.0025\n",
      "Epoch 9435:\tTraining Loss - 0.0030\n",
      "Epoch 9436:\tTraining Loss - 0.0012\n",
      "Epoch 9437:\tTraining Loss - 0.0031\n",
      "Epoch 9438:\tTraining Loss - 0.0033\n",
      "Epoch 9439:\tTraining Loss - 0.0050\n",
      "Epoch 9440:\tTraining Loss - 0.0041\n",
      "Epoch 9441:\tTraining Loss - 0.0037\n",
      "Epoch 9442:\tTraining Loss - 0.0043\n",
      "Epoch 9443:\tTraining Loss - 0.0050\n",
      "Epoch 9444:\tTraining Loss - 0.0034\n",
      "Epoch 9445:\tTraining Loss - 0.0013\n",
      "Epoch 9446:\tTraining Loss - 0.0039\n",
      "Epoch 9447:\tTraining Loss - 0.0027\n",
      "Epoch 9448:\tTraining Loss - 0.0030\n",
      "Epoch 9449:\tTraining Loss - 0.0021\n",
      "Epoch 9450:\tTraining Loss - 0.0036\n",
      "Epoch 9451:\tTraining Loss - 0.0028\n",
      "Epoch 9452:\tTraining Loss - 0.0047\n",
      "Epoch 9453:\tTraining Loss - 0.0071\n",
      "Epoch 9454:\tTraining Loss - 0.0033\n",
      "Epoch 9455:\tTraining Loss - 0.0059\n",
      "Epoch 9456:\tTraining Loss - 0.0025\n",
      "Epoch 9457:\tTraining Loss - 0.0032\n",
      "Epoch 9458:\tTraining Loss - 0.0030\n",
      "Epoch 9459:\tTraining Loss - 0.0058\n",
      "Epoch 9460:\tTraining Loss - 0.0019\n",
      "Epoch 9461:\tTraining Loss - 0.0025\n",
      "Epoch 9462:\tTraining Loss - 0.0010\n",
      "Epoch 9463:\tTraining Loss - 0.0037\n",
      "Epoch 9464:\tTraining Loss - 0.0029\n",
      "Epoch 9465:\tTraining Loss - 0.0043\n",
      "Epoch 9466:\tTraining Loss - 0.0047\n",
      "Epoch 9467:\tTraining Loss - 0.0007\n",
      "Epoch 9468:\tTraining Loss - 0.0052\n",
      "Epoch 9469:\tTraining Loss - 0.0040\n",
      "Epoch 9470:\tTraining Loss - 0.0066\n",
      "Epoch 9471:\tTraining Loss - 0.0052\n",
      "Epoch 9472:\tTraining Loss - 0.0050\n",
      "Epoch 9473:\tTraining Loss - 0.0038\n",
      "Epoch 9474:\tTraining Loss - 0.0032\n",
      "Epoch 9475:\tTraining Loss - 0.0046\n",
      "Epoch 9476:\tTraining Loss - 0.0026\n",
      "Epoch 9477:\tTraining Loss - 0.0020\n",
      "Epoch 9478:\tTraining Loss - 0.0086\n",
      "Epoch 9479:\tTraining Loss - 0.0065\n",
      "Epoch 9480:\tTraining Loss - 0.0039\n",
      "Epoch 9481:\tTraining Loss - 0.0037\n",
      "Epoch 9482:\tTraining Loss - 0.0046\n",
      "Epoch 9483:\tTraining Loss - 0.0032\n",
      "Epoch 9484:\tTraining Loss - 0.0061\n",
      "Epoch 9485:\tTraining Loss - 0.0094\n",
      "Epoch 9486:\tTraining Loss - 0.0018\n",
      "Epoch 9487:\tTraining Loss - 0.0081\n",
      "Epoch 9488:\tTraining Loss - 0.0030\n",
      "Epoch 9489:\tTraining Loss - 0.0041\n",
      "Epoch 9490:\tTraining Loss - 0.0043\n",
      "Epoch 9491:\tTraining Loss - 0.0043\n",
      "Epoch 9492:\tTraining Loss - 0.0042\n",
      "Epoch 9493:\tTraining Loss - 0.0029\n",
      "Epoch 9494:\tTraining Loss - 0.0043\n",
      "Epoch 9495:\tTraining Loss - 0.0022\n",
      "Epoch 9496:\tTraining Loss - 0.0026\n",
      "Epoch 9497:\tTraining Loss - 0.0023\n",
      "Epoch 9498:\tTraining Loss - 0.0015\n",
      "Epoch 9499:\tTraining Loss - 0.0076\n",
      "Epoch 9500:\tTraining Loss - 0.0028\n",
      "Epoch 9501:\tTraining Loss - 0.0014\n",
      "Epoch 9502:\tTraining Loss - 0.0049\n",
      "Epoch 9503:\tTraining Loss - 0.0034\n",
      "Epoch 9504:\tTraining Loss - 0.0024\n",
      "Epoch 9505:\tTraining Loss - 0.0045\n",
      "Epoch 9506:\tTraining Loss - 0.0033\n",
      "Epoch 9507:\tTraining Loss - 0.0018\n",
      "Epoch 9508:\tTraining Loss - 0.0030\n",
      "Epoch 9509:\tTraining Loss - 0.0048\n",
      "Epoch 9510:\tTraining Loss - 0.0051\n",
      "Epoch 9511:\tTraining Loss - 0.0049\n",
      "Epoch 9512:\tTraining Loss - 0.0089\n",
      "Epoch 9513:\tTraining Loss - 0.0051\n",
      "Epoch 9514:\tTraining Loss - 0.0033\n",
      "Epoch 9515:\tTraining Loss - 0.0041\n",
      "Epoch 9516:\tTraining Loss - 0.0043\n",
      "Epoch 9517:\tTraining Loss - 0.0027\n",
      "Epoch 9518:\tTraining Loss - 0.0030\n",
      "Epoch 9519:\tTraining Loss - 0.0035\n",
      "Epoch 9520:\tTraining Loss - 0.0065\n",
      "Epoch 9521:\tTraining Loss - 0.0017\n",
      "Epoch 9522:\tTraining Loss - 0.0036\n",
      "Epoch 9523:\tTraining Loss - 0.0038\n",
      "Epoch 9524:\tTraining Loss - 0.0060\n",
      "Epoch 9525:\tTraining Loss - 0.0025\n",
      "Epoch 9526:\tTraining Loss - 0.0041\n",
      "Epoch 9527:\tTraining Loss - 0.0036\n",
      "Epoch 9528:\tTraining Loss - 0.0025\n",
      "Epoch 9529:\tTraining Loss - 0.0031\n",
      "Epoch 9530:\tTraining Loss - 0.0029\n",
      "Epoch 9531:\tTraining Loss - 0.0026\n",
      "Epoch 9532:\tTraining Loss - 0.0022\n",
      "Epoch 9533:\tTraining Loss - 0.0021\n",
      "Epoch 9534:\tTraining Loss - 0.0015\n",
      "Epoch 9535:\tTraining Loss - 0.0018\n",
      "Epoch 9536:\tTraining Loss - 0.0068\n",
      "Epoch 9537:\tTraining Loss - 0.0052\n",
      "Epoch 9538:\tTraining Loss - 0.0039\n",
      "Epoch 9539:\tTraining Loss - 0.0030\n",
      "Epoch 9540:\tTraining Loss - 0.0033\n",
      "Epoch 9541:\tTraining Loss - 0.0041\n",
      "Epoch 9542:\tTraining Loss - 0.0049\n",
      "Epoch 9543:\tTraining Loss - 0.0023\n",
      "Epoch 9544:\tTraining Loss - 0.0024\n",
      "Epoch 9545:\tTraining Loss - 0.0020\n",
      "Epoch 9546:\tTraining Loss - 0.0027\n",
      "Epoch 9547:\tTraining Loss - 0.0012\n",
      "Epoch 9548:\tTraining Loss - 0.0027\n",
      "Epoch 9549:\tTraining Loss - 0.0046\n",
      "Epoch 9550:\tTraining Loss - 0.0032\n",
      "Epoch 9551:\tTraining Loss - 0.0023\n",
      "Epoch 9552:\tTraining Loss - 0.0021\n",
      "Epoch 9553:\tTraining Loss - 0.0017\n",
      "Epoch 9554:\tTraining Loss - 0.0045\n",
      "Epoch 9555:\tTraining Loss - 0.0033\n",
      "Epoch 9556:\tTraining Loss - 0.0029\n",
      "Epoch 9557:\tTraining Loss - 0.0015\n",
      "Epoch 9558:\tTraining Loss - 0.0044\n",
      "Epoch 9559:\tTraining Loss - 0.0042\n",
      "Epoch 9560:\tTraining Loss - 0.0042\n",
      "Epoch 9561:\tTraining Loss - 0.0053\n",
      "Epoch 9562:\tTraining Loss - 0.0030\n",
      "Epoch 9563:\tTraining Loss - 0.0012\n",
      "Epoch 9564:\tTraining Loss - 0.0028\n",
      "Epoch 9565:\tTraining Loss - 0.0078\n",
      "Epoch 9566:\tTraining Loss - 0.0046\n",
      "Epoch 9567:\tTraining Loss - 0.0083\n",
      "Epoch 9568:\tTraining Loss - 0.0125\n",
      "Epoch 9569:\tTraining Loss - 0.0032\n",
      "Epoch 9570:\tTraining Loss - 0.0069\n",
      "Epoch 9571:\tTraining Loss - 0.0056\n",
      "Epoch 9572:\tTraining Loss - 0.0033\n",
      "Epoch 9573:\tTraining Loss - 0.0029\n",
      "Epoch 9574:\tTraining Loss - 0.0039\n",
      "Epoch 9575:\tTraining Loss - 0.0019\n",
      "Epoch 9576:\tTraining Loss - 0.0037\n",
      "Epoch 9577:\tTraining Loss - 0.0034\n",
      "Epoch 9578:\tTraining Loss - 0.0026\n",
      "Epoch 9579:\tTraining Loss - 0.0051\n",
      "Epoch 9580:\tTraining Loss - 0.0053\n",
      "Epoch 9581:\tTraining Loss - 0.0045\n",
      "Epoch 9582:\tTraining Loss - 0.0034\n",
      "Epoch 9583:\tTraining Loss - 0.0016\n",
      "Epoch 9584:\tTraining Loss - 0.0036\n",
      "Epoch 9585:\tTraining Loss - 0.0049\n",
      "Epoch 9586:\tTraining Loss - 0.0054\n",
      "Epoch 9587:\tTraining Loss - 0.0017\n",
      "Epoch 9588:\tTraining Loss - 0.0049\n",
      "Epoch 9589:\tTraining Loss - 0.0041\n",
      "Epoch 9590:\tTraining Loss - 0.0037\n",
      "Epoch 9591:\tTraining Loss - 0.0037\n",
      "Epoch 9592:\tTraining Loss - 0.0031\n",
      "Epoch 9593:\tTraining Loss - 0.0039\n",
      "Epoch 9594:\tTraining Loss - 0.0059\n",
      "Epoch 9595:\tTraining Loss - 0.0022\n",
      "Epoch 9596:\tTraining Loss - 0.0025\n",
      "Epoch 9597:\tTraining Loss - 0.0025\n",
      "Epoch 9598:\tTraining Loss - 0.0032\n",
      "Epoch 9599:\tTraining Loss - 0.0025\n",
      "Epoch 9600:\tTraining Loss - 0.0024\n",
      "Epoch 9601:\tTraining Loss - 0.0017\n",
      "Epoch 9602:\tTraining Loss - 0.0031\n",
      "Epoch 9603:\tTraining Loss - 0.0020\n",
      "Epoch 9604:\tTraining Loss - 0.0082\n",
      "Epoch 9605:\tTraining Loss - 0.0026\n",
      "Epoch 9606:\tTraining Loss - 0.0060\n",
      "Epoch 9607:\tTraining Loss - 0.0061\n",
      "Epoch 9608:\tTraining Loss - 0.0030\n",
      "Epoch 9609:\tTraining Loss - 0.0053\n",
      "Epoch 9610:\tTraining Loss - 0.0024\n",
      "Epoch 9611:\tTraining Loss - 0.0061\n",
      "Epoch 9612:\tTraining Loss - 0.0044\n",
      "Epoch 9613:\tTraining Loss - 0.0050\n",
      "Epoch 9614:\tTraining Loss - 0.0026\n",
      "Epoch 9615:\tTraining Loss - 0.0016\n",
      "Epoch 9616:\tTraining Loss - 0.0054\n",
      "Epoch 9617:\tTraining Loss - 0.0026\n",
      "Epoch 9618:\tTraining Loss - 0.0018\n",
      "Epoch 9619:\tTraining Loss - 0.0026\n",
      "Epoch 9620:\tTraining Loss - 0.0042\n",
      "Epoch 9621:\tTraining Loss - 0.0027\n",
      "Epoch 9622:\tTraining Loss - 0.0035\n",
      "Epoch 9623:\tTraining Loss - 0.0038\n",
      "Epoch 9624:\tTraining Loss - 0.0069\n",
      "Epoch 9625:\tTraining Loss - 0.0019\n",
      "Epoch 9626:\tTraining Loss - 0.0020\n",
      "Epoch 9627:\tTraining Loss - 0.0042\n",
      "Epoch 9628:\tTraining Loss - 0.0038\n",
      "Epoch 9629:\tTraining Loss - 0.0017\n",
      "Epoch 9630:\tTraining Loss - 0.0036\n",
      "Epoch 9631:\tTraining Loss - 0.0092\n",
      "Epoch 9632:\tTraining Loss - 0.0062\n",
      "Epoch 9633:\tTraining Loss - 0.0027\n",
      "Epoch 9634:\tTraining Loss - 0.0067\n",
      "Epoch 9635:\tTraining Loss - 0.0033\n",
      "Epoch 9636:\tTraining Loss - 0.0019\n",
      "Epoch 9637:\tTraining Loss - 0.0017\n",
      "Epoch 9638:\tTraining Loss - 0.0056\n",
      "Epoch 9639:\tTraining Loss - 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9640:\tTraining Loss - 0.0052\n",
      "Epoch 9641:\tTraining Loss - 0.0037\n",
      "Epoch 9642:\tTraining Loss - 0.0038\n",
      "Epoch 9643:\tTraining Loss - 0.0031\n",
      "Epoch 9644:\tTraining Loss - 0.0056\n",
      "Epoch 9645:\tTraining Loss - 0.0046\n",
      "Epoch 9646:\tTraining Loss - 0.0067\n",
      "Epoch 9647:\tTraining Loss - 0.0019\n",
      "Epoch 9648:\tTraining Loss - 0.0047\n",
      "Epoch 9649:\tTraining Loss - 0.0034\n",
      "Epoch 9650:\tTraining Loss - 0.0024\n",
      "Epoch 9651:\tTraining Loss - 0.0025\n",
      "Epoch 9652:\tTraining Loss - 0.0023\n",
      "Epoch 9653:\tTraining Loss - 0.0030\n",
      "Epoch 9654:\tTraining Loss - 0.0056\n",
      "Epoch 9655:\tTraining Loss - 0.0059\n",
      "Epoch 9656:\tTraining Loss - 0.0014\n",
      "Epoch 9657:\tTraining Loss - 0.0030\n",
      "Epoch 9658:\tTraining Loss - 0.0031\n",
      "Epoch 9659:\tTraining Loss - 0.0013\n",
      "Epoch 9660:\tTraining Loss - 0.0024\n",
      "Epoch 9661:\tTraining Loss - 0.0028\n",
      "Epoch 9662:\tTraining Loss - 0.0037\n",
      "Epoch 9663:\tTraining Loss - 0.0045\n",
      "Epoch 9664:\tTraining Loss - 0.0041\n",
      "Epoch 9665:\tTraining Loss - 0.0087\n",
      "Epoch 9666:\tTraining Loss - 0.0027\n",
      "Epoch 9667:\tTraining Loss - 0.0043\n",
      "Epoch 9668:\tTraining Loss - 0.0058\n",
      "Epoch 9669:\tTraining Loss - 0.0041\n",
      "Epoch 9670:\tTraining Loss - 0.0032\n",
      "Epoch 9671:\tTraining Loss - 0.0054\n",
      "Epoch 9672:\tTraining Loss - 0.0037\n",
      "Epoch 9673:\tTraining Loss - 0.0062\n",
      "Epoch 9674:\tTraining Loss - 0.0034\n",
      "Epoch 9675:\tTraining Loss - 0.0006\n",
      "Epoch 9676:\tTraining Loss - 0.0061\n",
      "Epoch 9677:\tTraining Loss - 0.0031\n",
      "Epoch 9678:\tTraining Loss - 0.0017\n",
      "Epoch 9679:\tTraining Loss - 0.0021\n",
      "Epoch 9680:\tTraining Loss - 0.0023\n",
      "Epoch 9681:\tTraining Loss - 0.0038\n",
      "Epoch 9682:\tTraining Loss - 0.0038\n",
      "Epoch 9683:\tTraining Loss - 0.0019\n",
      "Epoch 9684:\tTraining Loss - 0.0026\n",
      "Epoch 9685:\tTraining Loss - 0.0037\n",
      "Epoch 9686:\tTraining Loss - 0.0046\n",
      "Epoch 9687:\tTraining Loss - 0.0025\n",
      "Epoch 9688:\tTraining Loss - 0.0064\n",
      "Epoch 9689:\tTraining Loss - 0.0028\n",
      "Epoch 9690:\tTraining Loss - 0.0034\n",
      "Epoch 9691:\tTraining Loss - 0.0047\n",
      "Epoch 9692:\tTraining Loss - 0.0067\n",
      "Epoch 9693:\tTraining Loss - 0.0027\n",
      "Epoch 9694:\tTraining Loss - 0.0019\n",
      "Epoch 9695:\tTraining Loss - 0.0016\n",
      "Epoch 9696:\tTraining Loss - 0.0091\n",
      "Epoch 9697:\tTraining Loss - 0.0022\n",
      "Epoch 9698:\tTraining Loss - 0.0036\n",
      "Epoch 9699:\tTraining Loss - 0.0036\n",
      "Epoch 9700:\tTraining Loss - 0.0016\n",
      "Epoch 9701:\tTraining Loss - 0.0067\n",
      "Epoch 9702:\tTraining Loss - 0.0032\n",
      "Epoch 9703:\tTraining Loss - 0.0097\n",
      "Epoch 9704:\tTraining Loss - 0.0027\n",
      "Epoch 9705:\tTraining Loss - 0.0043\n",
      "Epoch 9706:\tTraining Loss - 0.0015\n",
      "Epoch 9707:\tTraining Loss - 0.0066\n",
      "Epoch 9708:\tTraining Loss - 0.0067\n",
      "Epoch 9709:\tTraining Loss - 0.0024\n",
      "Epoch 9710:\tTraining Loss - 0.0027\n",
      "Epoch 9711:\tTraining Loss - 0.0015\n",
      "Epoch 9712:\tTraining Loss - 0.0034\n",
      "Epoch 9713:\tTraining Loss - 0.0039\n",
      "Epoch 9714:\tTraining Loss - 0.0041\n",
      "Epoch 9715:\tTraining Loss - 0.0063\n",
      "Epoch 9716:\tTraining Loss - 0.0028\n",
      "Epoch 9717:\tTraining Loss - 0.0072\n",
      "Epoch 9718:\tTraining Loss - 0.0043\n",
      "Epoch 9719:\tTraining Loss - 0.0032\n",
      "Epoch 9720:\tTraining Loss - 0.0067\n",
      "Epoch 9721:\tTraining Loss - 0.0023\n",
      "Epoch 9722:\tTraining Loss - 0.0063\n",
      "Epoch 9723:\tTraining Loss - 0.0057\n",
      "Epoch 9724:\tTraining Loss - 0.0016\n",
      "Epoch 9725:\tTraining Loss - 0.0029\n",
      "Epoch 9726:\tTraining Loss - 0.0016\n",
      "Epoch 9727:\tTraining Loss - 0.0030\n",
      "Epoch 9728:\tTraining Loss - 0.0016\n",
      "Epoch 9729:\tTraining Loss - 0.0035\n",
      "Epoch 9730:\tTraining Loss - 0.0038\n",
      "Epoch 9731:\tTraining Loss - 0.0035\n",
      "Epoch 9732:\tTraining Loss - 0.0026\n",
      "Epoch 9733:\tTraining Loss - 0.0015\n",
      "Epoch 9734:\tTraining Loss - 0.0030\n",
      "Epoch 9735:\tTraining Loss - 0.0061\n",
      "Epoch 9736:\tTraining Loss - 0.0033\n",
      "Epoch 9737:\tTraining Loss - 0.0050\n",
      "Epoch 9738:\tTraining Loss - 0.0012\n",
      "Epoch 9739:\tTraining Loss - 0.0041\n",
      "Epoch 9740:\tTraining Loss - 0.0025\n",
      "Epoch 9741:\tTraining Loss - 0.0075\n",
      "Epoch 9742:\tTraining Loss - 0.0017\n",
      "Epoch 9743:\tTraining Loss - 0.0029\n",
      "Epoch 9744:\tTraining Loss - 0.0040\n",
      "Epoch 9745:\tTraining Loss - 0.0054\n",
      "Epoch 9746:\tTraining Loss - 0.0048\n",
      "Epoch 9747:\tTraining Loss - 0.0041\n",
      "Epoch 9748:\tTraining Loss - 0.0015\n",
      "Epoch 9749:\tTraining Loss - 0.0017\n",
      "Epoch 9750:\tTraining Loss - 0.0048\n",
      "Epoch 9751:\tTraining Loss - 0.0021\n",
      "Epoch 9752:\tTraining Loss - 0.0044\n",
      "Epoch 9753:\tTraining Loss - 0.0069\n",
      "Epoch 9754:\tTraining Loss - 0.0027\n",
      "Epoch 9755:\tTraining Loss - 0.0024\n",
      "Epoch 9756:\tTraining Loss - 0.0011\n",
      "Epoch 9757:\tTraining Loss - 0.0064\n",
      "Epoch 9758:\tTraining Loss - 0.0027\n",
      "Epoch 9759:\tTraining Loss - 0.0032\n",
      "Epoch 9760:\tTraining Loss - 0.0056\n",
      "Epoch 9761:\tTraining Loss - 0.0067\n",
      "Epoch 9762:\tTraining Loss - 0.0006\n",
      "Epoch 9763:\tTraining Loss - 0.0052\n",
      "Epoch 9764:\tTraining Loss - 0.0053\n",
      "Epoch 9765:\tTraining Loss - 0.0037\n",
      "Epoch 9766:\tTraining Loss - 0.0045\n",
      "Epoch 9767:\tTraining Loss - 0.0026\n",
      "Epoch 9768:\tTraining Loss - 0.0105\n",
      "Epoch 9769:\tTraining Loss - 0.0022\n",
      "Epoch 9770:\tTraining Loss - 0.0021\n",
      "Epoch 9771:\tTraining Loss - 0.0046\n",
      "Epoch 9772:\tTraining Loss - 0.0075\n",
      "Epoch 9773:\tTraining Loss - 0.0094\n",
      "Epoch 9774:\tTraining Loss - 0.0036\n",
      "Epoch 9775:\tTraining Loss - 0.0028\n",
      "Epoch 9776:\tTraining Loss - 0.0021\n",
      "Epoch 9777:\tTraining Loss - 0.0021\n",
      "Epoch 9778:\tTraining Loss - 0.0020\n",
      "Epoch 9779:\tTraining Loss - 0.0038\n",
      "Epoch 9780:\tTraining Loss - 0.0034\n",
      "Epoch 9781:\tTraining Loss - 0.0065\n",
      "Epoch 9782:\tTraining Loss - 0.0041\n",
      "Epoch 9783:\tTraining Loss - 0.0033\n",
      "Epoch 9784:\tTraining Loss - 0.0016\n",
      "Epoch 9785:\tTraining Loss - 0.0034\n",
      "Epoch 9786:\tTraining Loss - 0.0038\n",
      "Epoch 9787:\tTraining Loss - 0.0025\n",
      "Epoch 9788:\tTraining Loss - 0.0040\n",
      "Epoch 9789:\tTraining Loss - 0.0040\n",
      "Epoch 9790:\tTraining Loss - 0.0071\n",
      "Epoch 9791:\tTraining Loss - 0.0036\n",
      "Epoch 9792:\tTraining Loss - 0.0067\n",
      "Epoch 9793:\tTraining Loss - 0.0029\n",
      "Epoch 9794:\tTraining Loss - 0.0024\n",
      "Epoch 9795:\tTraining Loss - 0.0019\n",
      "Epoch 9796:\tTraining Loss - 0.0013\n",
      "Epoch 9797:\tTraining Loss - 0.0024\n",
      "Epoch 9798:\tTraining Loss - 0.0026\n",
      "Epoch 9799:\tTraining Loss - 0.0019\n",
      "Epoch 9800:\tTraining Loss - 0.0012\n",
      "Epoch 9801:\tTraining Loss - 0.0016\n",
      "Epoch 9802:\tTraining Loss - 0.0030\n",
      "Epoch 9803:\tTraining Loss - 0.0016\n",
      "Epoch 9804:\tTraining Loss - 0.0038\n",
      "Epoch 9805:\tTraining Loss - 0.0064\n",
      "Epoch 9806:\tTraining Loss - 0.0081\n",
      "Epoch 9807:\tTraining Loss - 0.0098\n",
      "Epoch 9808:\tTraining Loss - 0.0037\n",
      "Epoch 9809:\tTraining Loss - 0.0047\n",
      "Epoch 9810:\tTraining Loss - 0.0030\n",
      "Epoch 9811:\tTraining Loss - 0.0033\n",
      "Epoch 9812:\tTraining Loss - 0.0031\n",
      "Epoch 9813:\tTraining Loss - 0.0036\n",
      "Epoch 9814:\tTraining Loss - 0.0052\n",
      "Epoch 9815:\tTraining Loss - 0.0047\n",
      "Epoch 9816:\tTraining Loss - 0.0032\n",
      "Epoch 9817:\tTraining Loss - 0.0045\n",
      "Epoch 9818:\tTraining Loss - 0.0072\n",
      "Epoch 9819:\tTraining Loss - 0.0034\n",
      "Epoch 9820:\tTraining Loss - 0.0037\n",
      "Epoch 9821:\tTraining Loss - 0.0048\n",
      "Epoch 9822:\tTraining Loss - 0.0015\n",
      "Epoch 9823:\tTraining Loss - 0.0042\n",
      "Epoch 9824:\tTraining Loss - 0.0057\n",
      "Epoch 9825:\tTraining Loss - 0.0041\n",
      "Epoch 9826:\tTraining Loss - 0.0043\n",
      "Epoch 9827:\tTraining Loss - 0.0041\n",
      "Epoch 9828:\tTraining Loss - 0.0074\n",
      "Epoch 9829:\tTraining Loss - 0.0031\n",
      "Epoch 9830:\tTraining Loss - 0.0060\n",
      "Epoch 9831:\tTraining Loss - 0.0023\n",
      "Epoch 9832:\tTraining Loss - 0.0030\n",
      "Epoch 9833:\tTraining Loss - 0.0034\n",
      "Epoch 9834:\tTraining Loss - 0.0023\n",
      "Epoch 9835:\tTraining Loss - 0.0035\n",
      "Epoch 9836:\tTraining Loss - 0.0043\n",
      "Epoch 9837:\tTraining Loss - 0.0039\n",
      "Epoch 9838:\tTraining Loss - 0.0031\n",
      "Epoch 9839:\tTraining Loss - 0.0068\n",
      "Epoch 9840:\tTraining Loss - 0.0033\n",
      "Epoch 9841:\tTraining Loss - 0.0040\n",
      "Epoch 9842:\tTraining Loss - 0.0027\n",
      "Epoch 9843:\tTraining Loss - 0.0034\n",
      "Epoch 9844:\tTraining Loss - 0.0035\n",
      "Epoch 9845:\tTraining Loss - 0.0031\n",
      "Epoch 9846:\tTraining Loss - 0.0034\n",
      "Epoch 9847:\tTraining Loss - 0.0019\n",
      "Epoch 9848:\tTraining Loss - 0.0061\n",
      "Epoch 9849:\tTraining Loss - 0.0059\n",
      "Epoch 9850:\tTraining Loss - 0.0060\n",
      "Epoch 9851:\tTraining Loss - 0.0039\n",
      "Epoch 9852:\tTraining Loss - 0.0039\n",
      "Epoch 9853:\tTraining Loss - 0.0020\n",
      "Epoch 9854:\tTraining Loss - 0.0060\n",
      "Epoch 9855:\tTraining Loss - 0.0025\n",
      "Epoch 9856:\tTraining Loss - 0.0035\n",
      "Epoch 9857:\tTraining Loss - 0.0053\n",
      "Epoch 9858:\tTraining Loss - 0.0089\n",
      "Epoch 9859:\tTraining Loss - 0.0028\n",
      "Epoch 9860:\tTraining Loss - 0.0052\n",
      "Epoch 9861:\tTraining Loss - 0.0043\n",
      "Epoch 9862:\tTraining Loss - 0.0042\n",
      "Epoch 9863:\tTraining Loss - 0.0049\n",
      "Epoch 9864:\tTraining Loss - 0.0039\n",
      "Epoch 9865:\tTraining Loss - 0.0048\n",
      "Epoch 9866:\tTraining Loss - 0.0042\n",
      "Epoch 9867:\tTraining Loss - 0.0037\n",
      "Epoch 9868:\tTraining Loss - 0.0033\n",
      "Epoch 9869:\tTraining Loss - 0.0055\n",
      "Epoch 9870:\tTraining Loss - 0.0080\n",
      "Epoch 9871:\tTraining Loss - 0.0055\n",
      "Epoch 9872:\tTraining Loss - 0.0047\n",
      "Epoch 9873:\tTraining Loss - 0.0044\n",
      "Epoch 9874:\tTraining Loss - 0.0036\n",
      "Epoch 9875:\tTraining Loss - 0.0033\n",
      "Epoch 9876:\tTraining Loss - 0.0032\n",
      "Epoch 9877:\tTraining Loss - 0.0017\n",
      "Epoch 9878:\tTraining Loss - 0.0031\n",
      "Epoch 9879:\tTraining Loss - 0.0020\n",
      "Epoch 9880:\tTraining Loss - 0.0021\n",
      "Epoch 9881:\tTraining Loss - 0.0021\n",
      "Epoch 9882:\tTraining Loss - 0.0060\n",
      "Epoch 9883:\tTraining Loss - 0.0022\n",
      "Epoch 9884:\tTraining Loss - 0.0020\n",
      "Epoch 9885:\tTraining Loss - 0.0025\n",
      "Epoch 9886:\tTraining Loss - 0.0030\n",
      "Epoch 9887:\tTraining Loss - 0.0043\n",
      "Epoch 9888:\tTraining Loss - 0.0020\n",
      "Epoch 9889:\tTraining Loss - 0.0016\n",
      "Epoch 9890:\tTraining Loss - 0.0025\n",
      "Epoch 9891:\tTraining Loss - 0.0034\n",
      "Epoch 9892:\tTraining Loss - 0.0038\n",
      "Epoch 9893:\tTraining Loss - 0.0054\n",
      "Epoch 9894:\tTraining Loss - 0.0043\n",
      "Epoch 9895:\tTraining Loss - 0.0038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9896:\tTraining Loss - 0.0020\n",
      "Epoch 9897:\tTraining Loss - 0.0037\n",
      "Epoch 9898:\tTraining Loss - 0.0051\n",
      "Epoch 9899:\tTraining Loss - 0.0025\n",
      "Epoch 9900:\tTraining Loss - 0.0051\n",
      "Epoch 9901:\tTraining Loss - 0.0028\n",
      "Epoch 9902:\tTraining Loss - 0.0053\n",
      "Epoch 9903:\tTraining Loss - 0.0067\n",
      "Epoch 9904:\tTraining Loss - 0.0017\n",
      "Epoch 9905:\tTraining Loss - 0.0050\n",
      "Epoch 9906:\tTraining Loss - 0.0033\n",
      "Epoch 9907:\tTraining Loss - 0.0050\n",
      "Epoch 9908:\tTraining Loss - 0.0051\n",
      "Epoch 9909:\tTraining Loss - 0.0069\n",
      "Epoch 9910:\tTraining Loss - 0.0049\n",
      "Epoch 9911:\tTraining Loss - 0.0061\n",
      "Epoch 9912:\tTraining Loss - 0.0020\n",
      "Epoch 9913:\tTraining Loss - 0.0023\n",
      "Epoch 9914:\tTraining Loss - 0.0069\n",
      "Epoch 9915:\tTraining Loss - 0.0044\n",
      "Epoch 9916:\tTraining Loss - 0.0064\n",
      "Epoch 9917:\tTraining Loss - 0.0055\n",
      "Epoch 9918:\tTraining Loss - 0.0029\n",
      "Epoch 9919:\tTraining Loss - 0.0025\n",
      "Epoch 9920:\tTraining Loss - 0.0045\n",
      "Epoch 9921:\tTraining Loss - 0.0025\n",
      "Epoch 9922:\tTraining Loss - 0.0039\n",
      "Epoch 9923:\tTraining Loss - 0.0046\n",
      "Epoch 9924:\tTraining Loss - 0.0026\n",
      "Epoch 9925:\tTraining Loss - 0.0048\n",
      "Epoch 9926:\tTraining Loss - 0.0020\n",
      "Epoch 9927:\tTraining Loss - 0.0031\n",
      "Epoch 9928:\tTraining Loss - 0.0035\n",
      "Epoch 9929:\tTraining Loss - 0.0031\n",
      "Epoch 9930:\tTraining Loss - 0.0049\n",
      "Epoch 9931:\tTraining Loss - 0.0032\n",
      "Epoch 9932:\tTraining Loss - 0.0020\n",
      "Epoch 9933:\tTraining Loss - 0.0035\n",
      "Epoch 9934:\tTraining Loss - 0.0023\n",
      "Epoch 9935:\tTraining Loss - 0.0008\n",
      "Epoch 9936:\tTraining Loss - 0.0048\n",
      "Epoch 9937:\tTraining Loss - 0.0040\n",
      "Epoch 9938:\tTraining Loss - 0.0045\n",
      "Epoch 9939:\tTraining Loss - 0.0046\n",
      "Epoch 9940:\tTraining Loss - 0.0014\n",
      "Epoch 9941:\tTraining Loss - 0.0034\n",
      "Epoch 9942:\tTraining Loss - 0.0016\n",
      "Epoch 9943:\tTraining Loss - 0.0053\n",
      "Epoch 9944:\tTraining Loss - 0.0025\n",
      "Epoch 9945:\tTraining Loss - 0.0025\n",
      "Epoch 9946:\tTraining Loss - 0.0017\n",
      "Epoch 9947:\tTraining Loss - 0.0044\n",
      "Epoch 9948:\tTraining Loss - 0.0016\n",
      "Epoch 9949:\tTraining Loss - 0.0089\n",
      "Epoch 9950:\tTraining Loss - 0.0037\n",
      "Epoch 9951:\tTraining Loss - 0.0039\n",
      "Epoch 9952:\tTraining Loss - 0.0031\n",
      "Epoch 9953:\tTraining Loss - 0.0019\n",
      "Epoch 9954:\tTraining Loss - 0.0074\n",
      "Epoch 9955:\tTraining Loss - 0.0041\n",
      "Epoch 9956:\tTraining Loss - 0.0014\n",
      "Epoch 9957:\tTraining Loss - 0.0023\n",
      "Epoch 9958:\tTraining Loss - 0.0031\n",
      "Epoch 9959:\tTraining Loss - 0.0013\n",
      "Epoch 9960:\tTraining Loss - 0.0023\n",
      "Epoch 9961:\tTraining Loss - 0.0027\n",
      "Epoch 9962:\tTraining Loss - 0.0016\n",
      "Epoch 9963:\tTraining Loss - 0.0076\n",
      "Epoch 9964:\tTraining Loss - 0.0019\n",
      "Epoch 9965:\tTraining Loss - 0.0088\n",
      "Epoch 9966:\tTraining Loss - 0.0034\n",
      "Epoch 9967:\tTraining Loss - 0.0013\n",
      "Epoch 9968:\tTraining Loss - 0.0057\n",
      "Epoch 9969:\tTraining Loss - 0.0032\n",
      "Epoch 9970:\tTraining Loss - 0.0016\n",
      "Epoch 9971:\tTraining Loss - 0.0030\n",
      "Epoch 9972:\tTraining Loss - 0.0058\n",
      "Epoch 9973:\tTraining Loss - 0.0034\n",
      "Epoch 9974:\tTraining Loss - 0.0020\n",
      "Epoch 9975:\tTraining Loss - 0.0023\n",
      "Epoch 9976:\tTraining Loss - 0.0053\n",
      "Epoch 9977:\tTraining Loss - 0.0039\n",
      "Epoch 9978:\tTraining Loss - 0.0036\n",
      "Epoch 9979:\tTraining Loss - 0.0006\n",
      "Epoch 9980:\tTraining Loss - 0.0034\n",
      "Epoch 9981:\tTraining Loss - 0.0012\n",
      "Epoch 9982:\tTraining Loss - 0.0022\n",
      "Epoch 9983:\tTraining Loss - 0.0076\n",
      "Epoch 9984:\tTraining Loss - 0.0030\n",
      "Epoch 9985:\tTraining Loss - 0.0042\n",
      "Epoch 9986:\tTraining Loss - 0.0082\n",
      "Epoch 9987:\tTraining Loss - 0.0084\n",
      "Epoch 9988:\tTraining Loss - 0.0041\n",
      "Epoch 9989:\tTraining Loss - 0.0072\n",
      "Epoch 9990:\tTraining Loss - 0.0017\n",
      "Epoch 9991:\tTraining Loss - 0.0014\n",
      "Epoch 9992:\tTraining Loss - 0.0077\n",
      "Epoch 9993:\tTraining Loss - 0.0024\n",
      "Epoch 9994:\tTraining Loss - 0.0047\n",
      "Epoch 9995:\tTraining Loss - 0.0047\n",
      "Epoch 9996:\tTraining Loss - 0.0020\n",
      "Epoch 9997:\tTraining Loss - 0.0020\n",
      "Epoch 9998:\tTraining Loss - 0.0026\n",
      "Epoch 9999:\tTraining Loss - 0.0033\n"
     ]
    }
   ],
   "source": [
    "predicted_seq = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        x_batch, y_batch = next_batch(train_seq, time_step_size, batch_size)\n",
    "        \n",
    "        _, loss_step, _summary = sess.run([train, loss, summaries], \n",
    "                                          feed_dict={x: x_batch, y: y_batch})\n",
    "        writer.add_summary(_summary, global_step=i)\n",
    "        print(f'Epoch {i}:\\tTraining Loss - {loss_step:.04f}')\n",
    "        \n",
    "    \n",
    "    seq = list(train_seq[-12:, :].squeeze())\n",
    "    \n",
    "    for j in range(12):\n",
    "        x_batch = np.array(seq[-12:]).reshape(1, time_step_size, -1)\n",
    "        pred = sess.run(y_pred, feed_dict={x:x_batch})\n",
    "        seq.append(pred[0,-1, 0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = seq[-12:]\n",
    "pred = scaler.inverse_transform(np.array(pred).reshape(-1, 1))\n",
    "pred = pred.squeeze().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joanna/anaconda3/envs/default/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4f4aaf3a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = sample_data.tail(12)\n",
    "test_df['prediction'] = pred\n",
    "\n",
    "test_df.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Implementation of single cells \n",
    "#### Simple RNN\n",
    "#### GRU\n",
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of single cell for simple RNN, LSTM, and GRU\n",
    "\"\"\"\n",
    "\n",
    "class SingleRNNCellExamples:\n",
    "    def __init__(self, input_vocab_size, embedding_dim, n_hid):\n",
    "        #self.embedding = tf.Variable(tf.random_uniform([input_vocab_size, embedding_dim]))\n",
    "        self.n_hid = n_hid\n",
    "        \n",
    "        # SimpleRNN\n",
    "        self.rnn_W = weight_variable('rnn_W', [embedding_dim + n_hid, n_hid])\n",
    "        self.rnn_b = bias_variable('rnn_b', [n_hid])\n",
    "        \n",
    "        # GRU\n",
    "        self.gru_W_h = weight_variable('gru_W_h', [embedding_dim + n_hid, n_hid])\n",
    "        self.gru_b_h = bias_variable('gru_b_h', [n_hid])\n",
    "        \n",
    "        self.gru_W_r = weight_variable('gru_W_r', [embedding_dim + n_hid, n_hid])\n",
    "        self.gru_b_r = bias_variable('gru_b_r', [n_hid])\n",
    "        \n",
    "        self.gru_W_u = weight_variable('gru_W_u', [embedding_dim + n_hid, n_hid])\n",
    "        self.gru_b_u = bias_variable('gru_b_u', [n_hid])\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm_W_o = weight_variable('lstm_W_o', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_o = bias_variable('lstm_b_o', [n_hid])\n",
    "        \n",
    "        self.lstm_W_u = weight_variable('lstm_W_u', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_u = bias_variable('lstm_b_u', [n_hid])    \n",
    " \n",
    "        self.lstm_W_f = weight_variable('lstm_W_f', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_f = bias_variable('lstm_b_f', [n_hid])    \n",
    "        \n",
    "        self.lstm_W_h = weight_variable('lstm_W_h', [embedding_dim + n_hid, n_hid])\n",
    "        self.lstm_b_h = bias_variable('lstm_b_h', [n_hid])\n",
    "        \n",
    "    def _rnn(self, x, hid, activation=tf.nn.tanh):\n",
    "        _input = tf.concat([x, hid], -1)\n",
    "        return activation(tf.matmul(_input, self.rnn_W) + self.rnn_b)\n",
    "    \n",
    "    def _gru(self, x, hid):\n",
    "        _input = tf.concat([x, hid], -1)\n",
    "        relevance_gate = tf.nn.sigmoid(tf.matmul(_input, self.gru_W_r) + self.gru_b_r)\n",
    "        update_gate = tf.nn.sigmoid(tf.matmul(_input, self.gru_W_u) + self.gru_b_u)\n",
    "        \n",
    "        scaled_input = tf.concat([x, tf.multiply(relevance_gate, hid)], -1)\n",
    "        candidate_hid = tf.nn.sigmoid(tf.matmul(scaled_input, self.gru_W_h) + self.gru_b_h) \n",
    "        \n",
    "        return update_gate * candidate_hidden + (tf.ones_like(update_gate) - update_gate) * hid\n",
    "    \n",
    "    def _lstm(self, x, hid):\n",
    "        _input = tf.concat([x, hid], -1)\n",
    "        candidate_hid = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_h) + self.lstm_b_h)\n",
    "        \n",
    "        update_gate = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_u) + self.lstm_b_u)\n",
    "        forget_gate = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_f) + self.lstm_b_f)\n",
    "        output_gate = tf.nn.sigmoid(tf.matmul(_input, self.lstm_W_o) + self.lstm_b_o)\n",
    "        \n",
    "        hid = update_gate * candidate_hid + forget_gate * hid\n",
    "        return output_gate * tf.nn.tanh(hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
