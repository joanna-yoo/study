{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_split = 0.05\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.7\n",
    "\n",
    "epochs = 1001\n",
    "validate_every_n_epochs = 100\n",
    "\n",
    "pretrained_model = 'models/pretrained'\n",
    "model_path = 'models/mnist'\n",
    "logs_path = 'logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/digit-recognizer/train.csv')\n",
    "test_df = pd.read_csv('data/digit-recognizer/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels, train_features = train_df.values[:, 0], train_df.values[:, 1:]\n",
    "test_features = test_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indices = list(range(train_features.shape[0]))\n",
    "np.random.shuffle(train_indices)\n",
    "val_size = int(len(train_indices) * train_test_split)\n",
    "val_indices, train_indices = train_indices[:val_size], train_indices[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_labels, train_labels = train_labels[val_indices,], train_labels[train_indices]\n",
    "val_features, train_features = train_features[val_indices,:], train_features[train_indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x, set):\n",
    "    if x not in set:\n",
    "        raise ValueError(\"Element {} not in the available values\".format(x))\n",
    "    return list(map(lambda s: int(x == s), set))\n",
    "\n",
    "classes = sorted(set(train_labels))\n",
    "\n",
    "train_labels = np.array([one_hot_encode(label, classes) for label in train_labels]).astype(np.float32)\n",
    "train_features = train_features.astype(np.float32)\n",
    "\n",
    "val_labels = np.array([one_hot_encode(label, classes) for label in val_labels]).astype(np.float32)\n",
    "val_features = val_features.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "val_iterator = val_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_batches = test_features.shape[0] // batch_size + 1\n",
    "\n",
    "if test_features.shape[0] % batch_size == 0:\n",
    "    num_test_batches -= 1\n",
    "\n",
    "def get_next_test_batch(batch_num, features):\n",
    "    start_index = batch_num * batch_size\n",
    "    assert start_index <= features.shape[0]\n",
    "    \n",
    "    end_index = min((batch_num + 1) * batch_size, features.shape[0])\n",
    "    return test_features[start_index:end_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(name, shape):\n",
    "    \"\"\"\n",
    "    Initializes weights\n",
    "    Param:\n",
    "        name: string. name of the global variable\n",
    "        shape: list of int. shape of the global_variable\n",
    "    Returns:\n",
    "        tf.Variable\n",
    "    \"\"\"\n",
    "    return tf.get_variable(name=name, shape=shape, dtype=tf.float32,\n",
    "                          initializer=tf.initializers.glorot_uniform)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    \"\"\"\n",
    "    Applies filter W to input x with padding 'SAME' and stride of 1\n",
    "    \"\"\"\n",
    "    return tf.nn.conv2d(x, W, padding='SAME', strides=[1, 1, 1, 1])\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    \"\"\"\n",
    "    Applies max_pool with strides [2, 2] and kernel size [2, 2]\n",
    "    \"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Definition\n",
    "\"\"\"\n",
    "\n",
    "num_channels_1 = 32\n",
    "kernel_size_1 = 5\n",
    "\n",
    "num_channels_2 = 64\n",
    "kernel_size_2 = 3\n",
    "\n",
    "n_dense = 128\n",
    "n_class = len(classes)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_class])\n",
    "\n",
    "x_reshaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "filter_1 = init_weights(\n",
    "    'filter_1', [kernel_size_1, kernel_size_1, 1, num_channels_1])\n",
    "bias_1 = init_weights('bias_1', [num_channels_1])\n",
    "conv_1 = tf.nn.relu(conv2d(x_reshaped, filter_1) + bias_1)\n",
    "\n",
    "filter_2 = init_weights(\n",
    "    'filter_2', [kernel_size_2, kernel_size_2, num_channels_1, num_channels_2])\n",
    "bias_2 = init_weights('bias_2', [num_channels_2])\n",
    "conv_2 = tf.nn.relu(conv2d(conv_1, filter_2) + bias_2)\n",
    "\n",
    "conv_2_shape = conv_2.get_shape().as_list()\n",
    "flattened_size = np.prod(conv_2_shape[1:])\n",
    "x_flattened = tf.reshape(conv_2, [-1, flattened_size])\n",
    "\n",
    "W_1 = init_weights('W_1', [flattened_size, n_dense])\n",
    "b_1 = init_weights('b_1', [n_dense])\n",
    "h_1 = tf.nn.relu(tf.matmul(x_flattened, W_1) + b_1)\n",
    "h_1 = tf.nn.dropout(h_1, keep_prob=keep_prob)\n",
    "\n",
    "W_2 = init_weights('W_2', [n_dense, n_class])\n",
    "b_2 = init_weights('b_2', [n_class])\n",
    "y_pred = tf.matmul(h_1, W_2) + b_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_hat):\n",
    "    scores = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_hat, 1))\n",
    "    return tf.reduce_mean(tf.cast(scores, tf.float32))\n",
    "accuracy = calculate_accuracy(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(logs_path, tf.get_default_graph())\n",
    "tf.summary.scalar('training_loss', loss)\n",
    "tf.summary.scalar('validation_accuracy', accuracy)\n",
    "summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "tf.add_to_collection(\n",
    "    'ops_to_restore', tf.get_default_graph().get_operations())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #if pretrained_model:\n",
    "    #    restorer = tf.train.import_meta_graph(pretrained_model + '.meta')\n",
    "    #    restorer.restore(sess, pretrained_model)\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        x_batch, y_batch = sess.run(train_iterator.get_next())\n",
    "        _, loss_epoch, summary_proto = sess.run(\n",
    "            [train, loss, summaries], feed_dict={x: x_batch, y: y_batch})\n",
    "        writer.add_summary(summary_proto, global_step=epoch)\n",
    "        print('Epoch {}: Training Loss -'.format(epoch), end=' ')\n",
    "        print(loss_epoch)\n",
    "        \n",
    "        if epoch % validate_every_n_epochs == 0:\n",
    "            x_val_batch, y_val_batch = sess.run(val_iterator.get_next())\n",
    "            accuracy_epoch, summary_proto = sess.run(\n",
    "                [accuracy, summaries], feed_dict={x: x_val_batch, y: y_val_batch})\n",
    "            writer.add_summary(summary_proto, global_step=epoch)\n",
    "            saver.save(sess, model_path, global_step=epoch)\n",
    "            print('Epoch {}: Validation Accuracy -'.format(epoch), end=' ')\n",
    "            print(accuracy_epoch)\n",
    "            \n",
    "    for j in range(num_test_batches):\n",
    "        x_test = get_next_test_batch(j, test_features).astype(np.float32)\n",
    "        dummy_y = np.zeros([batch_size, n_class]).astype(np.float32)\n",
    "        test_predicted = sess.run(\n",
    "            y_pred, feed_dict={x: tf.constant(x_test), y: tf.constant(dummy_y)})\n",
    "        \n",
    "        labels.extend(test_predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "labels = []\n",
    "#graph = tf.get_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph(pretrained_model + '.meta')\n",
    "    saver.restore(sess, pretrained_model)\n",
    "\n",
    "    for j in range(num_test_batches):\n",
    "        x_test = get_next_test_batch(j, test_features).astype(np.float32)\n",
    "        dummy_y = np.zeros([batch_size, n_class]).astype(np.float32)\n",
    "        test_predicted = sess.run(y_pred, feed_dict={x: x_test, y: dummy_y})\n",
    "\n",
    "        labels.extend(np.argmax(test_predicted, 1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
